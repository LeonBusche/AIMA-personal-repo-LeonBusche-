{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonBusche/AIMA-personal-repo-LeonBusche-/blob/main/week_07_generative_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d731a498",
      "metadata": {
        "id": "d731a498"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a668a88",
      "metadata": {
        "id": "4a668a88"
      },
      "source": [
        "**This week's exercise has 4 tasks, for a total of 10 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building a modern PyTorch segmentation model\n",
        "- Training a modern model on a real-world segmentation task and achieving passable results\n",
        "\n",
        "**Note**: This is the last exercise concerning pure computer vision. Starting next week, we will begin with Natural Language Processing, i.e. text data. Therefore, don't worry too much if this exercise feels hard or if you can't complete all of it ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e34867",
      "metadata": {
        "id": "96e34867"
      },
      "source": [
        "#### Chapter 3.5 - Segmentation\n",
        "\n",
        "In previous tasks, we solved classification problems - we provide some input(s), typically an image, and get out a few numbers, which are the predicted pseudo-probabilities that our input belongs to some class, such as \"tumor\" or \"no tumor\". For this exercise, we will explore a new task that is extremely common in medical AI research and in clinical practice. This task is called segmentation. In segmentation, the goal is to go from an input image to one or several segmentations (also called *segmentation maps*) of that image. For the example of LiTS, this means that our input remains the same - a 256x256 image with 1 channel. However, our model outputs and targets are now different - they also have the shape 256x256 pixels, times the number of output classes, in our case 3 (background, liver, liver+tumor). Each 256x256 output is basically a map of which pixels in the original image belong to a certain class with what (pseudo-)probability. The training objective, in its simplest form, is also the same; Cross-Entropy Loss, but per pixel, instead of per-image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696c73ae",
      "metadata": {
        "id": "696c73ae"
      },
      "source": [
        "To solve today's tasks, we will need to build ourselves a few new things that look almost the same as things we have already built.\n",
        "\n",
        "**Task 1 (2 points)**: We will need a new Dataset class. It is the same as usual, except this time, when we return image and target in the getitem method, our target is now also a multi-dimensional tensor of size.\n",
        "\n",
        "We will return two kinds of targets - class-index targets and one-hot encoded targets. Class-index targets you already know. Every pixel is assigned a class, which can be 0 for background, 1 for liver, and 2 for lesions. The corresponding tensor has the size $H * W$. One-hot encoded targets instead have size $C * H * W$ - each channel is one class (the 0th channel is background, etc.), and the values for each pixel in a channel are 1 if that pixel belongs to that class and 0 if not. We will need both later on - class-index targets because that is the input for the normal CrossEntropyLoss, and one-hot targets because we will use them in this format for our DiceLoss.\n",
        "\n",
        "Since the \"background\" class has no segmentations, you will have to improvise them from the existing segmentations for this task.\n",
        "\n",
        "Your dataset class should return both targets at the end of the \\_\\_getitem\\_\\_ method like this: `return image, c_targets, oh_targets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1bQ57_y3-YV",
      "metadata": {
        "id": "N1bQ57_y3-YV"
      },
      "outputs": [],
      "source": [
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca92bcfd",
      "metadata": {
        "id": "ca92bcfd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "class LiTS_Segmentation_Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "      self.csv = csv\n",
        "      self.data = pd.read_csv(self.csv)\n",
        "      self.mode = mode\n",
        "      assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "      return len(self.data)\n",
        "\n",
        "    def getitem_segmentation(self, idx: int):\n",
        "\n",
        "      file = self.data.loc[idx, \"filename\"]\n",
        "\n",
        "      # Load image\n",
        "      with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "        f = f.convert(\"L\")\n",
        "        image = ttf.pil_to_tensor(f)\n",
        "\n",
        "      image = image.to(dtype = torch.float32)\n",
        "      image -= torch.min(image)\n",
        "      image /= torch.max(image)\n",
        "      #Bis hierhin ist alles gleich\n",
        "\n",
        "\n",
        "        # We need both the class-index version for the cross-entropy loss and the one-hot version for our dice loss later\n",
        "        # return image, c_targets, oh_targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ],
      "metadata": {
        "id": "VePLDo84pLS9"
      },
      "id": "VePLDo84pLS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68999d16"
      },
      "source": [
        "```markdown\n",
        "### `__getitem__` Methode für LiTS_Segmentation_Dataset\n",
        "\n",
        "Diese Methode implementiert das Laden des Bildes und der zugehörigen Segmentierungsmasken (Leber und Läsion), um daraus die klassenindizierten (`c_targets`) und One-Hot-kodierten (`oh_targets`) Ziel-Tensoren zu erstellen. Es wird davon ausgegangen, dass die Segmentierungsmasken im selben Ordner wie die Bilder liegen und die Dateinamenskonvention `original_filename_liver_mask.png` und `original_filename_lesion_mask.png` verwenden. Wenn Ihre Daten anders benannt sind, müssen Sie die Pfadkonstruktion anpassen.\n",
        "\n",
        "Die `c_targets` werden wie folgt erstellt:\n",
        "- Initialisierung mit 0 (Hintergrund)\n",
        "- Bereiche, die der Leber-Maske entsprechen, werden auf 1 gesetzt (Leber)\n",
        "- Bereiche, die der Läsions-Maske entsprechen, werden auf 2 gesetzt (Läsion). Da Läsionen sich in der Leber befinden, überschreibt `2` hier `1`, was der Aufgabenstellung \"Leber+Tumor\" entspricht.\n",
        "\n",
        "Die `oh_targets` werden dann aus den `c_targets` mittels One-Hot-Kodierung für 3 Klassen (`0: Hintergrund`, `1: Leber`, `2: Läsion`) generiert.\n",
        "```"
      ],
      "id": "68999d16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9c83c28"
      },
      "source": [
        "```python\n",
        "import torch\n",
        "import torchvision.transforms.functional as ttf\n",
        "import PIL\n",
        "import torch.nn.functional as nnf\n",
        "\n",
        "\n",
        "# This method should be placed inside the LiTS_Segmentation_Dataset class.\n",
        "def getitem_segmentation(self, idx: int):\n",
        "\n",
        "    file = self.data.loc[idx, \"filename\"]\n",
        "\n",
        "    # Load image\n",
        "    with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "        f = f.convert(\"L\")\n",
        "        image = ttf.pil_to_tensor(f)\n",
        "\n",
        "    image = image.to(dtype = torch.float32)\n",
        "    image -= torch.min(image)\n",
        "    image /= torch.max(image)\n",
        "\n",
        "    # Assuming segmentation masks have a specific naming convention\n",
        "    # e.g., 'volume-0_slice-0.png' -> 'volume-0_slice-0_liver_mask.png', 'volume-0_slice-0_lesion_mask.png'\n",
        "    # Adjust these paths if your segmentation files are named differently or in other folders.\n",
        "    base_filename = file.rsplit('.', 1)[0] # Remove extension to add custom suffixes\n",
        "\n",
        "    liver_mask_path = f\"./Clean_LiTS/{self.mode}/{base_filename}_liver_mask.png\"\n",
        "    lesion_mask_path = f\"./Clean_LiTS/{self.mode}/{base_filename}_lesion_mask.png\"\n",
        "\n",
        "    # Load liver segmentation mask\n",
        "    with PIL.Image.open(liver_mask_path) as f_liver:\n",
        "        f_liver = f_liver.convert(\"L\") # Convert to grayscale if not already\n",
        "        liver_mask = ttf.pil_to_tensor(f_liver).squeeze(0) # Remove channel dim (1, H, W -> H, W)\n",
        "\n",
        "    # Load lesion segmentation mask\n",
        "    with PIL.Image.open(lesion_mask_path) as f_lesion:\n",
        "        f_lesion = f_lesion.convert(\"L\") # Convert to grayscale if not already\n",
        "        lesion_mask = ttf.pil_to_tensor(f_lesion).squeeze(0) # Remove channel dim (1, H, W -> H, W)\n",
        "\n",
        "    # Create class-index targets (c_targets)\n",
        "    # Initialize with background (0)\n",
        "    c_targets = torch.zeros_like(liver_mask, dtype=torch.long)\n",
        "\n",
        "    # Set liver pixels to 1\n",
        "    c_targets[liver_mask > 0] = 1 # Assuming mask values > 0 indicate the class\n",
        "\n",
        "    # Set lesion pixels to 2 (lesion takes precedence over liver as it's liver+tumor)\n",
        "    c_targets[lesion_mask > 0] = 2 # Assuming mask values > 0 indicate the class\n",
        "\n",
        "    # Create one-hot encoded targets (oh_targets)\n",
        "    # num_classes = 3 (background, liver, lesion)\n",
        "    oh_targets = nnf.one_hot(c_targets, num_classes=3).permute(2, 0, 1).to(dtype=torch.float32)\n",
        "\n",
        "    # return image, c_targets, oh_targets\n",
        "    return image, c_targets, oh_targets\n",
        "\n",
        "# The actual method call would be like this inside the class:\n",
        "# def __getitem__(self, idx: int):\n",
        "#     return self.getitem_segmentation(idx)\n",
        "```"
      ],
      "id": "e9c83c28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LoSJm-I84jI6",
      "metadata": {
        "id": "LoSJm-I84jI6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "train_dataset = LiTS_Segmentation_Dataset(...)\n",
        "val_dataset = LiTS_Segmentation_Dataset(...)\n",
        "test_dataset = LiTS_Segmentation_Dataset(...)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    prefetch_factor = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d416cc5",
      "metadata": {
        "id": "2d416cc5"
      },
      "source": [
        "**Task 2 (2 points)**: Plot a few images that contain livers and tumors, as well as their corresponding segmentation maps. Do they look correct? Is there anything special to note?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7888356d",
      "metadata": {
        "id": "7888356d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d744a45",
      "metadata": {
        "id": "0d744a45"
      },
      "source": [
        "**Task 3 (2 points)**: Next, we need a different loss function. At the bottom, we provide a training/testing loop that already contains cross-entropy loss and a functional segmentation model, plus evaluation. We have learned in the lecture that DICE score, and by extension a DICE-based loss, can be useful for imbalanced classes. We have also discovered that LiTS 2017 contains a class imbalance - slices with tumors are much more rare than slices with livers. Hence, we will make our own DICE loss.\n",
        "\n",
        "The formula for the DICE loss is computed as follows: $1 - \\frac{2 * (|X \\land Y|)+\\epsilon}{|X|+|Y|+\\epsilon}$, where $X$ is the prediction and $Y$ the target.\n",
        "\n",
        "The DICE Loss class you create should fulfill the following criteria:\n",
        "- It subclasses torch.nn.module.\n",
        "- It is a class that implements an \\_\\_init\\_\\_ function.\n",
        "- The loss also implements a \\_\\_forward\\_\\_ function that accepts as inputs a prediction tensor and a target tensor, both of shape B x 3 x 256 x 256 - 3 channels because we will segment background, liver, and liver+tumor again. The output is the computed loss.\n",
        "- You may add class weighting to offset the class imbalance.\n",
        "\n",
        "Your total loss should be `total_loss = ce_loss + dice_loss`, and your backward pass should be `total_loss.backward()`.\n",
        "Run the training for a few epochs, once with and once without DICE loss included as part of the overall loss. In your experiment, which version worked better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01ba8f4",
      "metadata": {
        "id": "c01ba8f4"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as nnf\n",
        "\n",
        "def compute_dice_score(prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the dice score for one class.\n",
        "    \"\"\"\n",
        "\n",
        "    prediction = prediction.to(dtype = torch.bool)\n",
        "    target = target.to(dtype = torch.bool)\n",
        "\n",
        "    intersection = torch.sum(prediction * target)   # TP\n",
        "    p_cardinality = torch.sum(prediction)           # TP+FP\n",
        "    t_cardinality = torch.sum(target)               # TP+FN\n",
        "    cardinality = p_cardinality + t_cardinality\n",
        "    eps = 1e-8\n",
        "\n",
        "    if cardinality != 0:\n",
        "        dice = (2 * intersection + eps) / (cardinality + eps) # 2*TP / (2*TP+FP+FN + eps)\n",
        "    else:\n",
        "        dice = None\n",
        "\n",
        "    return dice\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Binary Dice Loss. Targets must be one-hot encoded.\n",
        "    Needed to make a full DICE loss, this computes dice loss for one channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "        pass\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Dice Loss. Targets must be one-hot encoded.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
        "\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a3f09f",
      "metadata": {
        "id": "66a3f09f"
      },
      "source": [
        "**Task 4 (4 points)**: Finally, we want to make our own model that can handle segmentations. For this course, we will build ourselves a U-Net. The original paper can be found here: https://arxiv.org/pdf/1505.04597.\n",
        "\n",
        "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
        "\n",
        "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
        "\n",
        "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
        "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
        "- We will only have 3 skip connections.\n",
        "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
        "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
        "\n",
        "Note that training a segmentation models takes a little while - we do not award points for results here, because it would mean that you would have to wait a long time to see whether your changes helped performance. All we want to see is that your model learns anything useful at all. As a rough guideline, you will probably start seeing ok liver segmentations after 1 epoch, and good liver and ok lesion segmentations after 2 or 3 epochs.\n",
        "\n",
        "If everything works correctly, you can copy the previous training loop and should get some good results. Don't forget to look at some of your predictions! Are they reasonable? Empty? Weird? Can you discover some kind of systemic issues with your predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c43e445",
      "metadata": {
        "id": "6c43e445"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __forward__(self):\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab0024b",
      "metadata": {
        "id": "0ab0024b"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet() # Your model class goes here\n",
        "model = model.to(device)\n",
        "\n",
        "dice_loss = DiceLoss(num_classes = 3) # Your dice loss class goes here\n",
        "ce_loss = nn.CrossEntropyLoss(\n",
        "    weight = torch.tensor([1.0, 5.0, 20.0]).to(device = device),\n",
        "    reduction = \"mean\",\n",
        "    #ignore_index = 0\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cX3ZORkH-y2q",
      "metadata": {
        "id": "cX3ZORkH-y2q"
      },
      "outputs": [],
      "source": [
        "# If your model and loss work, this should at least execute successfully.\n",
        "# If you only wish to test your model, just comment out the dice_loss component everywhere.\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epochs = 5\n",
        "avg_liver_dice = 0\n",
        "avg_lesion_dice = 0\n",
        "dice_weight = 1\n",
        "ce_weight = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for step, (data, c_targets, oh_targets) in enumerate(train_dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "        predictions = model(data)\n",
        "\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validate once after every epoch\n",
        "        model.eval()\n",
        "\n",
        "        # Don't track gradients for validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            losses = []\n",
        "            background_dices = []\n",
        "            background_counts = []\n",
        "            liver_dices = []\n",
        "            liver_counts = []\n",
        "            lesion_dices = []\n",
        "            lesion_counts = []\n",
        "            batch_sizes = []\n",
        "\n",
        "            for val_step, (data, c_targets, oh_targets) in enumerate(tqdm(val_dataloader)):\n",
        "\n",
        "                data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "                predictions = model(data)\n",
        "                # Choose the likeliest prediction via argmax, then convert to one-hot, and put the new axis in front again\n",
        "                p_arg = nnf.one_hot(torch.argmax(predictions, dim = 1), num_classes = 3).moveaxis(-1, 1)\n",
        "\n",
        "                # loss\n",
        "                loss_1 = dice_loss(predictions, oh_targets)\n",
        "                loss_2 = ce_loss(predictions, c_targets)\n",
        "                total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "                losses.append(total_loss.item())\n",
        "                batch_sizes.append(data.size()[0])\n",
        "\n",
        "                background_seg = oh_targets[:, 0, :, :]\n",
        "                liver_seg = oh_targets[:, 1, :, :]\n",
        "                lesion_seg = oh_targets[:, 2, :, :]\n",
        "\n",
        "                background_dice = compute_dice_score(p_arg[:,0,:,:], background_seg)\n",
        "                background_counts.append(data.size()[0])\n",
        "                background_dices.append(background_dice)\n",
        "\n",
        "                if liver_seg.sum() != 0.0:\n",
        "                    liver_dice = compute_dice_score(p_arg[:,1,:,:], liver_seg)\n",
        "                    liver_counts.append(data.size()[0])\n",
        "                    liver_dices.append(liver_dice)\n",
        "\n",
        "                if lesion_seg.sum() != 0.0:\n",
        "                    lesion_dice = compute_dice_score(p_arg[:,2,:,:], lesion_seg)\n",
        "                    lesion_counts.append(data.size()[0])\n",
        "                    lesion_dices.append(lesion_dice)\n",
        "\n",
        "            avg_background_dice = sum([dice * size for dice, size in zip(background_dices, background_counts)])/sum(background_counts)\n",
        "            avg_liver_dice = sum([dice * size for dice, size in zip(liver_dices, liver_counts)])/sum(liver_counts)\n",
        "            avg_lesion_dice = sum([dice * size for dice, size in zip(lesion_dices, lesion_counts)])/sum(lesion_counts)\n",
        "\n",
        "            avg_loss = sum([l * bs for l, bs in zip(losses, background_counts)]) / sum(background_counts)\n",
        "            print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Liver Dice Score: {avg_liver_dice:.4f}, \\t Lesion Dice Score: {avg_lesion_dice:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fVDuNErcj7T",
      "metadata": {
        "id": "7fVDuNErcj7T"
      },
      "outputs": [],
      "source": [
        "# Test once\n",
        "model.eval()\n",
        "\n",
        "# Don't track gradients for testing\n",
        "with torch.no_grad():\n",
        "\n",
        "    losses = []\n",
        "    background_dices = []\n",
        "    background_counts = []\n",
        "    liver_dices = []\n",
        "    liver_counts = []\n",
        "    lesion_dices = []\n",
        "    lesion_counts = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for test_step, (data, c_targets, oh_targets) in enumerate(tqdm(test_dataloader)):\n",
        "\n",
        "        data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "        predictions = model(data)\n",
        "        # Choose the likeliest prediction via argmax, then convert to one-hot, and put the new axis in front again\n",
        "        p_arg = nnf.one_hot(torch.argmax(predictions, dim = 1), num_classes = 3).moveaxis(-1, 1)\n",
        "\n",
        "        # loss\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight # We could weight contributions from the different loss components here, although 1-to-1 should do just fine\n",
        "\n",
        "        losses.append(total_loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "\n",
        "        background_seg = oh_targets[:, 0, :, :]\n",
        "        liver_seg = oh_targets[:, 1, :, :]\n",
        "        lesion_seg = oh_targets[:, 2, :, :]\n",
        "\n",
        "        background_dice = compute_dice_score(p_arg[:,0,:,:], background_seg)\n",
        "        background_counts.append(data.size()[0])\n",
        "        background_dices.append(background_dice)\n",
        "\n",
        "        if liver_seg.sum() != 0.0:\n",
        "            liver_dice = compute_dice_score(p_arg[:,1,:,:], liver_seg)\n",
        "            liver_counts.append(data.size()[0])\n",
        "            liver_dices.append(liver_dice)\n",
        "\n",
        "        if lesion_seg.sum() != 0.0:\n",
        "            lesion_dice = compute_dice_score(p_arg[:,2,:,:], lesion_seg)\n",
        "            lesion_counts.append(data.size()[0])\n",
        "            lesion_dices.append(lesion_dice)\n",
        "\n",
        "    avg_background_dice = sum([dice * size for dice, size in zip(background_dices, background_counts)])/sum(background_counts)\n",
        "    avg_liver_dice = sum([dice * size for dice, size in zip(liver_dices, liver_counts)])/sum(liver_counts)\n",
        "    avg_lesion_dice = sum([dice * size for dice, size in zip(lesion_dices, lesion_counts)])/sum(lesion_counts)\n",
        "\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, background_counts)]) / sum(background_counts)\n",
        "    print(f\"Epoch: {epoch+1},\\t Test Loss: {avg_loss:.4f},\\t Liver Dice Score: {avg_liver_dice:.4f}, \\t Lesion Dice Score: {avg_lesion_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oDnF9k7ZHhkE",
      "metadata": {
        "id": "oDnF9k7ZHhkE"
      },
      "outputs": [],
      "source": [
        "# Try looking at some images and predicted segmentations to see how badly or how well you've done\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}