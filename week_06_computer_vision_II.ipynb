{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonBusche/AIMA-personal-repo-LeonBusche-/blob/main/week_06_computer_vision_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a61491",
      "metadata": {
        "id": "a5a61491"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f8126",
      "metadata": {
        "id": "c77f8126"
      },
      "source": [
        "**This week's exercise has 3 tasks, for a total of 10.5 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building core components of modern PyTorch models\n",
        "- Assembling modern PyTorch models from components\n",
        "- Training a modern model on a real-world task and achieving passable results\n",
        "\n",
        "**Note**: Since you have already proven that you are capable of creating the core components of a typical training loop yourself, we will provide some utility code for this section. This is done so that you can focus on the important parts of this lesson, and to help us debug your code in case you need help."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4f7614",
      "metadata": {
        "id": "8e4f7614"
      },
      "source": [
        "#### Chapter 3.1 - Data Augmentation\n",
        "\n",
        "**What is data augmentation?**\n",
        "Have you ever lost your glasses and then squinted, or tried to look through a rainy window? Or looked at a false color image, maybe a forest where the trees are blue and the sky green? You can usually make an educated guess what you are looking at, even though the image you see is different than usual. This is, essentialy, what data augmentation is. It's the same data, still recognizable, but slightly altered in some way.\n",
        "\n",
        "**What is that useful for?**\n",
        "Let me begin with an anecdote that you've probably heard in the lectures. Say you have pictures of cats and dogs, and want your model to tell the two apart. How many people you know go to the park with their dogs? I imagine many. Hence, many images of dogs are dogs lying on the grass. The same is generally untrue for cats, at least I have never heard of anyone walking their cat to the park. At any rate, here is what happens when I train a neural network on these images: The model takes a shortcut. It sees a lot of green and the correct answer for these pictures is always \"Dog\". It learns \"Green = Dog\". This is what we call overfitting. We have overfitted to the existence of green in the background as a quintessential part of what makes a dog. Sometimes, we get away with this, if our data always has this correlation.\n",
        "\n",
        "Now I get some new data. A bunch of people have taken pictures of their cats, sunbathing on the terrace. The garden is in the background. Lots of green. The model, in its infinite wisdom, will at first guess that these images are of dogs. Clearly, our model's ability to tell apart cats and dogs has not generalized to this new dataset.\n",
        "\n",
        "So how can we prevent the model from taking shortcuts and encourage learning information that generalizes? We force these generalizations in training. If I gave you an image of a dog, but the grass was brown, and the dog green, you could still identify it as a dog, instead of a cat, right? And so should the model, if we can manage it. So let's also make it train using pictures of cats and dogs where the colours are different or removed. Suddenly, the shortcut solution is no longer useful, and the model must rely on shape, texture, or contextual information like a leash. The practice of color change described is a practical and useful data augmentation that is used in state-of-the-art image recognition.\n",
        "\n",
        "In addition to color changes, there is a myriad of other techniques, such as cropping, image rotation or flipping, edge filters, solarization, random noise, and many, many more. Basically, anything that you believe may eventually show up in testing data and that you want the model to generalize to, can be made into a corresponding data augmentation.\n",
        "\n",
        "**How do we use data augmentations in practice?**\n",
        "There are two ways of adding data augmentation during training. Either, you can implement it inside of your dataset, so that it only returns augmented image tensors, or right before feeding your image tensors into your model. Both options are acceptable and come with advantages and disadvantages, although the more common way is to separate dataset and augmentations. We also showcase the native PyTorch way of augmenting data below.\n",
        "\n",
        "If you are particularly eager, or want to try your hand at making image augmentation functions yourself, it can be fun and is definitely good practice. However, PyTorch comes with a large selection of image augmentations right out of the box, and in the following chapter, we will look at how to make use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e352a55a",
      "metadata": {
        "id": "e352a55a",
        "outputId": "338194c4-c9db-4093-da33-f64feb6b4284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "# Torchvision contains two ways of utilizing transforms:\n",
        "# Functional and Composed.\n",
        "\n",
        "# Functional does what it advertises - it is a function which\n",
        "# you can use on your tensors. Here is an example which performs\n",
        "# a center crop:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transformed_images = ttf.center_crop(img = dummy_images, output_size = (128, 128))\n",
        "print(transformed_images.size())\n",
        "\n",
        "# Functional transforms have the inherent advantage of giving the\n",
        "# user very fine-grained control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55a993c1",
      "metadata": {
        "id": "55a993c1",
        "outputId": "a37a1d56-f89a-41cd-cdde-9c895fdf0be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# The alternative is the so-called Composed form, which uses\n",
        "# classes to achieve the same result. We make a Composed Transform\n",
        "# like so:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transforms = tt.Compose([\n",
        "    tt.RandomCrop(size = (128, 128)),\n",
        "    tt.RandomHorizontalFlip()\n",
        "])\n",
        "transformed_images = transforms(dummy_images)\n",
        "print(transformed_images.size())\n",
        "\n",
        "# As you can see, Compose offers us the option of sequentially\n",
        "# executing multiple transformations in a single line of code.\n",
        "# We also get the option of using randomized augmentations,\n",
        "# where the randomization is already done for us.\n",
        "\n",
        "# In practice, either style of writing transformations is fine.\n",
        "# In fact, they are equivalent, as Compose calls the functional\n",
        "# versions of the transforms under the hood. In the case of\n",
        "# randomized augmentations, the class handles all the randomizing\n",
        "# and then calls the functional transform with the random inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b4a34c",
      "metadata": {
        "id": "b8b4a34c"
      },
      "source": [
        "**Task 1 (1+1 points)**: A complete list of Torchvision's available transforms can be found here: https://docs.pytorch.org/vision/0.9/transforms.html. Consider the task we are working on right now - working with CT images from the LiTS 2017 dataset. Which data augmentations strike you as a good idea to add to our training **(1 point)**? Which do you think are a bad idea or cannot work at all **(1 point)**? Are there any which are missing in Torchvision? If you don't know what they do, try them out and judge for yourselves. Can you think of other image types with other physics behind them? Are the rules for them going to be different?\n",
        "\n",
        "There are no definitely correct or incorrect answers here. The goal for this task is for you to be able to argue your case convincingly (to us) and think closely about your dataset. You can test your assumptions when completing the other tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer\n",
        "\n",
        "Things like color jitter might be dangerous to use. The main difference between the sourroundings and the tumors are the brightness and constrast. Changing that could lead to more false positives, since the MLP would learn to see tumors or livers purly by shape and ignore the contrast.\n",
        "\n",
        "Cropping the image is surly not wanted in our case, since it could simply remove the tumor or liver, changing the class of the image.\n",
        "\n",
        "Modules like flip horizontal or anything that changes the orientation could benefit the MLP. Tumors don't come in only one orientation. BUT: Flipping the image also changes the medical orientation of liver, so maybe still dangerous. Small translations could also benefit the MLP.\n",
        "Also, blurring the image with gaussian blur could provide, that even not so charp images or images that are very difficult to classify can be classified. It simply raises the challenge, making the learning progress more complex -> which is good.\n",
        "Adjusting saturation might also benefit, by exxagurating differences in the image, making tumors or livers more visible. Simular with sharpness.\n",
        "\n",
        "Unsure, but I did not really see anything that would handle 3D maps or videos. Also, not all transformations work for PIL images. Different kinds of noise or blurring might also be of interest.\n",
        "\n"
      ],
      "metadata": {
        "id": "PvEc0QSCg5M-"
      },
      "id": "PvEc0QSCg5M-"
    },
    {
      "cell_type": "markdown",
      "id": "325ff768",
      "metadata": {
        "id": "325ff768"
      },
      "source": [
        "#### Chapter 3.2 - Regularization Techniques\n",
        "\n",
        "While there are multiple definitions or guidelines for what regularization is supposed to do (see lectures), in terms of practical concerns, all regularization techniques have the same aim, expressed through different means: Improving some aspect of the performance of your deep learning models. We differentiate them, broadly, from data augmentations, because regularization techniques generally concern themselves with the learning process, e.g. loss function modifications, learning rate optimizations, temporary model modifications, etc., and *not* the underlying data in our model training.\n",
        "\n",
        "There are a number of different strategies, far too many to list all of them here, but a few particularly successful ones have made it into common use - so much so that they are more prevalent than regularization-less, \"vanilla\" deep learning. These fall into different groups, briefly discussed below.\n",
        "\n",
        "#### Additional Loss Components\n",
        "\n",
        "The loss function for any given modern optimization task is typically continuous and not always smooth everywhere. As a consequence, there are many different parameter configurations in a model that result in the same train-time loss. Not all of these express the same behavior during training or testing, however. When we modify our loss to penalize certain training behaviors, we allow the training process to select for models and parameters that give models that generalize better, converge to a solution faster, etc.,  despite often expressing the same training loss. Let's look at some examples that should be familiar from the lecture:\n",
        "\n",
        "**L1 Loss** - Often also called LASSO, L1 Loss is a penalty term added to the normal loss during training, which is defined as:\n",
        "$L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|$. Growing linearly with parameter magnitude, we penalize the model. This, in turn, forces the model to use fewer and smaller weights - relying on more weights than it needs, and thus probably overfitting, is disincentivized. Similarly, we just forced our model to stick to weights near zero, which we already know is generally a preferable area for model activations to stick to.\n",
        "\n",
        "**L2 Loss** - L2 Loss is the more popular cousin of the L1 Loss, which does approximately the same thing, except the penalty is equal to the sum over all squared parameter magnitudes: $L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|^2$ The reasoning behind it is similar, but it has seen far more practical adoption.\n",
        "\n",
        "**Weight Decay** - An operation that effectively performs the same duty, weight decay reduces the magnitude of weights after each backward pass, for example by subtracting a small constant or multiplying with a factor. In essence, this eliminates parameters which are rarely \"used\" and were thus likely involved in overfitting on a small amount of data anyway. Parameters that are regularly updated (and therefore probably useful), will always remain near their optimal value despite weight decay. Interestingly, Weight Decay is mathematically equivalent to L2 Loss in terms of net parameter updates.\n",
        "\n",
        "#### Training Strategies\n",
        "**Early Stopping**: For early stopping, we monitor the performance of the model on a validation set and stop training when the performance stops improving. This ensures that the model does not continue to train on the training data and potentially overfit, while also saving computational resources.\n",
        "\n",
        "**Dropout**: Dropout is a regularization technique where, during training, a random subset of neurons in a layer is \"dropped out\" (set to zero) for each forward pass. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust and generalized features. During inference, all neurons are used, but their outputs are scaled to account for the dropout during training.\n",
        "\n",
        "**Learning Rate Scheduling (LR Scheduling)**: Learning rate scheduling involves dynamically adjusting the learning rate during training. A high learning rate at the start helps the model converge quickly, while a lower learning rate later allows for fine-tuning. Common strategies include step decay, exponential decay, and cosine annealing. Proper learning rate scheduling can lead to faster convergence and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd62d1",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "f6cd62d1"
      },
      "source": [
        "**Task 1.5 (3 x 0.5 points)**: Let us implement and compare the effects of different regularization techniques on a simple neural network. To do so, follow these steps:\n",
        "\n",
        "1. Create a small neural network (e.g., 2-3 layers) and train it on the LiTS dataset without any regularization. Record the training and validation accuracy/loss. (P.S.: You can use the model from last week's exercise as a starting point.)\n",
        "2. Add L2 regularization to the model and observe how it affects the training and validation performance (Check the Adam optimizer documentation to find out how to add this regularization). Compare the results with the unregularized model.\n",
        "3. Add dropout to the model and repeat the training process (check the PyTorch documentation to find out how to add this regularization - you do not need to implement it yourself). Compare the results with the previous models.\n",
        "\n",
        "For each step, make sure to copy the relevant code snippets into a new cell, instead of modifying the existing code. This way, we can keep track of the different versions of the model and their performances.\n",
        "\n",
        "You might want to look up the documentation for implementing these techniques in PyTorch.\n",
        "\n",
        "For each regularization technique, explain how it impacts the model's performance and generalization. Which combination of techniques works best for this dataset? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download our data again:\n",
        "!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "!rm -rf ./sample_data/\n",
        "!unzip -qq Clean_LiTS.zip\n",
        "!rm ./Clean_LiTS.zip"
      ],
      "metadata": {
        "id": "GcTcbYRPpxaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77424ebb-5d07-4033-fe3d-ae9ff8ef0d6e"
      },
      "id": "GcTcbYRPpxaF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
            "From (redirected): https://drive.google.com/uc?id=1TItTaso19GFTPdDnynVnqJvHsCm_RGlI&confirm=t&uuid=8a9089b1-a2b5-4267-be84-18d2cdca27e6\n",
            "To: /content/Clean_LiTS.zip\n",
            "100% 2.56G/2.56G [00:31<00:00, 80.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f91bab93",
      "metadata": {
        "id": "f91bab93"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bdaccb5c",
      "metadata": {
        "id": "bdaccb5c"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Insert your model here\n",
        "class YourModel(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Note that this model only has to work for us to count the task as completed.\n",
        "    You can build any model you like here. We made a tiny CNN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_classes):\n",
        "        in_channels = 1\n",
        "        out_classes = 3\n",
        "        super(YourModel, self).__init__()\n",
        "        #define convolutional layers with 'same' padding to maintain spatial dimensions\n",
        "        # Changed in_channels from 1 to 3 to match the original images from LiTS_Dataset\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (5, 5), padding='same')\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3), padding='same')\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3), padding='same')\n",
        "        #define final fully connected layer\n",
        "        # With 'same' padding, the spatial dimensions remain 256x256 after each conv layer.\n",
        "        # So, the output of conv3 will be 32 channels * 256 * 256.\n",
        "        self.fc1 = torch.nn.Linear(in_features = 32 * 256 * 256, out_features = 3)\n",
        "        # Initialize activation functions as model attributes\n",
        "        self.relu_activation = torch.nn.ReLU()\n",
        "        self.swish_activation = torch.nn.SiLU()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "      x = self.conv1(x)\n",
        "      x = self.swish_activation(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.swish_activation(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.swish_activation(x)\n",
        "      x = x.flatten(start_dim = 1)\n",
        "      x = self.fc1(x)\n",
        "\n",
        "\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5efe25fa",
      "metadata": {
        "id": "5efe25fa"
      },
      "outputs": [],
      "source": [
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "    #optimizer = your_optimizer\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9e04157a",
      "metadata": {
        "id": "9e04157a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e046fe78-b7d9-4bc1-82dd-80d9cb6422b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.0978\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 0.4134\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.8119\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.7490\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.7868\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.3287\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.5437\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.7141\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.4135\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.3003\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.3799\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.3251\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.3008\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.3232\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.5047\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.4132\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.2951\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.4433\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.3673\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.1533\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.2060\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.6639\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.3890\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.2172\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.3527\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.2872\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.4191\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.2322\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.1066\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.1930\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.3631\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.3010\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.2644\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.1136\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.3598\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.2267\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.4786\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.1757\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.1856\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.3748\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.1576\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.0798\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.0167\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.0709\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.0518\n",
            "Epoch: 1,\t Validation Loss: 0.6135,\t Accuracy: 0.7690\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.1381\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.0910\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.1714\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.1005\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.0605\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.3459\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.2935\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.1666\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.0839\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.1255\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.0706\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.0851\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.2923\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.0242\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.1799\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.3852\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.0924\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.1092\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.1819\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.1484\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.0172\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.0285\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.0568\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.0686\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.0327\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.2424\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.1607\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.0402\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.4058\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.0455\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.0886\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.2035\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.0951\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.1294\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.1042\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.1069\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.0565\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.1104\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.0289\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.0979\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.0759\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.0057\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.2886\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.0212\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.0749\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.0746\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.0381\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.1307\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.2090\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.0801\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.0015\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.0259\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.2373\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.0486\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.1961\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.0131\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.0708\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.0212\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.0844\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.0151\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.0774\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 0.0422\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.0087\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.0085\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.2126\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.0979\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.2035\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.1659\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.0470\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.0235\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.1333\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.0217\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.0418\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.0286\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.0091\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.1366\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.0407\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.1169\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.1377\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.0129\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.0096\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.0051\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.0332\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.1291\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.0534\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.1492\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.0636\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.0253\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.0326\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.6226\n",
            "Epoch: 3,\t Validation Loss: 0.7273,\t Accuracy: 0.8006\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.1062\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.1584\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.0148\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.0725\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.0250\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.0960\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.0029\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.0234\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.0064\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.0840\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.0637\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.0055\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.0123\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.2061\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.1389\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.1475\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.0058\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.0974\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.0013\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.0102\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.0444\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.0045\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.0074\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.0172\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.0811\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.0265\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.0069\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.0048\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.0465\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.0376\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.0078\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.0055\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.0119\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.0016\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.0276\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.0049\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.0044\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.0050\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.0038\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.0160\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.0600\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.0048\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.0290\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.0018\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.0758\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.0968\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.0169\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.0004\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.0076\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.0078\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.0010\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.0043\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.0029\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.0115\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.0188\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.0173\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.0670\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.0583\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.0044\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.0314\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.0158\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.0129\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.0480\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.0431\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.0164\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.0073\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.0025\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.0905\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.0096\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.0581\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.0184\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.0023\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.0472\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.0401\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.0044\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.0163\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.0241\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.0057\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.0081\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.0144\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.0235\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.0289\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.0280\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.0550\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.3487\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.0564\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.0079\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.0010\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.3182\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.1668\n",
            "Epoch: 5,\t Validation Loss: 1.1674,\t Accuracy: 0.7776\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.0011\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.0293\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.0147\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.0346\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.2044\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.1640\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.0230\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.0105\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.0099\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.1472\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.4066\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.0401\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.0072\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.0002\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.0790\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.0010\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.0111\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.0137\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.0145\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.0069\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.0368\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.0105\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.0008\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.2825\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.0960\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.0997\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.0041\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.0070\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.0142\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.0047\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.0009\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.0003\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.0863\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.0650\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.0047\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.0010\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.0010\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.0541\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.0032\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.0014\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.0243\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.0046\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.0088\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.1296\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.0006\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.0060\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.0552\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.0003\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.0091\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.1795\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.0020\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.0141\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.0782\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.0186\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.0412\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.0058\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.0052\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.0129\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.0017\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.0036\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.0514\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.2544\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.0076\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.0237\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.0030\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.0789\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.0010\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.0029\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.0145\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.0244\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.0002\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.0108\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.0185\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.0121\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.0053\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.0231\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.0064\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.0041\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.2825\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.0527\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.0060\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.2862\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.0101\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.0167\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.0047\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.0001\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.0016\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.0033\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.0035\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.1601\n",
            "Epoch: 7,\t Validation Loss: 1.1337,\t Accuracy: 0.7677\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.0357\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.0093\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.0002\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.0004\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.0006\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.0078\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.0011\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.1441\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.0038\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.0075\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.0013\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.0105\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.0019\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.0033\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.0313\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.0743\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.0172\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.0196\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.0503\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.0005\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.0048\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.1742\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.0133\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.0050\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.0593\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.0020\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.0512\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.1421\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.0073\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.0480\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.0853\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.0033\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.0694\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.0025\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.4509\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.0164\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.0024\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.0049\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.0118\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.0034\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.0027\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.0040\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.0016\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.0253\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.0036\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.1629\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.0006\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.0633\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.0032\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.0049\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.0377\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.0003\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.0017\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.0022\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.1732\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.0815\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.0011\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.0085\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.0089\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.2066\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.0011\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.0007\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.0012\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.0026\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.0005\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.0020\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.0010\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.0515\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.0134\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.0012\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.0005\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.0600\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.0079\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.0004\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.0007\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.0091\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.3091\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.0795\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.0247\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.0052\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.0048\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.0092\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.0128\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.0071\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.0024\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.0010\n",
            "Epoch: 9,\t Validation Loss: 1.2735,\t Accuracy: 0.7555\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.0007\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.0006\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.0041\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.0764\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.0003\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.0026\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.0015\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.0426\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.0753\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.0015\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.0009\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.0090\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.0027\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.0020\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.1072\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.0010\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.4553\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.0133\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.0184\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.0470\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.0069\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.0006\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.0002\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.2253\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.2523\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.0263\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.0024\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.0127\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.0270\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.0006\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.0060\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.0008\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.0090\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.0007\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.0010\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.0061\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.0010\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.0499\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.0024\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.0149\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.0103\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.0065\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.0031\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.0129\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.0015\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.0119\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.0043\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.0121\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.0832\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.0022\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.0027\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.0125\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.0117\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.0018\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.0049\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.0011\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.0010\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.0024\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.0013\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.0033\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.0070\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.0179\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.0017\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.0001\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.0021\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.0548\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.1438\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.0090\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.0005\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.0037\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.0039\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.0006\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.0048\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.0051\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.0032\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.0037\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.0020\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.0063\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.0062\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.0011\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.0011\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.0535\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.0025\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.0003\n",
            "Epoch: 11,\t Validation Loss: 1.4123,\t Accuracy: 0.7558\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.0001\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.0016\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.0004\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.0009\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.0158\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.0023\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.0131\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.0047\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.0055\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.0034\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.0012\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.0088\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.0018\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.0022\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.0001\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.0041\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.0855\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.0004\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.0002\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.0060\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.0003\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.0803\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.0026\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.0053\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.0007\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.0010\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.0142\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.0029\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.0000\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.0167\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.0008\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.0067\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.0024\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.0011\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.0038\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.0068\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.0005\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.0063\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.0001\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.0035\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.0587\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.0017\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.0021\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.0088\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.0835\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.0112\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.0007\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.0006\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.0077\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.0296\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.0021\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.0054\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.1025\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.0054\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.0006\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.2128\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.0037\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.0045\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.0008\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.0009\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.0074\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.1024\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.0014\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.0147\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.0036\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.0003\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.0505\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.0230\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.0715\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.0601\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.0034\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.0002\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.0083\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.0006\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.0007\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.0004\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.2204\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.0372\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.0247\n",
            "Epoch: 13,\t Validation Loss: 1.3749,\t Accuracy: 0.7578\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.0846\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.0075\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.0008\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.0029\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.0020\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.0033\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.0044\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.0016\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.0302\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.0063\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.0032\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.2549\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.0018\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.0002\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.0114\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.0596\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.0117\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.0376\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.0077\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.0000\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.0018\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.0019\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.0009\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.0003\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.1328\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.0997\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.0008\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.0148\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.0004\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.0022\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.0031\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.0001\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.0006\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.0024\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.0028\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.0005\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.0008\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.0165\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.0019\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.0513\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.0037\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.1113\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.0396\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.0015\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.0020\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.0018\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.0105\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.0153\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.0062\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.0123\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.0033\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.2651\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.0013\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.0276\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.0006\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.0068\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.0050\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.0448\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.0050\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.0011\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.0008\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.0023\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.0009\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.0004\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.1134\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.0041\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.0211\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.0003\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.0044\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.0012\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.0014\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.0005\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.0051\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.0353\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.0010\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.1701\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.0020\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.0002\n",
            "Epoch: 15,\t Validation Loss: 1.5663,\t Accuracy: 0.7460\n"
          ]
        }
      ],
      "source": [
        "#optimizer_norm = torch.optim.Adam(params = model.parameters(), lr = 1e-4)\n",
        "trained_model = train_loop(YourModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "10ff035d",
      "metadata": {
        "id": "10ff035d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f370170b-eb58-4b2c-ba3d-2f6f5c20b51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.4034,\t Accuracy: 0.6833\n"
          ]
        }
      ],
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605399f3",
      "metadata": {
        "id": "605399f3"
      },
      "source": [
        "### L2-Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1befc94e",
      "metadata": {
        "id": "1befc94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06587c6f-a36d-4776-fd9e-f08b8a9a9f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\t Step [1/2217]\t Loss: 1.0975\n",
            "Epoch [1/5]\t Step [51/2217]\t Loss: 0.8333\n",
            "Epoch [1/5]\t Step [101/2217]\t Loss: 0.8783\n",
            "Epoch [1/5]\t Step [151/2217]\t Loss: 0.7499\n",
            "Epoch [1/5]\t Step [201/2217]\t Loss: 0.6441\n",
            "Epoch [1/5]\t Step [251/2217]\t Loss: 0.1566\n",
            "Epoch [1/5]\t Step [301/2217]\t Loss: 0.5049\n",
            "Epoch [1/5]\t Step [351/2217]\t Loss: 0.2847\n",
            "Epoch [1/5]\t Step [401/2217]\t Loss: 0.5133\n",
            "Epoch [1/5]\t Step [451/2217]\t Loss: 0.7151\n",
            "Epoch [1/5]\t Step [501/2217]\t Loss: 0.4231\n",
            "Epoch [1/5]\t Step [551/2217]\t Loss: 0.4964\n",
            "Epoch [1/5]\t Step [601/2217]\t Loss: 0.2829\n",
            "Epoch [1/5]\t Step [651/2217]\t Loss: 0.7334\n",
            "Epoch [1/5]\t Step [701/2217]\t Loss: 0.3217\n",
            "Epoch [1/5]\t Step [751/2217]\t Loss: 0.5390\n",
            "Epoch [1/5]\t Step [801/2217]\t Loss: 0.2933\n",
            "Epoch [1/5]\t Step [851/2217]\t Loss: 0.3367\n",
            "Epoch [1/5]\t Step [901/2217]\t Loss: 0.5870\n",
            "Epoch [1/5]\t Step [951/2217]\t Loss: 0.2254\n",
            "Epoch [1/5]\t Step [1001/2217]\t Loss: 0.4438\n",
            "Epoch [1/5]\t Step [1051/2217]\t Loss: 0.4257\n",
            "Epoch [1/5]\t Step [1101/2217]\t Loss: 0.2089\n",
            "Epoch [1/5]\t Step [1151/2217]\t Loss: 0.3883\n",
            "Epoch [1/5]\t Step [1201/2217]\t Loss: 0.1249\n",
            "Epoch [1/5]\t Step [1251/2217]\t Loss: 0.1806\n",
            "Epoch [1/5]\t Step [1301/2217]\t Loss: 0.2112\n",
            "Epoch [1/5]\t Step [1351/2217]\t Loss: 0.2563\n",
            "Epoch [1/5]\t Step [1401/2217]\t Loss: 0.2304\n",
            "Epoch [1/5]\t Step [1451/2217]\t Loss: 0.0792\n",
            "Epoch [1/5]\t Step [1501/2217]\t Loss: 0.3784\n",
            "Epoch [1/5]\t Step [1551/2217]\t Loss: 0.3727\n",
            "Epoch [1/5]\t Step [1601/2217]\t Loss: 0.0834\n",
            "Epoch [1/5]\t Step [1651/2217]\t Loss: 0.1026\n",
            "Epoch [1/5]\t Step [1701/2217]\t Loss: 0.3945\n",
            "Epoch [1/5]\t Step [1751/2217]\t Loss: 0.1821\n",
            "Epoch [1/5]\t Step [1801/2217]\t Loss: 0.4725\n",
            "Epoch [1/5]\t Step [1851/2217]\t Loss: 0.3018\n",
            "Epoch [1/5]\t Step [1901/2217]\t Loss: 0.0589\n",
            "Epoch [1/5]\t Step [1951/2217]\t Loss: 0.2085\n",
            "Epoch [1/5]\t Step [2001/2217]\t Loss: 0.1842\n",
            "Epoch [1/5]\t Step [2051/2217]\t Loss: 0.4012\n",
            "Epoch [1/5]\t Step [2101/2217]\t Loss: 0.0885\n",
            "Epoch [1/5]\t Step [2151/2217]\t Loss: 0.0515\n",
            "Epoch [1/5]\t Step [2201/2217]\t Loss: 0.1106\n",
            "Epoch: 1,\t Validation Loss: 0.6459,\t Accuracy: 0.7779\n",
            "Epoch [2/5]\t Step [1/2217]\t Loss: 0.1366\n",
            "Epoch [2/5]\t Step [51/2217]\t Loss: 0.0624\n",
            "Epoch [2/5]\t Step [101/2217]\t Loss: 0.1341\n",
            "Epoch [2/5]\t Step [151/2217]\t Loss: 0.0911\n",
            "Epoch [2/5]\t Step [201/2217]\t Loss: 0.0649\n",
            "Epoch [2/5]\t Step [251/2217]\t Loss: 0.1042\n",
            "Epoch [2/5]\t Step [301/2217]\t Loss: 0.2088\n",
            "Epoch [2/5]\t Step [351/2217]\t Loss: 0.0965\n",
            "Epoch [2/5]\t Step [401/2217]\t Loss: 0.3120\n",
            "Epoch [2/5]\t Step [451/2217]\t Loss: 0.1419\n",
            "Epoch [2/5]\t Step [501/2217]\t Loss: 0.0782\n",
            "Epoch [2/5]\t Step [551/2217]\t Loss: 0.0438\n",
            "Epoch [2/5]\t Step [601/2217]\t Loss: 0.2613\n",
            "Epoch [2/5]\t Step [651/2217]\t Loss: 0.1995\n",
            "Epoch [2/5]\t Step [701/2217]\t Loss: 0.1736\n",
            "Epoch [2/5]\t Step [751/2217]\t Loss: 0.2474\n",
            "Epoch [2/5]\t Step [801/2217]\t Loss: 0.0385\n",
            "Epoch [2/5]\t Step [851/2217]\t Loss: 0.1795\n",
            "Epoch [2/5]\t Step [901/2217]\t Loss: 0.0624\n",
            "Epoch [2/5]\t Step [951/2217]\t Loss: 0.0922\n",
            "Epoch [2/5]\t Step [1001/2217]\t Loss: 0.0226\n",
            "Epoch [2/5]\t Step [1051/2217]\t Loss: 0.1595\n",
            "Epoch [2/5]\t Step [1101/2217]\t Loss: 0.5601\n",
            "Epoch [2/5]\t Step [1151/2217]\t Loss: 0.0803\n",
            "Epoch [2/5]\t Step [1201/2217]\t Loss: 0.2133\n",
            "Epoch [2/5]\t Step [1251/2217]\t Loss: 0.0623\n",
            "Epoch [2/5]\t Step [1301/2217]\t Loss: 0.0442\n",
            "Epoch [2/5]\t Step [1351/2217]\t Loss: 0.3955\n",
            "Epoch [2/5]\t Step [1401/2217]\t Loss: 0.2010\n",
            "Epoch [2/5]\t Step [1451/2217]\t Loss: 0.1524\n",
            "Epoch [2/5]\t Step [1501/2217]\t Loss: 0.0762\n",
            "Epoch [2/5]\t Step [1551/2217]\t Loss: 0.0695\n",
            "Epoch [2/5]\t Step [1601/2217]\t Loss: 0.2971\n",
            "Epoch [2/5]\t Step [1651/2217]\t Loss: 0.0670\n",
            "Epoch [2/5]\t Step [1701/2217]\t Loss: 0.0937\n",
            "Epoch [2/5]\t Step [1751/2217]\t Loss: 0.4561\n",
            "Epoch [2/5]\t Step [1801/2217]\t Loss: 0.0253\n",
            "Epoch [2/5]\t Step [1851/2217]\t Loss: 0.1225\n",
            "Epoch [2/5]\t Step [1901/2217]\t Loss: 0.1545\n",
            "Epoch [2/5]\t Step [1951/2217]\t Loss: 0.0868\n",
            "Epoch [2/5]\t Step [2001/2217]\t Loss: 0.1482\n",
            "Epoch [2/5]\t Step [2051/2217]\t Loss: 0.0896\n",
            "Epoch [2/5]\t Step [2101/2217]\t Loss: 0.0424\n",
            "Epoch [2/5]\t Step [2151/2217]\t Loss: 0.0423\n",
            "Epoch [2/5]\t Step [2201/2217]\t Loss: 0.0148\n",
            "Epoch [3/5]\t Step [1/2217]\t Loss: 0.1630\n",
            "Epoch [3/5]\t Step [51/2217]\t Loss: 0.0561\n",
            "Epoch [3/5]\t Step [101/2217]\t Loss: 0.0403\n",
            "Epoch [3/5]\t Step [151/2217]\t Loss: 0.1450\n",
            "Epoch [3/5]\t Step [201/2217]\t Loss: 0.1379\n",
            "Epoch [3/5]\t Step [251/2217]\t Loss: 0.0570\n",
            "Epoch [3/5]\t Step [301/2217]\t Loss: 0.0102\n",
            "Epoch [3/5]\t Step [351/2217]\t Loss: 0.0230\n",
            "Epoch [3/5]\t Step [401/2217]\t Loss: 0.0121\n",
            "Epoch [3/5]\t Step [451/2217]\t Loss: 0.0340\n",
            "Epoch [3/5]\t Step [501/2217]\t Loss: 0.1395\n",
            "Epoch [3/5]\t Step [551/2217]\t Loss: 0.0385\n",
            "Epoch [3/5]\t Step [601/2217]\t Loss: 0.0421\n",
            "Epoch [3/5]\t Step [651/2217]\t Loss: 0.0167\n",
            "Epoch [3/5]\t Step [701/2217]\t Loss: 0.0236\n",
            "Epoch [3/5]\t Step [751/2217]\t Loss: 0.0030\n",
            "Epoch [3/5]\t Step [801/2217]\t Loss: 0.0040\n",
            "Epoch [3/5]\t Step [851/2217]\t Loss: 0.0496\n",
            "Epoch [3/5]\t Step [901/2217]\t Loss: 0.1700\n",
            "Epoch [3/5]\t Step [951/2217]\t Loss: 0.0027\n",
            "Epoch [3/5]\t Step [1001/2217]\t Loss: 0.0103\n",
            "Epoch [3/5]\t Step [1051/2217]\t Loss: 0.1783\n",
            "Epoch [3/5]\t Step [1101/2217]\t Loss: 0.0375\n",
            "Epoch [3/5]\t Step [1151/2217]\t Loss: 0.0095\n",
            "Epoch [3/5]\t Step [1201/2217]\t Loss: 0.0067\n",
            "Epoch [3/5]\t Step [1251/2217]\t Loss: 0.0408\n",
            "Epoch [3/5]\t Step [1301/2217]\t Loss: 0.1477\n",
            "Epoch [3/5]\t Step [1351/2217]\t Loss: 0.1183\n",
            "Epoch [3/5]\t Step [1401/2217]\t Loss: 0.0077\n",
            "Epoch [3/5]\t Step [1451/2217]\t Loss: 0.0917\n",
            "Epoch [3/5]\t Step [1501/2217]\t Loss: 0.3029\n",
            "Epoch [3/5]\t Step [1551/2217]\t Loss: 0.1281\n",
            "Epoch [3/5]\t Step [1601/2217]\t Loss: 0.1959\n",
            "Epoch [3/5]\t Step [1651/2217]\t Loss: 0.1393\n",
            "Epoch [3/5]\t Step [1701/2217]\t Loss: 0.0456\n",
            "Epoch [3/5]\t Step [1751/2217]\t Loss: 0.0223\n",
            "Epoch [3/5]\t Step [1801/2217]\t Loss: 0.0314\n",
            "Epoch [3/5]\t Step [1851/2217]\t Loss: 0.2123\n",
            "Epoch [3/5]\t Step [1901/2217]\t Loss: 0.0361\n",
            "Epoch [3/5]\t Step [1951/2217]\t Loss: 0.0111\n",
            "Epoch [3/5]\t Step [2001/2217]\t Loss: 0.0299\n",
            "Epoch [3/5]\t Step [2051/2217]\t Loss: 0.0055\n",
            "Epoch [3/5]\t Step [2101/2217]\t Loss: 0.1631\n",
            "Epoch [3/5]\t Step [2151/2217]\t Loss: 0.0457\n",
            "Epoch [3/5]\t Step [2201/2217]\t Loss: 0.1687\n",
            "Epoch: 3,\t Validation Loss: 0.8335,\t Accuracy: 0.7887\n",
            "Epoch [4/5]\t Step [1/2217]\t Loss: 0.0926\n",
            "Epoch [4/5]\t Step [51/2217]\t Loss: 0.0086\n",
            "Epoch [4/5]\t Step [101/2217]\t Loss: 0.0246\n",
            "Epoch [4/5]\t Step [151/2217]\t Loss: 0.0093\n",
            "Epoch [4/5]\t Step [201/2217]\t Loss: 0.0022\n",
            "Epoch [4/5]\t Step [251/2217]\t Loss: 0.0679\n",
            "Epoch [4/5]\t Step [301/2217]\t Loss: 0.0617\n",
            "Epoch [4/5]\t Step [351/2217]\t Loss: 0.0036\n",
            "Epoch [4/5]\t Step [401/2217]\t Loss: 0.0530\n",
            "Epoch [4/5]\t Step [451/2217]\t Loss: 0.1765\n",
            "Epoch [4/5]\t Step [501/2217]\t Loss: 0.0193\n",
            "Epoch [4/5]\t Step [551/2217]\t Loss: 0.1203\n",
            "Epoch [4/5]\t Step [601/2217]\t Loss: 0.0088\n",
            "Epoch [4/5]\t Step [651/2217]\t Loss: 0.0332\n",
            "Epoch [4/5]\t Step [701/2217]\t Loss: 0.0857\n",
            "Epoch [4/5]\t Step [751/2217]\t Loss: 0.0176\n",
            "Epoch [4/5]\t Step [801/2217]\t Loss: 0.0150\n",
            "Epoch [4/5]\t Step [851/2217]\t Loss: 0.0316\n",
            "Epoch [4/5]\t Step [901/2217]\t Loss: 0.1728\n",
            "Epoch [4/5]\t Step [951/2217]\t Loss: 0.0127\n",
            "Epoch [4/5]\t Step [1001/2217]\t Loss: 0.0162\n",
            "Epoch [4/5]\t Step [1051/2217]\t Loss: 0.0795\n",
            "Epoch [4/5]\t Step [1101/2217]\t Loss: 0.3569\n",
            "Epoch [4/5]\t Step [1151/2217]\t Loss: 0.0064\n",
            "Epoch [4/5]\t Step [1201/2217]\t Loss: 0.0125\n",
            "Epoch [4/5]\t Step [1251/2217]\t Loss: 0.0061\n",
            "Epoch [4/5]\t Step [1301/2217]\t Loss: 0.0142\n",
            "Epoch [4/5]\t Step [1351/2217]\t Loss: 0.0522\n",
            "Epoch [4/5]\t Step [1401/2217]\t Loss: 0.1399\n",
            "Epoch [4/5]\t Step [1451/2217]\t Loss: 0.1057\n",
            "Epoch [4/5]\t Step [1501/2217]\t Loss: 0.2714\n",
            "Epoch [4/5]\t Step [1551/2217]\t Loss: 0.1123\n",
            "Epoch [4/5]\t Step [1601/2217]\t Loss: 0.0045\n",
            "Epoch [4/5]\t Step [1651/2217]\t Loss: 0.1386\n",
            "Epoch [4/5]\t Step [1701/2217]\t Loss: 0.0022\n",
            "Epoch [4/5]\t Step [1751/2217]\t Loss: 0.0389\n",
            "Epoch [4/5]\t Step [1801/2217]\t Loss: 0.0021\n",
            "Epoch [4/5]\t Step [1851/2217]\t Loss: 0.0537\n",
            "Epoch [4/5]\t Step [1901/2217]\t Loss: 0.0024\n",
            "Epoch [4/5]\t Step [1951/2217]\t Loss: 0.0136\n",
            "Epoch [4/5]\t Step [2001/2217]\t Loss: 0.1319\n",
            "Epoch [4/5]\t Step [2051/2217]\t Loss: 0.0108\n",
            "Epoch [4/5]\t Step [2101/2217]\t Loss: 0.1875\n",
            "Epoch [4/5]\t Step [2151/2217]\t Loss: 0.0002\n",
            "Epoch [4/5]\t Step [2201/2217]\t Loss: 0.0660\n",
            "Epoch [5/5]\t Step [1/2217]\t Loss: 0.0045\n",
            "Epoch [5/5]\t Step [51/2217]\t Loss: 0.0038\n",
            "Epoch [5/5]\t Step [101/2217]\t Loss: 0.0010\n",
            "Epoch [5/5]\t Step [151/2217]\t Loss: 0.0470\n",
            "Epoch [5/5]\t Step [201/2217]\t Loss: 0.0032\n",
            "Epoch [5/5]\t Step [251/2217]\t Loss: 0.0002\n",
            "Epoch [5/5]\t Step [301/2217]\t Loss: 0.0124\n",
            "Epoch [5/5]\t Step [351/2217]\t Loss: 0.2192\n",
            "Epoch [5/5]\t Step [401/2217]\t Loss: 0.0009\n",
            "Epoch [5/5]\t Step [451/2217]\t Loss: 0.0017\n",
            "Epoch [5/5]\t Step [501/2217]\t Loss: 0.0189\n",
            "Epoch [5/5]\t Step [551/2217]\t Loss: 0.0262\n",
            "Epoch [5/5]\t Step [601/2217]\t Loss: 0.0838\n",
            "Epoch [5/5]\t Step [651/2217]\t Loss: 0.2236\n",
            "Epoch [5/5]\t Step [701/2217]\t Loss: 0.0313\n",
            "Epoch [5/5]\t Step [751/2217]\t Loss: 0.0047\n",
            "Epoch [5/5]\t Step [801/2217]\t Loss: 0.1017\n",
            "Epoch [5/5]\t Step [851/2217]\t Loss: 0.0013\n",
            "Epoch [5/5]\t Step [901/2217]\t Loss: 0.0124\n",
            "Epoch [5/5]\t Step [951/2217]\t Loss: 0.0134\n",
            "Epoch [5/5]\t Step [1001/2217]\t Loss: 0.0047\n",
            "Epoch [5/5]\t Step [1051/2217]\t Loss: 0.0845\n",
            "Epoch [5/5]\t Step [1101/2217]\t Loss: 0.0414\n",
            "Epoch [5/5]\t Step [1151/2217]\t Loss: 0.0105\n",
            "Epoch [5/5]\t Step [1201/2217]\t Loss: 0.0276\n",
            "Epoch [5/5]\t Step [1251/2217]\t Loss: 0.3217\n",
            "Epoch [5/5]\t Step [1301/2217]\t Loss: 0.3221\n",
            "Epoch [5/5]\t Step [1351/2217]\t Loss: 0.0049\n",
            "Epoch [5/5]\t Step [1401/2217]\t Loss: 0.2924\n",
            "Epoch [5/5]\t Step [1451/2217]\t Loss: 0.3104\n",
            "Epoch [5/5]\t Step [1501/2217]\t Loss: 0.0062\n",
            "Epoch [5/5]\t Step [1551/2217]\t Loss: 0.0681\n",
            "Epoch [5/5]\t Step [1601/2217]\t Loss: 0.0030\n",
            "Epoch [5/5]\t Step [1651/2217]\t Loss: 0.0314\n",
            "Epoch [5/5]\t Step [1701/2217]\t Loss: 0.0677\n",
            "Epoch [5/5]\t Step [1751/2217]\t Loss: 0.0142\n",
            "Epoch [5/5]\t Step [1801/2217]\t Loss: 0.0014\n",
            "Epoch [5/5]\t Step [1851/2217]\t Loss: 0.2614\n",
            "Epoch [5/5]\t Step [1901/2217]\t Loss: 0.0284\n",
            "Epoch [5/5]\t Step [1951/2217]\t Loss: 0.0034\n",
            "Epoch [5/5]\t Step [2001/2217]\t Loss: 0.0271\n",
            "Epoch [5/5]\t Step [2051/2217]\t Loss: 0.0093\n",
            "Epoch [5/5]\t Step [2101/2217]\t Loss: 0.1506\n",
            "Epoch [5/5]\t Step [2151/2217]\t Loss: 0.2153\n",
            "Epoch [5/5]\t Step [2201/2217]\t Loss: 0.0040\n",
            "Epoch: 5,\t Validation Loss: 1.0202,\t Accuracy: 0.7832\n",
            "Test Loss: 1.8736,\t Accuracy: 0.6935\n"
          ]
        }
      ],
      "source": [
        "# add L2 Regularization to the model \"yourmodel\"\n",
        "\n",
        "# I wanted to change the train_loop, but since we are not supposed to modify the thing, here everything copied again\n",
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop_L2(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "    #optimizer = your_optimizer\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay = 1e-5)\n",
        "\n",
        "    num_epochs = 5 # I do NOT HAVE TIME, for waiting 30 minutes. I changed it therefore from 15 to 5 epochs!\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model_L2 = train_loop_L2(YourModel)\n",
        "\n",
        "trained_model_L2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_L2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e200dbd0",
      "metadata": {
        "id": "e200dbd0"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "941fe957"
      },
      "source": [
        "class YourModelWithDropout(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_classes):\n",
        "        self.dropout_rate=0.5\n",
        "        in_channels = 1\n",
        "        out_classes = 3\n",
        "        super(YourModelWithDropout, self).__init__()\n",
        "        # Define convolutional layers with 'same' padding to maintain spatial dimensions\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (5, 5), padding='same')\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3), padding='same')\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3), padding='same')\n",
        "\n",
        "        # Add a dropout layer after the convolutional blocks\n",
        "        self.dropout = torch.nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "        # Define final fully connected layer\n",
        "        self.fc1 = torch.nn.Linear(in_features = 32 * 256 * 256, out_features = 3)\n",
        "\n",
        "        # Initialize activation functions as model attributes\n",
        "        self.relu_activation = torch.nn.ReLU()\n",
        "        self.swish_activation = torch.nn.SiLU()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv1(x)\n",
        "        x = self.swish_activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.swish_activation(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.swish_activation(x)\n",
        "\n",
        "        # Apply dropout after convolutional layers and before flattening\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.flatten(start_dim = 1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n"
      ],
      "id": "941fe957",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "478bde61",
        "outputId": "4208eda5-9005-44a5-c9d7-5f7438f4ddbd"
      },
      "source": [
        "# Train the model with dropout using the flexible train_loop\n",
        "trained_model_dropout = train_loop_L2(YourModelWithDropout)\n"
      ],
      "id": "478bde61",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\t Step [1/2217]\t Loss: 1.1078\n",
            "Epoch [1/5]\t Step [51/2217]\t Loss: 0.8801\n",
            "Epoch [1/5]\t Step [101/2217]\t Loss: 0.1968\n",
            "Epoch [1/5]\t Step [151/2217]\t Loss: 0.8158\n",
            "Epoch [1/5]\t Step [201/2217]\t Loss: 0.9719\n",
            "Epoch [1/5]\t Step [251/2217]\t Loss: 0.3495\n",
            "Epoch [1/5]\t Step [301/2217]\t Loss: 0.2919\n",
            "Epoch [1/5]\t Step [351/2217]\t Loss: 0.4899\n",
            "Epoch [1/5]\t Step [401/2217]\t Loss: 0.7266\n",
            "Epoch [1/5]\t Step [451/2217]\t Loss: 0.5094\n",
            "Epoch [1/5]\t Step [501/2217]\t Loss: 0.2545\n",
            "Epoch [1/5]\t Step [551/2217]\t Loss: 0.6073\n",
            "Epoch [1/5]\t Step [601/2217]\t Loss: 0.4644\n",
            "Epoch [1/5]\t Step [651/2217]\t Loss: 0.2848\n",
            "Epoch [1/5]\t Step [701/2217]\t Loss: 0.4407\n",
            "Epoch [1/5]\t Step [751/2217]\t Loss: 0.5009\n",
            "Epoch [1/5]\t Step [801/2217]\t Loss: 0.3964\n",
            "Epoch [1/5]\t Step [851/2217]\t Loss: 0.2120\n",
            "Epoch [1/5]\t Step [901/2217]\t Loss: 0.3530\n",
            "Epoch [1/5]\t Step [951/2217]\t Loss: 0.1117\n",
            "Epoch [1/5]\t Step [1001/2217]\t Loss: 0.3479\n",
            "Epoch [1/5]\t Step [1051/2217]\t Loss: 0.3504\n",
            "Epoch [1/5]\t Step [1101/2217]\t Loss: 0.1661\n",
            "Epoch [1/5]\t Step [1151/2217]\t Loss: 0.3544\n",
            "Epoch [1/5]\t Step [1201/2217]\t Loss: 0.3332\n",
            "Epoch [1/5]\t Step [1251/2217]\t Loss: 0.2377\n",
            "Epoch [1/5]\t Step [1301/2217]\t Loss: 0.4748\n",
            "Epoch [1/5]\t Step [1351/2217]\t Loss: 0.1728\n",
            "Epoch [1/5]\t Step [1401/2217]\t Loss: 0.6455\n",
            "Epoch [1/5]\t Step [1451/2217]\t Loss: 0.2134\n",
            "Epoch [1/5]\t Step [1501/2217]\t Loss: 0.1252\n",
            "Epoch [1/5]\t Step [1551/2217]\t Loss: 0.2982\n",
            "Epoch [1/5]\t Step [1601/2217]\t Loss: 0.1625\n",
            "Epoch [1/5]\t Step [1651/2217]\t Loss: 0.1247\n",
            "Epoch [1/5]\t Step [1701/2217]\t Loss: 0.2982\n",
            "Epoch [1/5]\t Step [1751/2217]\t Loss: 0.1963\n",
            "Epoch [1/5]\t Step [1801/2217]\t Loss: 0.0920\n",
            "Epoch [1/5]\t Step [1851/2217]\t Loss: 0.2542\n",
            "Epoch [1/5]\t Step [1901/2217]\t Loss: 0.2619\n",
            "Epoch [1/5]\t Step [1951/2217]\t Loss: 0.2109\n",
            "Epoch [1/5]\t Step [2001/2217]\t Loss: 0.2048\n",
            "Epoch [1/5]\t Step [2051/2217]\t Loss: 0.1235\n",
            "Epoch [1/5]\t Step [2101/2217]\t Loss: 0.0827\n",
            "Epoch [1/5]\t Step [2151/2217]\t Loss: 0.1822\n",
            "Epoch [1/5]\t Step [2201/2217]\t Loss: 0.1220\n",
            "Epoch: 1,\t Validation Loss: 0.5034,\t Accuracy: 0.8059\n",
            "Epoch [2/5]\t Step [1/2217]\t Loss: 0.1483\n",
            "Epoch [2/5]\t Step [51/2217]\t Loss: 0.0570\n",
            "Epoch [2/5]\t Step [101/2217]\t Loss: 0.1124\n",
            "Epoch [2/5]\t Step [151/2217]\t Loss: 0.0672\n",
            "Epoch [2/5]\t Step [201/2217]\t Loss: 0.1702\n",
            "Epoch [2/5]\t Step [251/2217]\t Loss: 0.0789\n",
            "Epoch [2/5]\t Step [301/2217]\t Loss: 0.2650\n",
            "Epoch [2/5]\t Step [351/2217]\t Loss: 0.2061\n",
            "Epoch [2/5]\t Step [401/2217]\t Loss: 0.0326\n",
            "Epoch [2/5]\t Step [451/2217]\t Loss: 0.0377\n",
            "Epoch [2/5]\t Step [501/2217]\t Loss: 0.5257\n",
            "Epoch [2/5]\t Step [551/2217]\t Loss: 0.0706\n",
            "Epoch [2/5]\t Step [601/2217]\t Loss: 0.2989\n",
            "Epoch [2/5]\t Step [651/2217]\t Loss: 0.0803\n",
            "Epoch [2/5]\t Step [701/2217]\t Loss: 0.0660\n",
            "Epoch [2/5]\t Step [751/2217]\t Loss: 0.0376\n",
            "Epoch [2/5]\t Step [801/2217]\t Loss: 0.1345\n",
            "Epoch [2/5]\t Step [851/2217]\t Loss: 0.0907\n",
            "Epoch [2/5]\t Step [901/2217]\t Loss: 0.2133\n",
            "Epoch [2/5]\t Step [951/2217]\t Loss: 0.0824\n",
            "Epoch [2/5]\t Step [1001/2217]\t Loss: 0.0435\n",
            "Epoch [2/5]\t Step [1051/2217]\t Loss: 0.1747\n",
            "Epoch [2/5]\t Step [1101/2217]\t Loss: 0.0691\n",
            "Epoch [2/5]\t Step [1151/2217]\t Loss: 0.0114\n",
            "Epoch [2/5]\t Step [1201/2217]\t Loss: 0.1166\n",
            "Epoch [2/5]\t Step [1251/2217]\t Loss: 0.0874\n",
            "Epoch [2/5]\t Step [1301/2217]\t Loss: 0.0172\n",
            "Epoch [2/5]\t Step [1351/2217]\t Loss: 0.2299\n",
            "Epoch [2/5]\t Step [1401/2217]\t Loss: 0.0132\n",
            "Epoch [2/5]\t Step [1451/2217]\t Loss: 0.2279\n",
            "Epoch [2/5]\t Step [1501/2217]\t Loss: 0.0877\n",
            "Epoch [2/5]\t Step [1551/2217]\t Loss: 0.0571\n",
            "Epoch [2/5]\t Step [1601/2217]\t Loss: 0.0445\n",
            "Epoch [2/5]\t Step [1651/2217]\t Loss: 0.0734\n",
            "Epoch [2/5]\t Step [1701/2217]\t Loss: 0.0316\n",
            "Epoch [2/5]\t Step [1751/2217]\t Loss: 0.2474\n",
            "Epoch [2/5]\t Step [1801/2217]\t Loss: 0.2705\n",
            "Epoch [2/5]\t Step [1851/2217]\t Loss: 0.1155\n",
            "Epoch [2/5]\t Step [1901/2217]\t Loss: 0.0063\n",
            "Epoch [2/5]\t Step [1951/2217]\t Loss: 0.0176\n",
            "Epoch [2/5]\t Step [2001/2217]\t Loss: 0.0599\n",
            "Epoch [2/5]\t Step [2051/2217]\t Loss: 0.0120\n",
            "Epoch [2/5]\t Step [2101/2217]\t Loss: 0.0200\n",
            "Epoch [2/5]\t Step [2151/2217]\t Loss: 0.0682\n",
            "Epoch [2/5]\t Step [2201/2217]\t Loss: 0.0290\n",
            "Epoch [3/5]\t Step [1/2217]\t Loss: 0.2607\n",
            "Epoch [3/5]\t Step [51/2217]\t Loss: 0.1481\n",
            "Epoch [3/5]\t Step [101/2217]\t Loss: 0.0354\n",
            "Epoch [3/5]\t Step [151/2217]\t Loss: 0.0136\n",
            "Epoch [3/5]\t Step [201/2217]\t Loss: 0.0103\n",
            "Epoch [3/5]\t Step [251/2217]\t Loss: 0.0096\n",
            "Epoch [3/5]\t Step [301/2217]\t Loss: 0.0086\n",
            "Epoch [3/5]\t Step [351/2217]\t Loss: 0.0083\n",
            "Epoch [3/5]\t Step [401/2217]\t Loss: 0.0091\n",
            "Epoch [3/5]\t Step [451/2217]\t Loss: 0.1982\n",
            "Epoch [3/5]\t Step [501/2217]\t Loss: 0.1984\n",
            "Epoch [3/5]\t Step [551/2217]\t Loss: 0.0403\n",
            "Epoch [3/5]\t Step [601/2217]\t Loss: 0.1034\n",
            "Epoch [3/5]\t Step [651/2217]\t Loss: 0.0867\n",
            "Epoch [3/5]\t Step [701/2217]\t Loss: 0.1152\n",
            "Epoch [3/5]\t Step [751/2217]\t Loss: 0.1418\n",
            "Epoch [3/5]\t Step [801/2217]\t Loss: 0.0096\n",
            "Epoch [3/5]\t Step [851/2217]\t Loss: 0.0330\n",
            "Epoch [3/5]\t Step [901/2217]\t Loss: 0.0212\n",
            "Epoch [3/5]\t Step [951/2217]\t Loss: 0.0120\n",
            "Epoch [3/5]\t Step [1001/2217]\t Loss: 0.0740\n",
            "Epoch [3/5]\t Step [1051/2217]\t Loss: 0.0173\n",
            "Epoch [3/5]\t Step [1101/2217]\t Loss: 0.0372\n",
            "Epoch [3/5]\t Step [1151/2217]\t Loss: 0.0041\n",
            "Epoch [3/5]\t Step [1201/2217]\t Loss: 0.0061\n",
            "Epoch [3/5]\t Step [1251/2217]\t Loss: 0.0909\n",
            "Epoch [3/5]\t Step [1301/2217]\t Loss: 0.0045\n",
            "Epoch [3/5]\t Step [1351/2217]\t Loss: 0.0056\n",
            "Epoch [3/5]\t Step [1401/2217]\t Loss: 0.0262\n",
            "Epoch [3/5]\t Step [1451/2217]\t Loss: 0.0138\n",
            "Epoch [3/5]\t Step [1501/2217]\t Loss: 0.0900\n",
            "Epoch [3/5]\t Step [1551/2217]\t Loss: 0.1577\n",
            "Epoch [3/5]\t Step [1601/2217]\t Loss: 0.0072\n",
            "Epoch [3/5]\t Step [1651/2217]\t Loss: 0.0232\n",
            "Epoch [3/5]\t Step [1701/2217]\t Loss: 0.0830\n",
            "Epoch [3/5]\t Step [1751/2217]\t Loss: 0.0035\n",
            "Epoch [3/5]\t Step [1801/2217]\t Loss: 0.0036\n",
            "Epoch [3/5]\t Step [1851/2217]\t Loss: 0.0220\n",
            "Epoch [3/5]\t Step [1901/2217]\t Loss: 0.0407\n",
            "Epoch [3/5]\t Step [1951/2217]\t Loss: 0.0955\n",
            "Epoch [3/5]\t Step [2001/2217]\t Loss: 0.1071\n",
            "Epoch [3/5]\t Step [2051/2217]\t Loss: 0.2005\n",
            "Epoch [3/5]\t Step [2101/2217]\t Loss: 0.0543\n",
            "Epoch [3/5]\t Step [2151/2217]\t Loss: 0.0218\n",
            "Epoch [3/5]\t Step [2201/2217]\t Loss: 0.0667\n",
            "Epoch: 3,\t Validation Loss: 0.7632,\t Accuracy: 0.7947\n",
            "Epoch [4/5]\t Step [1/2217]\t Loss: 0.0900\n",
            "Epoch [4/5]\t Step [51/2217]\t Loss: 0.0138\n",
            "Epoch [4/5]\t Step [101/2217]\t Loss: 0.0405\n",
            "Epoch [4/5]\t Step [151/2217]\t Loss: 0.0194\n",
            "Epoch [4/5]\t Step [201/2217]\t Loss: 0.0156\n",
            "Epoch [4/5]\t Step [251/2217]\t Loss: 0.0307\n",
            "Epoch [4/5]\t Step [301/2217]\t Loss: 0.0373\n",
            "Epoch [4/5]\t Step [351/2217]\t Loss: 0.0180\n",
            "Epoch [4/5]\t Step [401/2217]\t Loss: 0.0327\n",
            "Epoch [4/5]\t Step [451/2217]\t Loss: 0.0150\n",
            "Epoch [4/5]\t Step [501/2217]\t Loss: 0.0196\n",
            "Epoch [4/5]\t Step [551/2217]\t Loss: 0.0228\n",
            "Epoch [4/5]\t Step [601/2217]\t Loss: 0.0003\n",
            "Epoch [4/5]\t Step [651/2217]\t Loss: 0.0807\n",
            "Epoch [4/5]\t Step [701/2217]\t Loss: 0.0169\n",
            "Epoch [4/5]\t Step [751/2217]\t Loss: 0.0039\n",
            "Epoch [4/5]\t Step [801/2217]\t Loss: 0.0347\n",
            "Epoch [4/5]\t Step [851/2217]\t Loss: 0.0160\n",
            "Epoch [4/5]\t Step [901/2217]\t Loss: 0.0478\n",
            "Epoch [4/5]\t Step [951/2217]\t Loss: 0.0021\n",
            "Epoch [4/5]\t Step [1001/2217]\t Loss: 0.2447\n",
            "Epoch [4/5]\t Step [1051/2217]\t Loss: 0.0055\n",
            "Epoch [4/5]\t Step [1101/2217]\t Loss: 0.0485\n",
            "Epoch [4/5]\t Step [1151/2217]\t Loss: 0.0426\n",
            "Epoch [4/5]\t Step [1201/2217]\t Loss: 0.0167\n",
            "Epoch [4/5]\t Step [1251/2217]\t Loss: 0.0694\n",
            "Epoch [4/5]\t Step [1301/2217]\t Loss: 0.0255\n",
            "Epoch [4/5]\t Step [1351/2217]\t Loss: 0.0172\n",
            "Epoch [4/5]\t Step [1401/2217]\t Loss: 0.0097\n",
            "Epoch [4/5]\t Step [1451/2217]\t Loss: 0.0763\n",
            "Epoch [4/5]\t Step [1501/2217]\t Loss: 0.0033\n",
            "Epoch [4/5]\t Step [1551/2217]\t Loss: 0.0055\n",
            "Epoch [4/5]\t Step [1601/2217]\t Loss: 0.3967\n",
            "Epoch [4/5]\t Step [1651/2217]\t Loss: 0.0121\n",
            "Epoch [4/5]\t Step [1701/2217]\t Loss: 0.0247\n",
            "Epoch [4/5]\t Step [1751/2217]\t Loss: 0.1958\n",
            "Epoch [4/5]\t Step [1801/2217]\t Loss: 0.0027\n",
            "Epoch [4/5]\t Step [1851/2217]\t Loss: 0.0089\n",
            "Epoch [4/5]\t Step [1901/2217]\t Loss: 0.0042\n",
            "Epoch [4/5]\t Step [1951/2217]\t Loss: 0.0101\n",
            "Epoch [4/5]\t Step [2001/2217]\t Loss: 0.0330\n",
            "Epoch [4/5]\t Step [2051/2217]\t Loss: 0.0098\n",
            "Epoch [4/5]\t Step [2101/2217]\t Loss: 0.2317\n",
            "Epoch [4/5]\t Step [2151/2217]\t Loss: 0.0199\n",
            "Epoch [4/5]\t Step [2201/2217]\t Loss: 0.0856\n",
            "Epoch [5/5]\t Step [1/2217]\t Loss: 0.0639\n",
            "Epoch [5/5]\t Step [51/2217]\t Loss: 0.0026\n",
            "Epoch [5/5]\t Step [101/2217]\t Loss: 0.0072\n",
            "Epoch [5/5]\t Step [151/2217]\t Loss: 0.0056\n",
            "Epoch [5/5]\t Step [201/2217]\t Loss: 0.0235\n",
            "Epoch [5/5]\t Step [251/2217]\t Loss: 0.0059\n",
            "Epoch [5/5]\t Step [301/2217]\t Loss: 0.2341\n",
            "Epoch [5/5]\t Step [351/2217]\t Loss: 0.0117\n",
            "Epoch [5/5]\t Step [401/2217]\t Loss: 0.0868\n",
            "Epoch [5/5]\t Step [451/2217]\t Loss: 0.0129\n",
            "Epoch [5/5]\t Step [501/2217]\t Loss: 0.1637\n",
            "Epoch [5/5]\t Step [551/2217]\t Loss: 0.0014\n",
            "Epoch [5/5]\t Step [601/2217]\t Loss: 0.0019\n",
            "Epoch [5/5]\t Step [651/2217]\t Loss: 0.0283\n",
            "Epoch [5/5]\t Step [701/2217]\t Loss: 0.0214\n",
            "Epoch [5/5]\t Step [751/2217]\t Loss: 0.0077\n",
            "Epoch [5/5]\t Step [801/2217]\t Loss: 0.0109\n",
            "Epoch [5/5]\t Step [851/2217]\t Loss: 0.0013\n",
            "Epoch [5/5]\t Step [901/2217]\t Loss: 0.0930\n",
            "Epoch [5/5]\t Step [951/2217]\t Loss: 0.0024\n",
            "Epoch [5/5]\t Step [1001/2217]\t Loss: 0.2093\n",
            "Epoch [5/5]\t Step [1051/2217]\t Loss: 0.0805\n",
            "Epoch [5/5]\t Step [1101/2217]\t Loss: 0.0940\n",
            "Epoch [5/5]\t Step [1151/2217]\t Loss: 0.0421\n",
            "Epoch [5/5]\t Step [1201/2217]\t Loss: 0.0122\n",
            "Epoch [5/5]\t Step [1251/2217]\t Loss: 0.0089\n",
            "Epoch [5/5]\t Step [1301/2217]\t Loss: 0.0088\n",
            "Epoch [5/5]\t Step [1351/2217]\t Loss: 0.1331\n",
            "Epoch [5/5]\t Step [1401/2217]\t Loss: 0.3058\n",
            "Epoch [5/5]\t Step [1451/2217]\t Loss: 0.0037\n",
            "Epoch [5/5]\t Step [1501/2217]\t Loss: 0.0026\n",
            "Epoch [5/5]\t Step [1551/2217]\t Loss: 0.0008\n",
            "Epoch [5/5]\t Step [1601/2217]\t Loss: 0.0009\n",
            "Epoch [5/5]\t Step [1651/2217]\t Loss: 0.1136\n",
            "Epoch [5/5]\t Step [1701/2217]\t Loss: 0.0015\n",
            "Epoch [5/5]\t Step [1751/2217]\t Loss: 0.0017\n",
            "Epoch [5/5]\t Step [1801/2217]\t Loss: 0.0052\n",
            "Epoch [5/5]\t Step [1851/2217]\t Loss: 0.0566\n",
            "Epoch [5/5]\t Step [1901/2217]\t Loss: 0.0023\n",
            "Epoch [5/5]\t Step [1951/2217]\t Loss: 0.1465\n",
            "Epoch [5/5]\t Step [2001/2217]\t Loss: 0.0010\n",
            "Epoch [5/5]\t Step [2051/2217]\t Loss: 0.2842\n",
            "Epoch [5/5]\t Step [2101/2217]\t Loss: 0.0055\n",
            "Epoch [5/5]\t Step [2151/2217]\t Loss: 0.0801\n",
            "Epoch [5/5]\t Step [2201/2217]\t Loss: 0.0306\n",
            "Epoch: 5,\t Validation Loss: 1.5537,\t Accuracy: 0.7720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "045a56fc",
        "outputId": "0ed6a02c-9067-43cc-e9d3-44d420dfd828"
      },
      "source": [
        "# Test the model trained with dropout using the test_loop function\n",
        "trained_model_dropout.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_dropout(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n"
      ],
      "id": "045a56fc",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.9281,\t Accuracy: 0.6975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a23885a",
      "metadata": {
        "id": "0a23885a"
      },
      "source": [
        "#### Chapter 3.3 - Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It was introduced to address the problem of internal covariate shift, which refers to the change in the distribution of layer inputs during training as the parameters of the previous layers change.\n",
        "\n",
        "**Intuition:**\n",
        "The idea behind Batch Normalization is to normalize the inputs to each layer so that they have a mean of 0 and a standard deviation of 1. This ensures that the inputs to each layer are on a similar scale, which helps the network learn faster and more effectively. By normalizing the inputs, Batch Normalization reduces the sensitivity of the network to the initialization of weights and allows for the use of higher learning rates.\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, Batch Normalization computes the mean and variance of the inputs.\n",
        "2. The inputs are then normalized using these statistics:\n",
        "    $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "    where $\\mu$ is the mean, $\\sigma^2$ is the variance, and $\\epsilon$ is a small constant added for numerical stability.\n",
        "3. To allow the network to learn the optimal scale and shift for the normalized inputs, two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), are introduced:\n",
        "    $y = \\gamma \\hat{x} + \\beta$\n",
        "\n",
        "**Problems it solves:**\n",
        "1. **Internal Covariate Shift:** By normalizing the inputs to each layer, Batch Normalization reduces the changes in the distribution of layer inputs during training, making the optimization process more stable.\n",
        "2. **Faster Training:** Normalized inputs allow for the use of higher learning rates, leading to faster convergence.\n",
        "3. **Regularization Effect:** Batch Normalization introduces some noise due to the mini-batch statistics, which acts as a form of regularization and reduces the need for other regularization techniques like Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff313393",
      "metadata": {
        "id": "ff313393"
      },
      "outputs": [],
      "source": [
        "# Here is how to add it into your model as a layer:\n",
        "\n",
        "bn = torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95af6dfc",
      "metadata": {
        "id": "95af6dfc"
      },
      "source": [
        "#### Chapter 3.4 - Modern Computer Vision Models\n",
        "\n",
        "AlexNet (original paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) is in many senses the grandfather of modern neural networks, being the first one to successfully combine multiple GPUs for training with a deep neural network. While it is no longer in use today, the lessons learned from AlexNet very much are, and multi-GPU setups and deep convolutional neural networks remain a staple of computer vision methods.\n",
        "\n",
        "Modern Computer Vision uses a number of different models, but perhaps none is as prolific as the original ResNet, in particular the ResNet-50. Even though it is far from the strongest model available today, its flexibility, modest size, and robust performance across tasks makes it a favorite, both in general computer vision and medical computer vision, where it is commonly used as the encoder in segmentation models (more on that later). The original paper (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has garnered almost 300'000 citations, and its descendants have dominated challenges and paper submissions in the field for a significant amount of time.\n",
        "\n",
        "**Task 2 (up to 6 points)**: Your task is to write one of these two modern models from scratch. The points are awarded for correctly implementing these models. You can choose your own difficulty here, and can earn fewer or more points, depending on which you feel more comfortable building. AlexNet requires only components that you have already seen last week - convolutions, pooling, and linear layers, while ResNet requires you to build skip connections and bottleneck blocks from scratch.\n",
        "\n",
        "Option 1 - AlexNet **(4 points)**:\n",
        "- Building the model **(2 points)**\n",
        "- You do not have to implement the parts where multiple GPUs are required\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "Option 2 - ResNet-50 **(7 points)**\n",
        "- Correctly implementing Skip Connections **(1 point)**\n",
        "- Correctly implementing Residual/Bottleneck Blocks **(3 points)**\n",
        "- Correctly building the ResNet from these Blocks **(1 point)**\n",
        "- For BatchNorm you are allowed to simply use the existing implementation\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "You must verify that your model actually trains and is capable of solving the classification task on LiTS 2017. You should be able to explain every piece of code to the tutors that grade your solution, so if you use any help in building the model (e.g. Chat-GPT, Cursor, etc.), be prepared to explain what code blocks do what, and why you implemented them in the specific way you did, and not any other. The points are awarded for programming *and* understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c7122f",
      "metadata": {
        "id": "62c7122f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet 50\n",
        "\n",
        "I assume we are not allowed to just use torchvision.models.resnet50 ?\n",
        "\n",
        "**Bottleneck layers**\n",
        "\n",
        "ResNet uses bottlenecks -> layer with few nodes compared to the previous layers.\n",
        "+ reduces computing time\n",
        "\n",
        "** skip connection **\n",
        "\n",
        "A skip connection connects the input and output layer, learning the difference between in and output. It basically makes sure, the network does not forget the original input it was learning. It sends the original input directly, deep into the network.\n",
        "\n",
        "Im following a tutorial of medium.com:\n",
        "https://medium.com/@karuneshu21/how-to-resnet-in-pytorch-9acb01f36cf5"
      ],
      "metadata": {
        "id": "8ZAPfr13jGvz"
      },
      "id": "8ZAPfr13jGvz"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# all nn libraries nn.layer, convs and loss functions\n",
        "import torch.nn as nn\n",
        "\n",
        "# Display Image\n",
        "from IPython.display import Image\n",
        "\n",
        "# visualisation\n",
        "!pip install torchview\n",
        "import torchvision\n",
        "from torchview import draw_graph\n",
        "\n",
        "\n",
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# set model parameters for the 50 version\n",
        "model_params= ([64,128,256,512],[3,4,6,3],4,True)\n",
        "#[64,128,256,512] -> channels in each intermediate block\n",
        "# [3,4,6,3] -> # repeatition for Bottlenecks in each block\n",
        "# 4 -> expansion_factor\n",
        "# True -> create Bottleneck layer statu\n",
        "\n",
        "# First, I create a class for the Bootleneck\n",
        "class Bottleneck(nn.Module):\n",
        "\n",
        "    def __init__(self,in_channels,intermediate_channels,expansion,is_Bottleneck,stride):\n",
        "\n",
        "        \"\"\"\n",
        "        Creates a Bottleneck with conv 1x1->3x3->1x1 layers.\n",
        "\n",
        "        Note:\n",
        "          1. Addition of feature maps occur at just before the final ReLU with the input feature maps\n",
        "          2. if input size is different from output, select projected mapping or else identity mapping.\n",
        "          3. if is_Bottleneck=False (3x3->3x3) are used else (1x1->3x3->1x1). Bottleneck is required for resnet-50/101/152\n",
        "        Args:\n",
        "            in_channels (int) : input channels to the Bottleneck\n",
        "            intermediate_channels (int) : number of channels to 3x3 conv\n",
        "            expansion (int) : factor by which the input #channels are increased\n",
        "            stride (int) : stride applied in the 3x3 conv. 2 for first Bottleneck of the block and 1 for remaining\n",
        "\n",
        "        Attributes:\n",
        "            Layer consisting of conv->batchnorm->relu\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(Bottleneck,self).__init__()\n",
        "\n",
        "        self.expansion = expansion\n",
        "        self.in_channels = in_channels\n",
        "        self.intermediate_channels = intermediate_channels\n",
        "        self.is_Bottleneck = is_Bottleneck\n",
        "\n",
        "        # i.e. if dim(x) == dim(F) => Identity function\n",
        "        if self.in_channels==self.intermediate_channels*self.expansion:\n",
        "            self.identity = True\n",
        "        else:\n",
        "            self.identity = False\n",
        "            projection_layer = []\n",
        "            projection_layer.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=stride, padding=0, bias=False ))\n",
        "            projection_layer.append(nn.BatchNorm2d(self.intermediate_channels*self.expansion))\n",
        "            # Only conv->BN and no ReLU\n",
        "            # projection_layer.append(nn.ReLU())\n",
        "            self.projection = nn.Sequential(*projection_layer)\n",
        "\n",
        "        # commonly used relu\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # is_Bottleneck = True for all ResNet 50+\n",
        "        if self.is_Bottleneck:\n",
        "            # bottleneck\n",
        "            # 1x1\n",
        "            self.conv1_1x1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False )\n",
        "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
        "\n",
        "            # 3x3\n",
        "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
        "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
        "\n",
        "            # 1x1\n",
        "            self.conv3_1x1 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False )\n",
        "            self.batchnorm3 = nn.BatchNorm2d( self.intermediate_channels*self.expansion )\n",
        "\n",
        "        else:\n",
        "            # basicblock\n",
        "            # 3x3\n",
        "            self.conv1_3x3 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
        "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
        "\n",
        "            # 3x3\n",
        "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False )\n",
        "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # input stored to be added before the final relu\n",
        "        in_x = x\n",
        "\n",
        "        if self.is_Bottleneck:\n",
        "            # conv1x1->BN->relu\n",
        "            x = self.relu(self.batchnorm1(self.conv1_1x1(x)))\n",
        "\n",
        "            # conv3x3->BN->relu\n",
        "            x = self.relu(self.batchnorm2(self.conv2_3x3(x)))\n",
        "\n",
        "            # conv1x1->BN\n",
        "            x = self.batchnorm3(self.conv3_1x1(x))\n",
        "\n",
        "        else:\n",
        "            # conv3x3->BN->relu\n",
        "            x = self.relu(self.batchnorm1(self.conv1_3x3(x)))\n",
        "\n",
        "            # conv3x3->BN\n",
        "            x = self.batchnorm2(self.conv2_3x3(x))\n",
        "\n",
        "\n",
        "        # identity or projected mapping\n",
        "        if self.identity:\n",
        "            x += in_x\n",
        "        else:\n",
        "            x += self.projection(in_x)\n",
        "\n",
        "        # final relu\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Bottleneck(64*4,64,4,stride=1)\n",
        "\n",
        "def test_Bottleneck():\n",
        "    x = torch.randn(1,64,112,112)\n",
        "    model = Bottleneck(64,64,4,True,2)\n",
        "    print(model(x).shape)\n",
        "    del model\n",
        "\n",
        "test_Bottleneck()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfH2nh1RjUoG",
        "outputId": "2459c2ef-4d6f-41d9-c3aa-76a3c1c15646"
      },
      "id": "bfH2nh1RjUoG",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchview\n",
            "  Downloading torchview-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchview) (0.21)\n",
            "Downloading torchview-0.2.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.7\n",
            "cuda\n",
            "torch.Size([1, 256, 56, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, resnet_variant,in_channels,num_classes):\n",
        "        \"\"\"\n",
        "        Creates the ResNet architecture based on the provided variant. 18/34/50/101 etc.\n",
        "        Based on the input parameters, define the channels list, repeatition list along with expansion factor(4) and stride(3/1)\n",
        "        using _make_blocks method, create a sequence of multiple Bottlenecks\n",
        "        Average Pool at the end before the FC layer\n",
        "\n",
        "        Args:\n",
        "            resnet_variant (list) : eg. [[64,128,256,512],[3,4,6,3],4,True]\n",
        "            in_channels (int) : image channels 1\n",
        "            num_classes (int) : output #classes\n",
        "\n",
        "        Attributes:\n",
        "            Layer consisting of conv->batchnorm->relu\n",
        "\n",
        "        \"\"\"\n",
        "        super(ResNet,self).__init__()\n",
        "        self.channels_list = resnet_variant[0]\n",
        "        self.repeatition_list = resnet_variant[1]\n",
        "        self.expansion = resnet_variant[2]\n",
        "        self.is_Bottleneck = resnet_variant[3]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False )\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        self.block1 = self._make_blocks( 64 , self.channels_list[0], self.repeatition_list[0], self.expansion, self.is_Bottleneck, stride=1 )\n",
        "        self.block2 = self._make_blocks( self.channels_list[0]*self.expansion , self.channels_list[1], self.repeatition_list[1], self.expansion, self.is_Bottleneck, stride=2 )\n",
        "        self.block3 = self._make_blocks( self.channels_list[1]*self.expansion , self.channels_list[2], self.repeatition_list[2], self.expansion, self.is_Bottleneck, stride=2 )\n",
        "        self.block4 = self._make_blocks( self.channels_list[2]*self.expansion , self.channels_list[3], self.repeatition_list[3], self.expansion, self.is_Bottleneck, stride=2 )\n",
        "\n",
        "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear( self.channels_list[3]*self.expansion , num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "\n",
        "        x = self.block2(x)\n",
        "\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = self.block4(x)\n",
        "\n",
        "        x = self.average_pool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_blocks(self,in_channels,intermediate_channels,num_repeat, expansion, is_Bottleneck, stride):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels : #channels of the Bottleneck input\n",
        "            intermediate_channels : #channels of the 3x3 in the Bottleneck\n",
        "            num_repeat : #Bottlenecks in the block\n",
        "            expansion : factor by which intermediate_channels are multiplied to create the output channels\n",
        "            is_Bottleneck : status if Bottleneck in required\n",
        "            stride : stride to be used in the first Bottleneck conv 3x3\n",
        "\n",
        "        Attributes:\n",
        "            Sequence of Bottleneck layers\n",
        "\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "\n",
        "        layers.append(Bottleneck(in_channels,intermediate_channels,expansion,is_Bottleneck,stride=stride))\n",
        "        for num in range(1,num_repeat):\n",
        "            layers.append(Bottleneck(intermediate_channels*expansion,intermediate_channels,expansion,is_Bottleneck,stride=1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P19I8Jkxpg2T"
      },
      "id": "P19I8Jkxpg2T",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we need to train ResNet on our dataset LiTs2017\n",
        "# define train loop:\n",
        "\n",
        "\n",
        "# add L2 Regularization to the model \"yourmodel\"\n",
        "\n",
        "# I wanted to change the train_loop, but since we are not supposed to modify the thing, here everything copied again\n",
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def ResNet_train_loop_L2(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(model_params,in_channels = 1, num_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "    #optimizer = your_optimizer\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay = 1e-5)\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "ResNet_trained_model_L2 = ResNet_train_loop_L2(ResNet)\n",
        "\n",
        "ResNet_trained_model_L2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = ResNet_trained_model_L2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzhq16ysryG4",
        "outputId": "9387aa51-3682-4122-afc0-14e11d96f0b5"
      },
      "id": "Tzhq16ysryG4",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\t Step [1/2217]\t Loss: 1.1623\n",
            "Epoch [1/5]\t Step [51/2217]\t Loss: 0.5063\n",
            "Epoch [1/5]\t Step [101/2217]\t Loss: 0.4296\n",
            "Epoch [1/5]\t Step [151/2217]\t Loss: 0.7780\n",
            "Epoch [1/5]\t Step [201/2217]\t Loss: 0.5455\n",
            "Epoch [1/5]\t Step [251/2217]\t Loss: 0.1844\n",
            "Epoch [1/5]\t Step [301/2217]\t Loss: 0.7579\n",
            "Epoch [1/5]\t Step [351/2217]\t Loss: 0.4367\n",
            "Epoch [1/5]\t Step [401/2217]\t Loss: 0.1403\n",
            "Epoch [1/5]\t Step [451/2217]\t Loss: 0.6293\n",
            "Epoch [1/5]\t Step [501/2217]\t Loss: 0.1979\n",
            "Epoch [1/5]\t Step [551/2217]\t Loss: 0.2181\n",
            "Epoch [1/5]\t Step [601/2217]\t Loss: 0.2484\n",
            "Epoch [1/5]\t Step [651/2217]\t Loss: 0.4334\n",
            "Epoch [1/5]\t Step [701/2217]\t Loss: 0.2242\n",
            "Epoch [1/5]\t Step [751/2217]\t Loss: 0.3987\n",
            "Epoch [1/5]\t Step [801/2217]\t Loss: 0.0793\n",
            "Epoch [1/5]\t Step [851/2217]\t Loss: 0.1317\n",
            "Epoch [1/5]\t Step [901/2217]\t Loss: 0.4233\n",
            "Epoch [1/5]\t Step [951/2217]\t Loss: 0.2115\n",
            "Epoch [1/5]\t Step [1001/2217]\t Loss: 0.1304\n",
            "Epoch [1/5]\t Step [1051/2217]\t Loss: 0.1488\n",
            "Epoch [1/5]\t Step [1101/2217]\t Loss: 0.2950\n",
            "Epoch [1/5]\t Step [1151/2217]\t Loss: 0.4269\n",
            "Epoch [1/5]\t Step [1201/2217]\t Loss: 0.1141\n",
            "Epoch [1/5]\t Step [1251/2217]\t Loss: 0.0322\n",
            "Epoch [1/5]\t Step [1301/2217]\t Loss: 0.1602\n",
            "Epoch [1/5]\t Step [1351/2217]\t Loss: 0.3983\n",
            "Epoch [1/5]\t Step [1401/2217]\t Loss: 0.0977\n",
            "Epoch [1/5]\t Step [1451/2217]\t Loss: 0.4047\n",
            "Epoch [1/5]\t Step [1501/2217]\t Loss: 0.1508\n",
            "Epoch [1/5]\t Step [1551/2217]\t Loss: 0.0238\n",
            "Epoch [1/5]\t Step [1601/2217]\t Loss: 0.1771\n",
            "Epoch [1/5]\t Step [1651/2217]\t Loss: 0.0696\n",
            "Epoch [1/5]\t Step [1701/2217]\t Loss: 0.0934\n",
            "Epoch [1/5]\t Step [1751/2217]\t Loss: 0.2141\n",
            "Epoch [1/5]\t Step [1801/2217]\t Loss: 0.0632\n",
            "Epoch [1/5]\t Step [1851/2217]\t Loss: 0.0979\n",
            "Epoch [1/5]\t Step [1901/2217]\t Loss: 0.0906\n",
            "Epoch [1/5]\t Step [1951/2217]\t Loss: 0.2475\n",
            "Epoch [1/5]\t Step [2001/2217]\t Loss: 0.1449\n",
            "Epoch [1/5]\t Step [2051/2217]\t Loss: 0.1460\n",
            "Epoch [1/5]\t Step [2101/2217]\t Loss: 0.1962\n",
            "Epoch [1/5]\t Step [2151/2217]\t Loss: 0.8222\n",
            "Epoch [1/5]\t Step [2201/2217]\t Loss: 0.0406\n",
            "Epoch: 1,\t Validation Loss: 0.5272,\t Accuracy: 0.8151\n",
            "Epoch [2/5]\t Step [1/2217]\t Loss: 0.0081\n",
            "Epoch [2/5]\t Step [51/2217]\t Loss: 0.0736\n",
            "Epoch [2/5]\t Step [101/2217]\t Loss: 0.1011\n",
            "Epoch [2/5]\t Step [151/2217]\t Loss: 0.2163\n",
            "Epoch [2/5]\t Step [201/2217]\t Loss: 0.1333\n",
            "Epoch [2/5]\t Step [251/2217]\t Loss: 0.4013\n",
            "Epoch [2/5]\t Step [301/2217]\t Loss: 0.2042\n",
            "Epoch [2/5]\t Step [351/2217]\t Loss: 0.0240\n",
            "Epoch [2/5]\t Step [401/2217]\t Loss: 0.0409\n",
            "Epoch [2/5]\t Step [451/2217]\t Loss: 0.0448\n",
            "Epoch [2/5]\t Step [501/2217]\t Loss: 0.1609\n",
            "Epoch [2/5]\t Step [551/2217]\t Loss: 0.1220\n",
            "Epoch [2/5]\t Step [601/2217]\t Loss: 0.1242\n",
            "Epoch [2/5]\t Step [651/2217]\t Loss: 0.1556\n",
            "Epoch [2/5]\t Step [701/2217]\t Loss: 0.1307\n",
            "Epoch [2/5]\t Step [751/2217]\t Loss: 0.0218\n",
            "Epoch [2/5]\t Step [801/2217]\t Loss: 0.1085\n",
            "Epoch [2/5]\t Step [851/2217]\t Loss: 0.0294\n",
            "Epoch [2/5]\t Step [901/2217]\t Loss: 0.2236\n",
            "Epoch [2/5]\t Step [951/2217]\t Loss: 0.0274\n",
            "Epoch [2/5]\t Step [1001/2217]\t Loss: 0.0549\n",
            "Epoch [2/5]\t Step [1051/2217]\t Loss: 0.0170\n",
            "Epoch [2/5]\t Step [1101/2217]\t Loss: 0.0781\n",
            "Epoch [2/5]\t Step [1151/2217]\t Loss: 0.2120\n",
            "Epoch [2/5]\t Step [1201/2217]\t Loss: 0.0454\n",
            "Epoch [2/5]\t Step [1251/2217]\t Loss: 0.0237\n",
            "Epoch [2/5]\t Step [1301/2217]\t Loss: 0.0134\n",
            "Epoch [2/5]\t Step [1351/2217]\t Loss: 0.0848\n",
            "Epoch [2/5]\t Step [1401/2217]\t Loss: 0.2611\n",
            "Epoch [2/5]\t Step [1451/2217]\t Loss: 0.1074\n",
            "Epoch [2/5]\t Step [1501/2217]\t Loss: 0.1190\n",
            "Epoch [2/5]\t Step [1551/2217]\t Loss: 0.0457\n",
            "Epoch [2/5]\t Step [1601/2217]\t Loss: 0.0782\n",
            "Epoch [2/5]\t Step [1651/2217]\t Loss: 0.0588\n",
            "Epoch [2/5]\t Step [1701/2217]\t Loss: 0.1129\n",
            "Epoch [2/5]\t Step [1751/2217]\t Loss: 0.1247\n",
            "Epoch [2/5]\t Step [1801/2217]\t Loss: 0.1012\n",
            "Epoch [2/5]\t Step [1851/2217]\t Loss: 0.0369\n",
            "Epoch [2/5]\t Step [1901/2217]\t Loss: 0.0362\n",
            "Epoch [2/5]\t Step [1951/2217]\t Loss: 0.0381\n",
            "Epoch [2/5]\t Step [2001/2217]\t Loss: 0.0648\n",
            "Epoch [2/5]\t Step [2051/2217]\t Loss: 0.0693\n",
            "Epoch [2/5]\t Step [2101/2217]\t Loss: 0.0036\n",
            "Epoch [2/5]\t Step [2151/2217]\t Loss: 0.4384\n",
            "Epoch [2/5]\t Step [2201/2217]\t Loss: 0.1029\n",
            "Epoch [3/5]\t Step [1/2217]\t Loss: 0.2006\n",
            "Epoch [3/5]\t Step [51/2217]\t Loss: 0.0144\n",
            "Epoch [3/5]\t Step [101/2217]\t Loss: 0.2466\n",
            "Epoch [3/5]\t Step [151/2217]\t Loss: 0.2671\n",
            "Epoch [3/5]\t Step [201/2217]\t Loss: 0.0980\n",
            "Epoch [3/5]\t Step [251/2217]\t Loss: 0.0142\n",
            "Epoch [3/5]\t Step [301/2217]\t Loss: 0.1347\n",
            "Epoch [3/5]\t Step [351/2217]\t Loss: 0.0036\n",
            "Epoch [3/5]\t Step [401/2217]\t Loss: 0.1173\n",
            "Epoch [3/5]\t Step [451/2217]\t Loss: 0.1775\n",
            "Epoch [3/5]\t Step [501/2217]\t Loss: 0.0768\n",
            "Epoch [3/5]\t Step [551/2217]\t Loss: 0.0316\n",
            "Epoch [3/5]\t Step [601/2217]\t Loss: 0.0582\n",
            "Epoch [3/5]\t Step [651/2217]\t Loss: 0.0271\n",
            "Epoch [3/5]\t Step [701/2217]\t Loss: 0.0172\n",
            "Epoch [3/5]\t Step [751/2217]\t Loss: 0.0229\n",
            "Epoch [3/5]\t Step [801/2217]\t Loss: 0.2079\n",
            "Epoch [3/5]\t Step [851/2217]\t Loss: 0.2634\n",
            "Epoch [3/5]\t Step [901/2217]\t Loss: 0.0553\n",
            "Epoch [3/5]\t Step [951/2217]\t Loss: 0.0449\n",
            "Epoch [3/5]\t Step [1001/2217]\t Loss: 0.2479\n",
            "Epoch [3/5]\t Step [1051/2217]\t Loss: 0.0385\n",
            "Epoch [3/5]\t Step [1101/2217]\t Loss: 0.0494\n",
            "Epoch [3/5]\t Step [1151/2217]\t Loss: 0.0099\n",
            "Epoch [3/5]\t Step [1201/2217]\t Loss: 0.0210\n",
            "Epoch [3/5]\t Step [1251/2217]\t Loss: 0.1556\n",
            "Epoch [3/5]\t Step [1301/2217]\t Loss: 0.2473\n",
            "Epoch [3/5]\t Step [1351/2217]\t Loss: 0.0474\n",
            "Epoch [3/5]\t Step [1401/2217]\t Loss: 0.1103\n",
            "Epoch [3/5]\t Step [1451/2217]\t Loss: 0.1122\n",
            "Epoch [3/5]\t Step [1501/2217]\t Loss: 0.0183\n",
            "Epoch [3/5]\t Step [1551/2217]\t Loss: 0.0127\n",
            "Epoch [3/5]\t Step [1601/2217]\t Loss: 0.0316\n",
            "Epoch [3/5]\t Step [1651/2217]\t Loss: 0.2699\n",
            "Epoch [3/5]\t Step [1701/2217]\t Loss: 0.0140\n",
            "Epoch [3/5]\t Step [1751/2217]\t Loss: 0.0303\n",
            "Epoch [3/5]\t Step [1801/2217]\t Loss: 0.1573\n",
            "Epoch [3/5]\t Step [1851/2217]\t Loss: 0.0259\n",
            "Epoch [3/5]\t Step [1901/2217]\t Loss: 0.1212\n",
            "Epoch [3/5]\t Step [1951/2217]\t Loss: 0.0555\n",
            "Epoch [3/5]\t Step [2001/2217]\t Loss: 0.0153\n",
            "Epoch [3/5]\t Step [2051/2217]\t Loss: 0.0631\n",
            "Epoch [3/5]\t Step [2101/2217]\t Loss: 0.0097\n",
            "Epoch [3/5]\t Step [2151/2217]\t Loss: 0.0083\n",
            "Epoch [3/5]\t Step [2201/2217]\t Loss: 0.0113\n",
            "Epoch: 3,\t Validation Loss: 0.6702,\t Accuracy: 0.7901\n",
            "Epoch [4/5]\t Step [1/2217]\t Loss: 0.0727\n",
            "Epoch [4/5]\t Step [51/2217]\t Loss: 0.0223\n",
            "Epoch [4/5]\t Step [101/2217]\t Loss: 0.3198\n",
            "Epoch [4/5]\t Step [151/2217]\t Loss: 0.1063\n",
            "Epoch [4/5]\t Step [201/2217]\t Loss: 0.0451\n",
            "Epoch [4/5]\t Step [251/2217]\t Loss: 0.0326\n",
            "Epoch [4/5]\t Step [301/2217]\t Loss: 0.0472\n",
            "Epoch [4/5]\t Step [351/2217]\t Loss: 0.0068\n",
            "Epoch [4/5]\t Step [401/2217]\t Loss: 0.0200\n",
            "Epoch [4/5]\t Step [451/2217]\t Loss: 0.0673\n",
            "Epoch [4/5]\t Step [501/2217]\t Loss: 0.1485\n",
            "Epoch [4/5]\t Step [551/2217]\t Loss: 0.0098\n",
            "Epoch [4/5]\t Step [601/2217]\t Loss: 0.4083\n",
            "Epoch [4/5]\t Step [651/2217]\t Loss: 0.0797\n",
            "Epoch [4/5]\t Step [701/2217]\t Loss: 0.0187\n",
            "Epoch [4/5]\t Step [751/2217]\t Loss: 0.0647\n",
            "Epoch [4/5]\t Step [801/2217]\t Loss: 0.0250\n",
            "Epoch [4/5]\t Step [851/2217]\t Loss: 0.0527\n",
            "Epoch [4/5]\t Step [901/2217]\t Loss: 0.0099\n",
            "Epoch [4/5]\t Step [951/2217]\t Loss: 0.0492\n",
            "Epoch [4/5]\t Step [1001/2217]\t Loss: 0.0017\n",
            "Epoch [4/5]\t Step [1051/2217]\t Loss: 0.0890\n",
            "Epoch [4/5]\t Step [1101/2217]\t Loss: 0.0230\n",
            "Epoch [4/5]\t Step [1151/2217]\t Loss: 0.1899\n",
            "Epoch [4/5]\t Step [1201/2217]\t Loss: 0.0356\n",
            "Epoch [4/5]\t Step [1251/2217]\t Loss: 0.0100\n",
            "Epoch [4/5]\t Step [1301/2217]\t Loss: 0.0408\n",
            "Epoch [4/5]\t Step [1351/2217]\t Loss: 0.1268\n",
            "Epoch [4/5]\t Step [1401/2217]\t Loss: 0.1794\n",
            "Epoch [4/5]\t Step [1451/2217]\t Loss: 0.0349\n",
            "Epoch [4/5]\t Step [1501/2217]\t Loss: 0.0318\n",
            "Epoch [4/5]\t Step [1551/2217]\t Loss: 0.1951\n",
            "Epoch [4/5]\t Step [1601/2217]\t Loss: 0.0055\n",
            "Epoch [4/5]\t Step [1651/2217]\t Loss: 0.0140\n",
            "Epoch [4/5]\t Step [1701/2217]\t Loss: 0.0109\n",
            "Epoch [4/5]\t Step [1751/2217]\t Loss: 0.0223\n",
            "Epoch [4/5]\t Step [1801/2217]\t Loss: 0.3257\n",
            "Epoch [4/5]\t Step [1851/2217]\t Loss: 0.0020\n",
            "Epoch [4/5]\t Step [1901/2217]\t Loss: 0.0418\n",
            "Epoch [4/5]\t Step [1951/2217]\t Loss: 0.2824\n",
            "Epoch [4/5]\t Step [2001/2217]\t Loss: 0.0192\n",
            "Epoch [4/5]\t Step [2051/2217]\t Loss: 0.0068\n",
            "Epoch [4/5]\t Step [2101/2217]\t Loss: 0.0034\n",
            "Epoch [4/5]\t Step [2151/2217]\t Loss: 0.3365\n",
            "Epoch [4/5]\t Step [2201/2217]\t Loss: 0.0181\n",
            "Epoch [5/5]\t Step [1/2217]\t Loss: 0.1970\n",
            "Epoch [5/5]\t Step [51/2217]\t Loss: 0.0109\n",
            "Epoch [5/5]\t Step [101/2217]\t Loss: 0.1412\n",
            "Epoch [5/5]\t Step [151/2217]\t Loss: 0.0021\n",
            "Epoch [5/5]\t Step [201/2217]\t Loss: 0.0069\n",
            "Epoch [5/5]\t Step [251/2217]\t Loss: 0.0106\n",
            "Epoch [5/5]\t Step [301/2217]\t Loss: 0.0779\n",
            "Epoch [5/5]\t Step [351/2217]\t Loss: 0.0721\n",
            "Epoch [5/5]\t Step [401/2217]\t Loss: 0.0194\n",
            "Epoch [5/5]\t Step [451/2217]\t Loss: 0.0169\n",
            "Epoch [5/5]\t Step [501/2217]\t Loss: 0.0156\n",
            "Epoch [5/5]\t Step [551/2217]\t Loss: 0.0128\n",
            "Epoch [5/5]\t Step [601/2217]\t Loss: 0.1318\n",
            "Epoch [5/5]\t Step [651/2217]\t Loss: 0.1500\n",
            "Epoch [5/5]\t Step [701/2217]\t Loss: 0.0556\n",
            "Epoch [5/5]\t Step [751/2217]\t Loss: 0.0084\n",
            "Epoch [5/5]\t Step [801/2217]\t Loss: 0.0025\n",
            "Epoch [5/5]\t Step [851/2217]\t Loss: 0.1934\n",
            "Epoch [5/5]\t Step [901/2217]\t Loss: 0.0898\n",
            "Epoch [5/5]\t Step [951/2217]\t Loss: 0.0167\n",
            "Epoch [5/5]\t Step [1001/2217]\t Loss: 0.0043\n",
            "Epoch [5/5]\t Step [1051/2217]\t Loss: 0.1067\n",
            "Epoch [5/5]\t Step [1101/2217]\t Loss: 0.1139\n",
            "Epoch [5/5]\t Step [1151/2217]\t Loss: 0.0122\n",
            "Epoch [5/5]\t Step [1201/2217]\t Loss: 0.0271\n",
            "Epoch [5/5]\t Step [1251/2217]\t Loss: 0.0070\n",
            "Epoch [5/5]\t Step [1301/2217]\t Loss: 0.0250\n",
            "Epoch [5/5]\t Step [1351/2217]\t Loss: 0.0151\n",
            "Epoch [5/5]\t Step [1401/2217]\t Loss: 0.0127\n",
            "Epoch [5/5]\t Step [1451/2217]\t Loss: 0.2427\n",
            "Epoch [5/5]\t Step [1501/2217]\t Loss: 0.0195\n",
            "Epoch [5/5]\t Step [1551/2217]\t Loss: 0.0107\n",
            "Epoch [5/5]\t Step [1601/2217]\t Loss: 0.0117\n",
            "Epoch [5/5]\t Step [1651/2217]\t Loss: 0.0226\n",
            "Epoch [5/5]\t Step [1701/2217]\t Loss: 0.0174\n",
            "Epoch [5/5]\t Step [1751/2217]\t Loss: 0.1444\n",
            "Epoch [5/5]\t Step [1801/2217]\t Loss: 0.0031\n",
            "Epoch [5/5]\t Step [1851/2217]\t Loss: 0.2102\n",
            "Epoch [5/5]\t Step [1901/2217]\t Loss: 0.2235\n",
            "Epoch [5/5]\t Step [1951/2217]\t Loss: 0.2483\n",
            "Epoch [5/5]\t Step [2001/2217]\t Loss: 0.0013\n",
            "Epoch [5/5]\t Step [2051/2217]\t Loss: 0.5826\n",
            "Epoch [5/5]\t Step [2101/2217]\t Loss: 0.0101\n",
            "Epoch [5/5]\t Step [2151/2217]\t Loss: 0.1176\n",
            "Epoch [5/5]\t Step [2201/2217]\t Loss: 0.1113\n",
            "Epoch: 5,\t Validation Loss: 0.5213,\t Accuracy: 0.8223\n",
            "Test Loss: 0.5743,\t Accuracy: 0.8045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        predictions = ResNet_trained_model_L2(data)\n",
        "        probs = torch.softmax(predictions, dim=1)\n",
        "\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size(0))\n",
        "\n",
        "        class_predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "        hits += (class_predictions == targets).sum().item()\n",
        "\n",
        "        all_preds.append(class_predictions.cpu())\n",
        "        all_targets.append(targets.cpu())\n",
        "        all_probs.append(probs.cpu())\n",
        "\n",
        "# combine\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_targets = torch.cat(all_targets)\n",
        "all_probs = torch.cat(all_probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "QhQz_v4zNfnV"
      },
      "id": "QhQz_v4zNfnV",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "# Display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix  ResNet Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FCWVLagOPVJP",
        "outputId": "294f9027-0991-4619-b86a-6642ece89130"
      },
      "id": "FCWVLagOPVJP",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHHCAYAAADAlkARAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXdNJREFUeJzt3XdYFFfbBvB7ly6wCyhVAbtiJTbEbiRg7zEqMWgseRWMJdYvYo/EEgs2NMUWjCYaG4mF2DCKDcWCig1LRMCIsILS5/vDMMkKriy7sMLev1xzXe7MmZlndgn78JxzZiSCIAggIiIivSbVdQBERESke0wIiIiIiAkBERERMSEgIiIiMCEgIiIiMCEgIiIiMCEgIiIiMCEgIiIiMCEgIiIiMCHQW7du3YK3tzfkcjkkEgl2796t1ePfu3cPEokEGzdu1Opxy7IOHTqgQ4cOug6D3lEbN26ERCLBvXv31N539uzZkEgk2g+K9AoTAh26c+cOPvvsM1SvXh2mpqaQyWRo3bo1VqxYgZcvX5bouf38/HDlyhV89dVX2LJlC5o1a1ai5ytNQ4cOhUQigUwmK/R9vHXrFiQSCSQSCZYsWaL28ePj4zF79mxER0drIdp3S34il79IpVLY2NigS5cuiIyMLLHzHjt2TDxnVFRUge1Dhw6FhYVFsY79+++/Y/bs2UVu36FDB0gkEtSqVavQ7eHh4WKsO3bsKFZMRO8iQ10HoK9+++03fPjhhzAxMcEnn3yCBg0aICsrC3/++ScmT56MmJgYrF+/vkTO/fLlS0RGRuLLL79EQEBAiZzD1dUVL1++hJGRUYkc/20MDQ3x4sUL7Nu3DwMGDFDaFhoaClNTU2RkZBTr2PHx8ZgzZw6qVq0Kd3f3Iu936NChYp1PFwYNGoSuXbsiNzcXN2/exJo1a9CxY0ecO3cODRs2LNFzz549G/v27dPa8X7//XesXr1araTA1NQUt2/fxtmzZ9GiRQulbZr+/BC9q1gh0IG4uDgMHDgQrq6uuHbtGlasWIGRI0fC398fP/30E65du4b69euX2PmfPHkCALCysiqxc0gkEpiamsLAwKDEzqGKiYkJOnXqhJ9++qnAtq1bt6Jbt26lFsuLFy8AAMbGxjA2Ni6182qiSZMm+Pjjj+Hn54evvvoKP/30EzIzM7F27doSPa+7uzvCwsJw4cKFEj3P29SoUQN16tQp8POTkZGBXbt2lerPD1FpYUKgA4sWLUJaWhq+//57ODo6Fthes2ZNjBs3Tnydk5ODefPmoUaNGjAxMUHVqlXxf//3f8jMzFTar2rVqujevTv+/PNPtGjRAqampqhevTo2b94stpk9ezZcXV0BAJMnT4ZEIkHVqlUBvCrL5v/7vwrrnwwPD0ebNm1gZWUFCwsL1KlTB//3f/8nbn/TGIIjR46gbdu2MDc3h5WVFXr16oXr168Xer7bt29j6NChsLKyglwux7Bhw8Qv16IYPHgw9u/fj5SUFHHduXPncOvWLQwePLhA++TkZEyaNAkNGzaEhYUFZDIZunTpgkuXLoltjh07hubNmwMAhg0bJpaO86+zQ4cOaNCgAaKiotCuXTtUqFBBfF9eH0Pg5+cHU1PTAtfv4+MDa2trxMfHF/laS1rbtm0BvOrm+q+UlBSMHz8ezs7OMDExQc2aNbFw4ULk5eUptdu2bRuaNm0KS0tLyGQyNGzYECtWrChwnrFjx8La2rrIf83v379f/HmytLREt27dEBMTI24fOnQoVq9eDQBKXSFFMWjQIGzfvl3pWvbt24cXL14UqDrlu3jxIrp06QKZTAYLCwt06tQJp0+fLtAuJiYG77//PszMzFClShXMnz+/wHtW1Gsk0hYmBDqwb98+VK9eHa1atSpS+xEjRmDmzJlo0qQJli1bhvbt2yMoKAgDBw4s0Pb27dvo378/PvjgA3zzzTewtrbG0KFDxV8gffv2xbJlywC8+oW3ZcsWLF++XK34Y2Ji0L17d2RmZmLu3Ln45ptv0LNnT5w8eVLlfn/88Qd8fHyQlJSE2bNnY+LEiTh16hRat25d6ECqAQMG4Pnz5wgKCsKAAQOwceNGzJkzp8hx9u3bFxKJBL/++qu4buvWrahbty6aNGlSoP3du3exe/dudO/eHUuXLsXkyZNx5coVtG/fXvxydnNzw9y5cwEAo0aNwpYtW7Blyxa0a9dOPM7Tp0/RpUsXuLu7Y/ny5ejYsWOh8a1YsQK2trbw8/NDbm4uAGDdunU4dOgQVq5cCScnpyJfa0nL/3ysra3FdS9evED79u3x448/4pNPPkFwcDBat26N6dOnY+LEiWK78PBwDBo0CNbW1li4cCG+/vprdOjQodCfF5lMhgkTJmDfvn1vrRJs2bIF3bp1g4WFBRYuXIjAwEBcu3YNbdq0EeP97LPP8MEHH4jt85eiGDx4MB4/foxjx46J67Zu3YpOnTrBzs6uQPuYmBi0bdsWly5dwpQpUxAYGIi4uDh06NABZ86cEdslJCSgY8eOiI6OxrRp0zB+/Hhs3ry50ASpKNdIpDUClarU1FQBgNCrV68itY+OjhYACCNGjFBaP2nSJAGAcOTIEXGdq6urAECIiIgQ1yUlJQkmJibCF198Ia6Li4sTAAiLFy9WOqafn5/g6upaIIZZs2YJ//1RWbZsmQBAePLkyRvjzj/Hhg0bxHXu7u6CnZ2d8PTpU3HdpUuXBKlUKnzyyScFzvfpp58qHbNPnz5CxYoV33jO/16Hubm5IAiC0L9/f6FTp06CIAhCbm6u4ODgIMyZM6fQ9yAjI0PIzc0tcB0mJibC3LlzxXXnzp0rcG352rdvLwAQQkJCCt3Wvn17pXUHDx4UAAjz588X7t69K1hYWAi9e/d+6zWWlPz3Zc6cOcKTJ0+EhIQE4cSJE0Lz5s0FAMIvv/witp03b55gbm4u3Lx5U+kY06ZNEwwMDIQHDx4IgiAI48aNE2QymZCTk/PG8x49elQ8fkpKimBtbS307NlT3P7fz1QQBOH58+eClZWVMHLkSKXjJCQkCHK5XGm9v7+/oM6vuvbt2wv169cXBEEQmjVrJgwfPlwQBEF49uyZYGxsLGzatEkp3ny9e/cWjI2NhTt37ojr4uPjBUtLS6Fdu3biuvHjxwsAhDNnzojrkpKSBLlcLgAQ4uLi1L7G1/8fJSoOVghKmUKhAABYWloWqf3vv/8OAEp/cQHAF198AeDV4MT/qlevnljeBQBbW1vUqVMHd+/eLXbMr8sfe7Bnz543ljlf9/jxY0RHR2Po0KGwsbER1zdq1AgffPCBeJ3/9b///U/pddu2bfH06VPxPSyKwYMH49ixY0hISMCRI0eQkJBQaHcB8GrcgVT66n+J3NxcPH36VOwOUadP28TEBMOGDStSW29vb3z22WeYO3cu+vbtC1NTU6xbt67I5yops2bNgq2tLRwcHNC2bVtcv34d33zzDfr37y+2+eWXX9C2bVtYW1vj77//FhcvLy/k5uYiIiICwKufl/T0dISHhxfp3HK5HOPHj8fevXtx8eLFQtuEh4cjJSUFgwYNUjq3gYEBPDw8cPToUc3fBLz6+fn111+RlZWFHTt2wMDAAH369CnQLjc3F4cOHULv3r1RvXp1cb2joyMGDx6MP//8U/y5/f3339GyZUulwYq2trbw9fXVyTUS5eMsg1Imk8kAAM+fPy9S+/v370MqlaJmzZpK6x0cHGBlZYX79+8rrXdxcSlwDGtrazx79qyYERf00Ucf4bvvvsOIESMwbdo0dOrUCX379kX//v3FL9TCrgMA6tSpU2Cbm5sbDh48iPT0dJibm4vrX7+W/HL1s2fPxPfxbbp27QpLS0ts374d0dHRaN68OWrWrFlouTUvLw8rVqzAmjVrEBcXJ5bxAaBixYpFOh8AVK5cWa3Bg0uWLMGePXsQHR2NrVu3FlqOft2TJ0+U4lOHra3tWwd7jho1Ch9++CEyMjJw5MgRBAcHFzjfrVu3cPnyZdja2hZ6jKSkJADAmDFj8PPPP6NLly6oXLkyvL29MWDAAHTu3PmN5x83bhyWLVuG2bNnY8+ePQW237p1CwDw/vvvF7p/UX8+3mbgwIGYNGkS9u/fj9DQUHTv3r3QZP7Jkyd48eLFG3++8/Ly8PDhQ9SvXx/379+Hh4dHgXav71ta10iUjwlBKZPJZHBycsLVq1fV2q+oA6He9IteEIRin+P1LwIzMzNERETg6NGj+O2333DgwAFs374d77//Pg4dOqS1mQWaXEs+ExMT9O3bF5s2bcLdu3dVDlZbsGABAgMD8emnn2LevHmwsbGBVCrF+PHji1wJAV69P+q4ePGi+OV55coVDBo06K37NG/evEAyWFRxcXGFDh79r1q1asHLywsA0L17dxgYGGDatGno2LGjeM+KvLw8fPDBB5gyZUqhx6hduzYAwM7ODtHR0Th48CD279+P/fv3Y8OGDfjkk0+wadOmQvfNrxLMnj270CpB/uexZcsWODg4FNhuaKidX22Ojo7o0KEDvvnmG5w8eRI7d+7UynGLorSukSgff6J0oHv37li/fj0iIyPh6empsq2rqyvy8vJw69YtuLm5iesTExORkpIizhjQBmtra6UR+fkK++KRSqXo1KkTOnXqhKVLl2LBggX48ssvcfToUfGL5PXrAIDY2NgC227cuIFKlSopVQe0afDgwfjhhx8glUoLHYiZb8eOHejYsSO+//57pfUpKSmoVKmS+Fqbd4RLT0/HsGHDUK9ePbRq1QqLFi1Cnz59xJkMbxIaGlrsm1cV9uXyNl9++SW+/fZbzJgxAwcOHADwampeWlpaoZ/364yNjdGjRw/06NEDeXl5GDNmDNatW4fAwMAC1a9848ePx/LlyzFnzpwCU2Rr1KgB4FWy8bbza/p5DR48GCNGjICVlRW6du1aaBtbW1tUqFDhjT/fUqkUzs7OAF79v5D/1/9/vb6vOtdIpA0cQ6ADU6ZMgbm5OUaMGIHExMQC2+/cuSOOOM7/BfT6TIClS5cCgFbnQ9eoUQOpqam4fPmyuO7x48fYtWuXUrvk5OQC++bfoOf1qZD5HB0d4e7ujk2bNiklHVevXsWhQ4fe+ItWGzp27Ih58+Zh1apVKr8MDQwMClQffvnlFzx69EhpXX7iUljypK6pU6fiwYMH2LRpE5YuXYqqVavCz8/vje9jvtatW8PLy6tYi6mpqdpxWllZ4bPPPsPBgwfFOzQOGDAAkZGROHjwYIH2KSkpyMnJAfBq1sV/SaVSNGrUCMCbf16Af6sE+d0p/+Xj4wOZTIYFCxYgOzu7wL7599oANP+8+vfvj1mzZmHNmjVv7AoyMDCAt7c39uzZo9QdlZiYiK1bt6JNmzZiib9r1644ffo0zp49qxRvaGhosa+RSBtYIdCBGjVqYOvWrfjoo4/g5uamdKfCU6dO4ZdffsHQoUMBAI0bN4afnx/Wr1+PlJQUtG/fHmfPnsWmTZvQu3fvN05pK46BAwdi6tSp6NOnDz7//HO8ePECa9euRe3atZUG1c2dOxcRERHo1q0bXF1dkZSUhDVr1qBKlSpo06bNG4+/ePFidOnSBZ6enhg+fDhevnyJlStXQi6Xq3UXOXVJpVLMmDHjre26d++OuXPnYtiwYWjVqhWuXLmC0NBQpUFiwKvPz8rKCiEhIbC0tIS5uTk8PDxQrVo1teI6cuQI1qxZg1mzZonTIDds2IAOHTogMDAQixYtUut4JW3cuHFYvnw5vv76a2zbtg2TJ0/G3r170b17dwwdOhRNmzZFeno6rly5gh07duDevXuoVKkSRowYgeTkZLz//vuoUqUK7t+/j5UrV8Ld3V2p6vWmcy5btgyXLl1SqiDJZDKsXbsWQ4YMQZMmTTBw4EDY2triwYMH+O2339C6dWusWrUKANC0aVMAwOeffw4fHx8YGBiorBS9rqg/n/PnzxfvzzFmzBgYGhpi3bp1yMzMVPosp0yZgi1btqBz584YN24czM3NsX79eri6uiol4+pcI5FW6HiWg167efOmMHLkSKFq1aqCsbGxYGlpKbRu3VpYuXKlkJGRIbbLzs4W5syZI1SrVk0wMjISnJ2dhenTpyu1EYRX0w67detW4DyvT3d707RDQRCEQ4cOCQ0aNBCMjY2FOnXqCD/++GOBKU2HDx8WevXqJTg5OQnGxsaCk5OTMGjQIKXpZ4VNOxQEQfjjjz+E1q1bC2ZmZoJMJhN69OghXLt2TalN/vlen9a4YcMGpWlZb/L6FLXCvGna4RdffCE4OjoKZmZmQuvWrYXIyMhCpwvu2bNHqFevnmBoaKh0nf+dsva6/x5HoVAIrq6uQpMmTYTs7GyldhMmTBCkUqkQGRmp8hpKgqqfDUEQhKFDhwoGBgbC7du3BUF4NTVu+vTpQs2aNQVjY2OhUqVKQqtWrYQlS5YIWVlZgiAIwo4dOwRvb2/Bzs5OMDY2FlxcXITPPvtMePz4sXjcwqbx5cv/eSjsMz169Kjg4+MjyOVywdTUVKhRo4YwdOhQ4fz582KbnJwcYezYsYKtra0gkUjeOj1P1Wf4tngvXLgg+Pj4CBYWFkKFChWEjh07CqdOnSqw/+XLl4X27dsLpqamQuXKlYV58+YJ33//faE/30W5Rk47JG2QCIIaI7SIiIioXOIYAiIiImJCQEREREwIiIiICEwIiIiICEwIiIiICEwIiIiICGX8xkR5eXmIj4+HpaWlVm8nS0REpUMQBDx//hxOTk5vfDiaNmRkZCArK0vj4xgbGxfrbp9lQZlOCOLj48X7gxMRUdn18OFDVKlSpUSOnZGRATPLikDOC42P5eDggLi4uHKZFJTphCD/MaTG9fwgMSj642apbLr9x7t1K18qWUYGrPrpg+fPFahVzaXQx0prS1ZWFpDzAib1/ABNvitys5BwbROysrKYELxr8rsJJAbGTAj0AJ//rl+YEOiXUun2NTTV6LtCkJTvYXdlOiEgIiIqMgkATRKPcp6jMiEgIiL9IJG+WjTZvxwr31dHRERERcIKARER6QeJRMMug/LdZ8CEgIiI9AO7DFQq31dHRERERcIKARER6Qd2GajEhICIiPSEhl0G5byoXr6vjoiIiIqEFQIiItIP7DJQiQkBERHpB84yUKl8Xx0REREVCSsERESkH9hloBITAiIi0g/sMlCJCQEREekHVghUKt/pDhERERUJKwRERKQf2GWgEhMCIiLSDxKJhgkBuwyIiIionGOFgIiI9INU8mrRZP9yjAkBERHpB44hUKl8Xx0REREVCSsERESkH3gfApWYEBARkX5gl4FK5fvqiIiIqEhYISAiIv3ALgOVWCEgIiL9kN9loMmihoiICPTo0QNOTk6QSCTYvXt3gTbXr19Hz549IZfLYW5ujubNm+PBgwfi9oyMDPj7+6NixYqwsLBAv379kJiYqHSMBw8eoFu3bqhQoQLs7OwwefJk5OTkqP32MCEgIiL9kF8h0GRRQ3p6Oho3bozVq1cXuv3OnTto06YN6tati2PHjuHy5csIDAyEqamp2GbChAnYt28ffvnlFxw/fhzx8fHo27evuD03NxfdunVDVlYWTp06hU2bNmHjxo2YOXOm+m+PIAiC2nu9IxQKBeRyOUwajoTEwFjX4VAJS4wM1nUIVIqMDMp3eZZeUSgUcKhkhdTUVMhkshI7h1wuh8n78yAxNH37Dm8g5GQg80hgsWKVSCTYtWsXevfuLa4bOHAgjIyMsGXLlkL3SU1Nha2tLbZu3Yr+/fsDAG7cuAE3NzdERkaiZcuW2L9/P7p37474+HjY29sDAEJCQjB16lQ8efIExsZF/25khYCIiPSDlroMFAqF0pKZmal2KHl5efjtt99Qu3Zt+Pj4wM7ODh4eHkrdClFRUcjOzoaXl5e4rm7dunBxcUFkZCQAIDIyEg0bNhSTAQDw8fGBQqFATEyMWjExISAiIv2gpS4DZ2dnyOVycQkKClI7lKSkJKSlpeHrr79G586dcejQIfTp0wd9+/bF8ePHAQAJCQkwNjaGlZWV0r729vZISEgQ2/w3Gcjfnr9NHZxlQEREpIaHDx8qdRmYmJiofYy8vDwAQK9evTBhwgQAgLu7O06dOoWQkBC0b99eO8GqgRUCIiLSE5p2F7z6ypTJZEpLcRKCSpUqwdDQEPXq1VNa7+bmJs4ycHBwQFZWFlJSUpTaJCYmwsHBQWzz+qyD/Nf5bYqKCQEREemHUp5loIqxsTGaN2+O2NhYpfU3b96Eq6srAKBp06YwMjLC4cOHxe2xsbF48OABPD09AQCenp64cuUKkpKSxDbh4eGQyWQFko23YZcBERFRCUhLS8Pt27fF13FxcYiOjoaNjQ1cXFwwefJkfPTRR2jXrh06duyIAwcOYN++fTh27BgAQC6XY/jw4Zg4cSJsbGwgk8kwduxYeHp6omXLlgAAb29v1KtXD0OGDMGiRYuQkJCAGTNmwN/fX+3KBRMCIiLSDxKJhs8yUK9CcP78eXTs2FF8PXHiRACAn58fNm7ciD59+iAkJARBQUH4/PPPUadOHezcuRNt2rQR91m2bBmkUin69euHzMxM+Pj4YM2aNeJ2AwMDhIWFYfTo0fD09IS5uTn8/Pwwd+5c9S+P9yGgsoL3IdAvvA+BfijV+xD4LIHEyKzYxxGyXyLz4KQSjVWXOIaAiIiI2GVARER6gg83UokJARER6YdiPKCowP7lGBMCIiLSD6wQqFS+0x0iIiIqElYIiIhIP7DLQCUmBEREpB/YZaBS+U53iIiIqEhYISAiIr0gkUggYYXgjZgQEBGRXmBCoBq7DIiIiIgVAiIi0hOSfxZN9i/HmBAQEZFeYJeBauwyICIiIlYIiIhIP7BCoBoTAiIi0gtMCFRjQqAjrd6rgbFDvNC4rgscbeXwnbQevx+/LG5/dm5VofvNXLELK388DAC4tGcOXJwqKm2fs2oPlm8KBwBMHdkV00Z1LXCM9JeZqNLuC21dCmlo8Xe/Y8n3B5TW1XSxw8ntMwAAk77ehojzsUh8ooB5BWM0a1gNgWN6oVZVe12ESxo6deE2Vv54GJduPEDC3wpsWTQC3To0LrTtxKBt2LjrJL6a0BejB3Us5UjLHyYEqr0TCcHq1auxePFiJCQkoHHjxli5ciVatGih67BKVAUzE1y9+Qg/7o3Ej4tHFdhep/N0pdderepj5YzB2Hs0Wmn9VyFh2Lz7pPg6LT1T/PeqH//Ahl9PKLXfveZzXLx2XwtXQNpUp7ojdgT7i68NDP4d3tOorjP6+TRDZQdrpCheYPF3+/HR+DU4t3OWUjsqG9IzMtGgVmX49miJT6Z+98Z2YUcv4fzVe3C0lZdidKTPdJ4QbN++HRMnTkRISAg8PDywfPly+Pj4IDY2FnZ2droOr8T8ceoa/jh17Y3bk54+V3rdtV1DnIi6hfuPniqtT3uRUaBtvvSXWUh/mSW+blCrMtyqO+KLoG0aRE4lwdBACruKskK3fdK7tfhvF8eKmPZZN7w/ZCEePn6KqlVsSytE0pIPWtXHB63qq2wTn5SCqd/swI4VYzBwYkgpRaYHOO1QJZ3/ebF06VKMHDkSw4YNQ7169RASEoIKFSrghx9+0HVo7wxbG0t4t2mAH/dEFtg23s8bd8IX4viPUzH2404q/2Ic0qsVbt1PRGT0nZIMl4rh7sMnaNRjBpr3m4PRszbhr4TkQtulv8zEtrAzcHGqCCd761KOkkpDXl4eRs/ajLEfd4JbDUddh1Ou5HcZaLKUZzqtEGRlZSEqKgrTp/9bHpdKpfDy8kJkZMEvP301qJsH0tIzsO+17oJ124/j0o2HSFGko0Wj6pjp3xP2leSYsfzXAscwMTbEh52bieML6N3RpH5VBM/wRQ1XOyT9rcCS7/ej1+gVOP7jdFiYmwIANuw8gbmr9+DFyyzUdLHDLyvGwNhI5wU+KgErNv8BA0MDfPZRe12HQnpGp79R/v77b+Tm5sLeXnlwlL29PW7cuFGgfWZmJjIz/+0jVygUJR7ju8C3Z0v8cuA8MrNylNav2XpE/HfM7XhkZedg2f8NwtzVe5GVrdy2e4fGsDA3xU+/nSmVmKnoOnnWE/9dv2ZlNKnviqZ9ZmPP4Yvw7ekJAOjn0wztW9RB4t8KrNl6BCNnbMC+dRNgamKkq7CpBERff4B1247h6Jap5f6vUV149fRjTQYVai+Wd5HOuwzUERQUBLlcLi7Ozs66DqnEebrXQO2qDtiy59Rb20bF3IORoQFcnGwKbBvSuxUOnriKJ8mFjzegd4fcsgJquNgh7q8n4jqZhRmqO9vB872a+H7Bp7h1P0lpVgqVD5HRd/DkWRoa9ZwJW89xsPUch4ePkxG4Yhca95ql6/DKPAk07DIo5xmBTisElSpVgoGBARITE5XWJyYmwsHBoUD76dOnY+LEieJrhUJR7pOCj3t54uK1B7h669Fb2zasXQW5uXkFvvRdnCqibdNaGPzF+pIKk7Qo/UUm7v31N/p3bl7odkEQAEEoUAWisu+jLi3QvkUdpXUffr4GA7o0x+AeLXUUFekLnSYExsbGaNq0KQ4fPozevXsDeDWg5vDhwwgICCjQ3sTEBCYmJqUcZckwNzNGNed/R4i7OlVEg9qVkZL6An8lPgMAWJqbolen9xC4fFeB/Zs3rIamDVzx5/lbeP4iAy0aVsNXE/rh5/3nkPr8pVLbj3u2RMLfCoSfiinZi6JimR28G95t6qOKow0Sn6Ri0Xf7YWAgQZ8PmuDeo7+x548L6OBRFxWtLPA4KQXBW/6AqYmRUlcDlR1pLzKVqj/345/iys2/YC2rgCoONrCxMldqb2hoALuKMtRy5X0nNMX7EKim81FJEydOhJ+fH5o1a4YWLVpg+fLlSE9Px7Bhw3QdWolyd3NF2Lpx4usFE/sBALaGnYb/nB8BAH29m0IikWDnwfMF9s/MykbfD5pi2siuMDYyxP34p1j701GsDj2i1E4ikWBw95b4KewM8vKEErwiKq74Jyn436xNeJaajopWFmjRuAZ+/3YiKllbIicnD2cu3cX67ceR+vwFbG0s0dK9BsLWT4CtjaWuQ6diiL7+AD1HB4uvZ/yT8A/q1gKrZw3RVVj6gdMOVZIIgqDzb4lVq1aJNyZyd3dHcHAwPDw83rqfQqGAXC6HScORkBgYl0KkpEuJkcFvb0TlhpFBOf/tSwBe/R53qGSF1NRUyGSF34tDG+eQy+WwHvgdJMYVin0cIesFnm0bUaKx6pLOKwQAEBAQUGgXARERkdZo2GUgsMuAiIio7NN0DEF5nwrKhICIiPQCEwLVytR9CIiIiKhkMCEgIiL9INHCooaIiAj06NEDTk5OkEgk2L179xvb/u9//4NEIsHy5cuV1icnJ8PX1xcymQxWVlYYPnw40tLSlNpcvnwZbdu2hampKZydnbFo0SL1Av0HEwIiItILpf1wo/T0dDRu3BirV69W2W7Xrl04ffo0nJycCmzz9fVFTEwMwsPDERYWhoiICIwaNUrcrlAo4O3tDVdXV0RFRWHx4sWYPXs21q9X/0Z0HENARERUArp06YIuXbqobPPo0SOMHTsWBw8eRLdu3ZS2Xb9+HQcOHMC5c+fQrFkzAMDKlSvRtWtXLFmyBE5OTggNDUVWVhZ++OEHGBsbo379+oiOjsbSpUuVEoeiYIWAiIj0grYqBAqFQmn570P31JGXl4chQ4Zg8uTJqF+/foHtkZGRsLKyEpMBAPDy8oJUKsWZM2fENu3atYOx8b/34vHx8UFsbCyePXumVjxMCIiISC9oKyFwdnZWetBeUFBQseJZuHAhDA0N8fnnnxe6PSEhAXZ2dkrrDA0NYWNjg4SEBLFNYU8Mzt+mDnYZEBERqeHhw4dKdyoszjN2oqKisGLFCly4cOGdmc7ICgEREekFbVUIZDKZ0lKchODEiRNISkqCi4sLDA0NYWhoiPv37+OLL75A1apVAQAODg5ISkpS2i8nJwfJycniE4EdHBwKfWJw/jZ1MCEgIiL9UMrTDlUZMmQILl++jOjoaHFxcnLC5MmTcfDgQQCAp6cnUlJSEBUVJe535MgR5OXlic/78fT0REREBLKzs8U24eHhqFOnDqytrdWKiV0GREREJSAtLQ23b98WX8fFxSE6Oho2NjZwcXFBxYoVldobGRnBwcEBderUAQC4ubmhc+fOGDlyJEJCQpCdnY2AgAAMHDhQnKI4ePBgzJkzB8OHD8fUqVNx9epVrFixAsuWLVM7XiYERESkF0r71sXnz59Hx44dxdcTJ04EAPj5+WHjxo1FOkZoaCgCAgLQqVMnSKVS9OvXD8HB/z75VS6X49ChQ/D390fTpk1RqVIlzJw5U+0phwATAiIi0hOlnRB06NABgiAUuf29e/cKrLOxscHWrVtV7teoUSOcOHFCrdgKw4SAiIj0Ah9upBoHFRIRERErBEREpCc0nSlQvgsETAiIiEg/sMtANXYZEBERESsERESkH1ghUI0JARER6QUJNEwIyvkgAnYZEBERESsERESkH9hloBoTAiIi0g+cdqgSuwyIiIiIFQIiItIP7DJQjQkBERHpBSYEqjEhICIivSCRvFo02b884xgCIiIiYoWAiIj0w6sKgSZdBloM5h3EhICIiPSDhl0GnHZIRERE5R4rBEREpBc4y0A1JgRERKQXOMtANXYZEBERESsERESkH6RSCaTS4v+ZL2iwb1nAhICIiPQCuwxUY5cBERERsUJARET6gbMMVGNCQEREeoFdBqoxISAiIr3ACoFqHENARERErBAQEZF+YIVANSYERESkFziGQDV2GRARERETAiIi0g8SSMRug2Itaj7/OCIiAj169ICTkxMkEgl2794tbsvOzsbUqVPRsGFDmJubw8nJCZ988gni4+OVjpGcnAxfX1/IZDJYWVlh+PDhSEtLU2pz+fJltG3bFqampnB2dsaiRYuK9f4wISAiIr2Q32WgyaKO9PR0NG7cGKtXry6w7cWLF7hw4QICAwNx4cIF/Prrr4iNjUXPnj2V2vn6+iImJgbh4eEICwtDREQERo0aJW5XKBTw9vaGq6sroqKisHjxYsyePRvr169X+/3hGAIiIqIS0KVLF3Tp0qXQbXK5HOHh4UrrVq1ahRYtWuDBgwdwcXHB9evXceDAAZw7dw7NmjUDAKxcuRJdu3bFkiVL4OTkhNDQUGRlZeGHH36AsbEx6tevj+joaCxdulQpcSgKVgiIiEgvaNRd8J8ZCgqFQmnJzMzUSnypqamQSCSwsrICAERGRsLKykpMBgDAy8sLUqkUZ86cEdu0a9cOxsbGYhsfHx/Exsbi2bNnap2fCQEREekFbXUZODs7Qy6Xi0tQUJDGsWVkZGDq1KkYNGgQZDIZACAhIQF2dnZK7QwNDWFjY4OEhASxjb29vVKb/Nf5bYqKXQZERERqePjwofilDQAmJiYaHS87OxsDBgyAIAhYu3atpuEVGxMCIiLSC9q6MZFMJlNKCDSRnwzcv38fR44cUTqug4MDkpKSlNrn5OQgOTkZDg4OYpvExESlNvmv89sUFbsMiIhIL5T2LIO3yU8Gbt26hT/++AMVK1ZU2u7p6YmUlBRERUWJ644cOYK8vDx4eHiIbSIiIpCdnS22CQ8PR506dWBtba1WPEwIiIhIL2hrUGFRpaWlITo6GtHR0QCAuLg4REdH48GDB8jOzkb//v1x/vx5hIaGIjc3FwkJCUhISEBWVhYAwM3NDZ07d8bIkSNx9uxZnDx5EgEBARg4cCCcnJwAAIMHD4axsTGGDx+OmJgYbN++HStWrMDEiRPVfn/YZUBERFQCzp8/j44dO4qv87+k/fz8MHv2bOzduxcA4O7urrTf0aNH0aFDBwBAaGgoAgIC0KlTJ0ilUvTr1w/BwcFiW7lcjkOHDsHf3x9NmzZFpUqVMHPmTLWnHALlJCG4cfBrWGqpP4feXafvPtV1CFSKatpa6DoEKgXPn2eU3sk0LfuruW+HDh0gCMIbt6vals/GxgZbt25V2aZRo0Y4ceKEesEVolwkBERERG/Dpx2qxjEERERExAoBERHpBz7+WDUmBEREpBfYZaAauwyIiIiIFQIiItIP7DJQjQkBERHpBXYZqMYuAyIiImKFgIiI9AMrBKoxISAiIr3AMQSqMSEgIiK9wAqBahxDQERERKwQEBGRfmCXgWpMCIiISC+wy0A1dhkQERERKwRERKQfJNCwy0BrkbybmBAQEZFekEokkGqQEWiyb1nALgMiIiJihYCIiPQDZxmoxoSAiIj0AmcZqMaEgIiI9IJU8mrRZP/yjGMIiIiIiBUCIiLSExINy/7lvELAhICIiPQCBxWqxi4DIiIiYoWAiIj0g+Sf/zTZvzxjQkBERHqBswxUY5cBERERsUJARET6gTcmUq1ICcHevXuLfMCePXsWOxgiIqKSwlkGqhUpIejdu3eRDiaRSJCbm6tJPERERKQDRRpDkJeXV6SFyQAREb2r8h9/rMmijoiICPTo0QNOTk6QSCTYvXu30nZBEDBz5kw4OjrCzMwMXl5euHXrllKb5ORk+Pr6QiaTwcrKCsOHD0daWppSm8uXL6Nt27YwNTWFs7MzFi1aVLz3p1h7/SMjI0OT3YmIiEpNfpeBJos60tPT0bhxY6xevbrQ7YsWLUJwcDBCQkJw5swZmJubw8fHR+m71dfXFzExMQgPD0dYWBgiIiIwatQocbtCoYC3tzdcXV0RFRWFxYsXY/bs2Vi/fr3a74/agwpzc3OxYMEChISEIDExETdv3kT16tURGBiIqlWrYvjw4WoHQUREVNJKe1Bhly5d0KVLl0K3CYKA5cuXY8aMGejVqxcAYPPmzbC3t8fu3bsxcOBAXL9+HQcOHMC5c+fQrFkzAMDKlSvRtWtXLFmyBE5OTggNDUVWVhZ++OEHGBsbo379+oiOjsbSpUuVEoeiULtC8NVXX2Hjxo1YtGgRjI2NxfUNGjTAd999p+7hiIiI9E5cXBwSEhLg5eUlrpPL5fDw8EBkZCQAIDIyElZWVmIyAABeXl6QSqU4c+aM2KZdu3ZK38c+Pj6IjY3Fs2fP1IpJ7YRg8+bNWL9+PXx9fWFgYCCub9y4MW7cuKHu4YiIiEqFtroMFAqF0pKZmal2LAkJCQAAe3t7pfX29vbitoSEBNjZ2SltNzQ0hI2NjVKbwo7x33MUldoJwaNHj1CzZs0C6/Py8pCdna3u4YiIiEqFtgYVOjs7Qy6Xi0tQUJCOr0w71B5DUK9ePZw4cQKurq5K63fs2IH33ntPa4ERERG9ix4+fAiZTCa+NjExUfsYDg4OAIDExEQ4OjqK6xMTE+Hu7i62SUpKUtovJycHycnJ4v4ODg5ITExUapP/Or9NUamdEMycORN+fn549OgR8vLy8OuvvyI2NhabN29GWFiYuocjIiIqFZJ/Fk32BwCZTKaUEBRHtWrV4ODggMOHD4sJgEKhwJkzZzB69GgAgKenJ1JSUhAVFYWmTZsCAI4cOYK8vDx4eHiIbb788ktkZ2fDyMgIABAeHo46derA2tparZjU7jLo1asX9u3bhz/++APm5uaYOXMmrl+/jn379uGDDz5Q93BERESlIn+WgSaLOtLS0hAdHY3o6GgArwYSRkdH48GDB5BIJBg/fjzmz5+PvXv34sqVK/jkk0/g5OQk3gzQzc0NnTt3xsiRI3H27FmcPHkSAQEBGDhwIJycnAAAgwcPhrGxMYYPH46YmBhs374dK1aswMSJE9V+f4r1LIO2bdsiPDy8OLsSERHphfPnz6Njx47i6/wvaT8/P2zcuBFTpkxBeno6Ro0ahZSUFLRp0wYHDhyAqampuE9oaCgCAgLQqVMnSKVS9OvXD8HBweJ2uVyOQ4cOwd/fH02bNkWlSpUwc+ZMtaccAoBEEAShuBd6/fp1AK/GFeSXM0qTQqGAXC5HXPxTWGpYvqF3X9R99abQUNlW09ZC1yFQKXj+XAH3Gg5ITU3VuAz/JvnfFR+uOwEjs+L/XGW/TMMvn7Ut0Vh1Se0KwV9//YVBgwbh5MmTsLKyAgCkpKSgVatW2LZtG6pUqaLtGImIiDTGpx2qpvYYghEjRiA7OxvXr19HcnIykpOTcf36deTl5WHEiBElESMRERGVMLUrBMePH8epU6dQp04dcV2dOnWwcuVKtG3bVqvBERERaVM5/yNfI2onBM7OzoXegCg3N1cc9UhERPSuYZeBamp3GSxevBhjx47F+fPnxXXnz5/HuHHjsGTJEq0GR0REpC1SieZLeVakCoG1tbVSZpSeng4PDw8YGr7aPScnB4aGhvj000/F+ZNERERUdhQpIVi+fHkJh0FERFSy2GWgWpESAj8/v5KOg4iIqERp69bF5VWx7lSYLyMjA1lZWUrryuPNGoiIiMo7tROC9PR0TJ06FT///DOePn1aYHtubq5WAiMiItKm/z7CuLj7l2dqzzKYMmUKjhw5grVr18LExATfffcd5syZAycnJ2zevLkkYiQiItKYRKL5Up6pXSHYt28fNm/ejA4dOmDYsGFo27YtatasCVdXV4SGhsLX17ck4iQiIqISpHaFIDk5GdWrVwfwarxAcnIyAKBNmzaIiIjQbnRERERaUtqPPy5r1K4QVK9eHXFxcXBxcUHdunXx888/o0WLFti3b5/4sCPSjrQXGVj87e84EHEFfz9LQ4PalTFnXF+4u7kAANJfZGJByD4cPHEFz1JfwMXJBp/2b4chvVvrOHJS5aedx7Dt1+NK6yo7VsSaJQEAgINHohBx6gruxD3Gy4wshK6fCgvzfx+HmvgkBT/vOo7L1+4hJSUNNtaWaN+6IT7s3Q5Ghgalei2k2rc/HUH4ySuIe/gEpsaGcK9XFRNHdEU1ZzsAQIriBVZvOYRTUTfxOOkZrOUW6NSqPsYO9YGluRkAYNehc5ix5OdCjx+xfRYqWvOpkEWladm/nOcD6icEw4YNw6VLl9C+fXtMmzYNPXr0wKpVq5CdnY2lS5eWRIx6a/LX2xB7NwErAj+GfSUZfj14HoPGr8GRH6fB0dYKc1buxskLtxAc+DGcHW1w/Gwsvly6A/aV5PBu00DX4ZMKLlVsMXf6J+JrA4N/i3WZmdl4r1FNvNeoJrZsP1xg30fxfyNPAMZ82h2ODja4/zAJq7/bh8zMbAzz9S6V+Klozl25g0E9W6FhbWfk5OZhxYb9GDn9W+z9djIqmBnjyVMFkp6mYtLI7qjhaof4xBTMDd6JpKcKLJ/56uejS3t3tGlWR+m4Xy7ZjqysHCYDpFVqJwQTJkwQ/+3l5YUbN24gKioKNWvWRKNGjdQ6VkREBBYvXoyoqCg8fvwYu3bt4p0O//EyMwu/H7+MH4KGo6V7DQDAF8O74I+TMdiy6ySmjOqGqKtx+LBLc7RqUgsA8HGvVgjdcwrR1+4zIXjHGUilsLYq/Jd5zy4tAQBXrt0rdHuTxjXRpHFN8bWDnTUePf4bB/44z4TgHbN+wUil119N+ghtB8zBtVt/oVmj6qhVzQErZv57nxcXp0oYN6wzpi78CTm5uTA0MICpiRFMTYzENskpaTgTfQfzJn5YatdRXnCWgWpqjyF4naurK/r27at2MgC8msLYuHFjrF69WtMwyp3c3Dzk5ubBxNhIab2piRHOXr4LAGjaoBrC/7yKx09SIAgCTl64hbsPn6Bdi7q6CJnUEJ+YjKH+32DU+BX4ZvWvePJ3qkbHe/EiExYWZlqKjkrK8/QMAIDcsoLKNhYVTGFoUHj3z94/omBmYgTvtur/ztV3nGWgWpEqBMHBwUU+4Oeff17ktl26dEGXLl2K3F6fWFQwRdMGVbF840HUrGoPW2tL7P7jAqJi7qFq5UoAgHkT+mHqou1o3mc2DA2kkEolWDTlI7GiQO+m2jUqY9xnvVDZsRKSU55j26/HMX3uBgQvHI0KZiZqH+9xQjJ+O3QWwwZ/UALRkrbk5eVhYchevFe/KmpVcyi0zbPUdISE/oEPu3q88Tg7D5xF147vKVUNqGh462LVipQQLFu2rEgHk0gkaiUE6srMzERmZqb4WqFQlNi53gUrAj/GF0E/oVnvWTAwkKJB7Sro5dUEV2IfAgA27IjAhZh72PD1CFR2sMGZS3fw5dKdsK8kR9vmdd5ydNKVpu61xH9XdbFH7RpVMHLccpw8E4MPOjRR61hPkxWYvehHtPKoB+/3m2o7VNKi+at24da9BGxZOqbQ7WnpGRg943vUcLHHmCGFd/1EX7uHuw+S8PWUQSUZKumpIiUEcXFxJR1HkQQFBWHOnDm6DqPUVK1cCTtXjcWLl5l4np4B+0pyjJ65ES5OlfAyMwsL1/+G7xZ8ik6t6gMA6tV0QsytRwj56SgTgjLEwtwUTo4V8TghWa39nj57jhlfbULdWs7wH96jhKIjbZi/aheOn76OTd+MgYOtVYHt6S8y8NmX38G8ggmCZ/u9cbbIzv1nUbeGE+rXrlLCEZdPUmjWT65xH/s7rkxd3/Tp05GamiouDx8+1HVIpaKCmQnsK8mRoniB42dvwLtNA+Tk5CE7J7dACctAKoEgCDqKlIrjZUYWEhKTYW1lWeR9niYrMGP+RtSo5oTPP+sFaXl/UHsZJQgC5q/ahcMnr+KHxZ+hiqNNgTZp6RkYOf1bGBkaYNWcYQXGDeVLf5mJAxGX0bdzi5IOu9zifQhU0+jhRqXNxMQEJibq97GWVcfOXIcgADVc7HDv0d+Yv3oParjY46NuHjAyNEBL9xr4as1emJoYoYqDDU5H38aOA+cxa2wvXYdOKmwIPYTmTWrDtpIVkp89x087j0EqlaJdq1czQ56lpOFZShoeJ76qGNx/mAgzUxPYVpLD0sIMT5MV+HL+JthWkmPY4A+gULwQj/2mmQukG/NW7sLvRy9i5ZyhqGBmgifJr7o5Lc3NYGpiJCYDGZlZ+HrqIKS9yEDai1cDD23kFkrTUQ8cu4Tc3Fz06KRetxJRUZWphEDfPE/LwNfrwvD4SQqsZObo0r4Rpo7qJpYT18zxw9frwjB27o9IUbxAFQdrTB3VlTcmesf9nazAklU78TztJeSWFeBWxwWL5gyHXGYOADhw+LzSjYv+b95GAMDno3qhU3t3RF+5i8eJyXicmIxPxyqP79kTOqvUroPebntYJABg6KQQpfXzJw1AH+/muHb7ES7feAAA6DJ0oVKbQ5uno7LDvxWFXw+ehVfrhpBxNkmxSSSAJsW0cl4ggETQYX05LS0Nt2/fBgC89957WLp0KTp27AgbGxu4uLi8dX+FQgG5XI64+Kew5GOXy72o+890HQKVopq2rHbog+fPFXCv4YDU1FTISuj3eP53xZifzsGkQvF/rjJfpGHNoOYlGqsu6bRCcP78eXTs2FF8PXHiRACAn58fNm7cqKOoiIiI9E+xEoITJ05g3bp1uHPnDnbs2IHKlStjy5YtqFatGtq0aVPk43To0IED4IiIqFTwPgSqqT3LYOfOnfDx8YGZmRkuXrwo3hcgNTUVCxYs0HqARERE2iCVaL6UZ2onBPPnz0dISAi+/fZbGBn9Oz2mdevWuHDhglaDIyIiotKhdpdBbGws2rVrV2C9XC5HSkqKNmIiIiLSOj7+WDW1KwQODg7izID/+vPPP1G9enWtBEVERKRt+U871GQpz9ROCEaOHIlx48bhzJkzkEgkiI+PR2hoKCZNmoTRo0eXRIxEREQak2phKc/Uvr5p06Zh8ODB6NSpE9LS0tCuXTuMGDECn332GcaOHVsSMRIREZU5ubm5CAwMRLVq1WBmZoYaNWpg3rx5SrPrBEHAzJkz4ejoCDMzM3h5eeHWrVtKx0lOToavry9kMhmsrKwwfPhwpKWlaT1etRMCiUSCL7/8EsnJybh69SpOnz6NJ0+eYN68eVoPjoiISFvyxxBosqhj4cKFWLt2LVatWoXr169j4cKFWLRoEVauXCm2WbRoEYKDgxESEoIzZ87A3NwcPj4+yMjIENv4+voiJiYG4eHhCAsLQ0REBEaNGqWtt0VU7BsTGRsbo169etqMhYiIqMRIodk4ACnU2/fUqVPo1asXunXrBgCoWrUqfvrpJ5w9exbAq+rA8uXLMWPGDPTq9eoZNJs3b4a9vT12796NgQMH4vr16zhw4ADOnTuHZs2aAQBWrlyJrl27YsmSJXBycir29bxO7YSgY8eOKm/OcOTIEY0CIiIiKg9atWqF9evX4+bNm6hduzYuXbqEP//8E0uXLgUAxMXFISEhAV5eXuI+crkcHh4eiIyMxMCBAxEZGQkrKysxGQAALy8vSKVSnDlzBn369NFavGonBO7u7kqvs7OzER0djatXr8LPz09bcREREWmVtqYdKhQKpfVvehLvtGnToFAoULduXRgYGCA3NxdfffUVfH19AQAJCQkAAHt7e6X97O3txW0JCQmws7NT2m5oaAgbGxuxjbaonRAsW7as0PWzZ88ukUEORERE2qDp3Qbz93V2dlZaP2vWLMyePbtA+59//hmhoaHYunUr6tevj+joaIwfPx5OTk7v5B/QWnu40ccff4wWLVpgyZIl2jokERHRO+fhw4dKTzssrDoAAJMnT8a0adMwcOBAAEDDhg1x//59BAUFwc/PDw4ODgCAxMREODo6ivslJiaK1XgHBwckJSUpHTcnJwfJycni/tqitWmVkZGRMDU11dbhiIiItEoi0ezmRPldBjKZTGl5U0Lw4sULSKXKX7MGBgbIy8sDAFSrVg0ODg44fPiwuF2hUODMmTPw9PQEAHh6eiIlJQVRUVFimyNHjiAvLw8eHh7afHvUrxD07dtX6bUgCHj8+DHOnz+PwMBArQVGRESkTaV96+IePXrgq6++gouLC+rXr4+LFy9i6dKl+PTTT/85ngTjx4/H/PnzUatWLVSrVg2BgYFwcnJC7969AQBubm7o3LkzRo4ciZCQEGRnZyMgIAADBw7U6gwDoBgJgVwuV3otlUpRp04dzJ07F97e3loLjIiIqCxbuXIlAgMDMWbMGCQlJcHJyQmfffYZZs6cKbaZMmUK0tPTMWrUKKSkpKBNmzY4cOCAUsU9NDQUAQEB6NSpE6RSKfr164fg4GCtxysR/nvLpLfIzc3FyZMn0bBhQ1hbW2s9GHUpFArI5XLExT+F5X/6c6h8irr/TNchUCmqaWuh6xCoFDx/roB7DQekpqYq9ctrU/53xYw9F2Bqblns42SkP8f8Xk1KNFZdUmsMgYGBAby9vflUQyIiKnMkWvivPFN7UGGDBg1w9+7dkoiFiIioxORPO9RkKc/UTgjmz5+PSZMmISwsDI8fP4ZCoVBaiIiIqOwp8qDCuXPn4osvvkDXrl0BAD179lS6hbEgCJBIJMjNzdV+lERERBrS1o2JyqsiJwRz5szB//73Pxw9erQk4yEiIioREolE5bN4irJ/eVbkhCB/MkL79u1LLBgiIiLSDbXuQ1DesyMiIiq/2GWgmloJQe3atd+aFCQnJ2sUEBERUUko7TsVljVqJQRz5swpcKdCIiIiKvvUSggGDhxY4LnMREREZUH+Q4o02b88K3JCwPEDRERUlnEMgWpFvjGRGo88ICIiojKmyBWC/Oc3ExERlUkaDios548yUP/xx0RERGWRFBJINfhW12TfsoAJARER6QVOO1RN7YcbERERUfnDCgEREekFzjJQjQkBERHpBd6HQDV2GRARERErBEREpB84qFA1JgRERKQXpNCwy6CcTztklwERERGxQkBERPqBXQaqMSEgIiK9IIVmZfHyXlIv79dHRERERcAKARER6QWJRAKJBnV/TfYtC5gQEBGRXpBAswcWlu90gAkBERHpCd6pUDWOISAiIiJWCIiISH+U77/xNcOEgIiI9ALvQ6AauwyIiIiICQEREemH/GmHmizqevToET7++GNUrFgRZmZmaNiwIc6fPy9uFwQBM2fOhKOjI8zMzODl5YVbt24pHSM5ORm+vr6QyWSwsrLC8OHDkZaWpvH78TomBEREpBekWljU8ezZM7Ru3RpGRkbYv38/rl27hm+++QbW1tZim0WLFiE4OBghISE4c+YMzM3N4ePjg4yMDLGNr68vYmJiEB4ejrCwMERERGDUqFHFfBfejGMIiIiISsDChQvh7OyMDRs2iOuqVasm/lsQBCxfvhwzZsxAr169AACbN2+Gvb09du/ejYEDB+L69es4cOAAzp07h2bNmgEAVq5cia5du2LJkiVwcnLSWrysEBARkV7QVpeBQqFQWjIzMws93969e9GsWTN8+OGHsLOzw3vvvYdvv/1W3B4XF4eEhAR4eXmJ6+RyOTw8PBAZGQkAiIyMhJWVlZgMAICXlxekUinOnDmj1feHCQEREekFiRYWAHB2doZcLheXoKCgQs939+5drF27FrVq1cLBgwcxevRofP7559i0aRMAICEhAQBgb2+vtJ+9vb24LSEhAXZ2dkrbDQ0NYWNjI7bRFnYZEBERqeHhw4eQyWTiaxMTk0Lb5eXloVmzZliwYAEA4L333sPVq1cREhICPz+/UolVHeUiIahgYghzk3JxKaSCm4Ps7Y2o3Cjvc77pFWOD0itUa+vhRjKZTCkheBNHR0fUq1dPaZ2bmxt27twJAHBwcAAAJCYmwtHRUWyTmJgId3d3sU1SUpLSMXJycpCcnCzury3sMiAiIr1Q2rMMWrdujdjYWKV1N2/ehKurK4BXAwwdHBxw+PBhcbtCocCZM2fg6ekJAPD09ERKSgqioqLENkeOHEFeXh48PDzUjEg1/llNRER6obQffzxhwgS0atUKCxYswIABA3D27FmsX78e69evF483fvx4zJ8/H7Vq1UK1atUQGBgIJycn9O7dG8CrikLnzp0xcuRIhISEIDs7GwEBARg4cKBWZxgATAiIiIhKRPPmzbFr1y5Mnz4dc+fORbVq1bB8+XL4+vqKbaZMmYL09HSMGjUKKSkpaNOmDQ4cOABTU1OxTWhoKAICAtCpUydIpVL069cPwcHBWo9XIgiCoPWjlhKFQgG5XI7Ep6lF6s+hsu2JovCpPVQ+cQyBfniuUKBeVTukppbc7/H874rQkzdRwcKy2Md5kfYcvq1rl2isusQKARER6QU+3Eg1DiokIiIiVgiIiEg/SCGBFMX/M1+TfcsCJgRERKQX2GWgGrsMiIiIiBUCIiLSD5J//tNk//KMCQEREekFdhmoxi4DIiIiYoWAiIj0g0TDWQbsMiAiIioH2GWgGhMCIiLSC0wIVOMYAiIiImKFgIiI9AOnHarGhICIiPSCVPJq0WT/8oxdBkRERMQKARER6Qd2GajGhICIiPQCZxmoxi4DIiIiYoWAiIj0gwSalf3LeYGACQEREekHzjJQjV0GRERExAoBERHpB84yUI0JARER6QXOMlCNCQEREekFCTQbGFjO8wGOISAiIiJWCIiISE9IIYFUg7q/tJzXCJgQEBGRXmCXgWrsMiAiIiJWCIiISE+wRKASEwIiItILvA+BauwyICIiIiYERESkJyT/3pyoOIsmBYKvv/4aEokE48ePF9dlZGTA398fFStWhIWFBfr164fExESl/R48eIBu3bqhQoUKsLOzw+TJk5GTk1P8QFRgQkBERHpBooWlOM6dO4d169ahUaNGSusnTJiAffv24ZdffsHx48cRHx+Pvn37ittzc3PRrVs3ZGVl4dSpU9i0aRM2btyImTNnFjMS1ZgQEBERlZC0tDT4+vri22+/hbW1tbg+NTUV33//PZYuXYr3338fTZs2xYYNG3Dq1CmcPn0aAHDo0CFcu3YNP/74I9zd3dGlSxfMmzcPq1evRlZWltZjZUJARET6QUslAoVCobRkZma+8ZT+/v7o1q0bvLy8lNZHRUUhOztbaX3dunXh4uKCyMhIAEBkZCQaNmwIe3t7sY2Pjw8UCgViYmI0eCMKx4SAiIj0gkQL/wGAs7Mz5HK5uAQFBRV6vm3btuHChQuFbk9ISICxsTGsrKyU1tvb2yMhIUFs899kIH97/jZt47RDIiLSC9p62uHDhw8hk8nE9SYmJgXaPnz4EOPGjUN4eDhMTU2Lf9JSxAoBERGRGmQymdJSWEIQFRWFpKQkNGnSBIaGhjA0NMTx48cRHBwMQ0ND2NvbIysrCykpKUr7JSYmwsHBAQDg4OBQYNZB/uv8NtrEhICIiPRCac4y6NSpE65cuYLo6GhxadasGXx9fcV/GxkZ4fDhw+I+sbGxePDgATw9PQEAnp6euHLlCpKSksQ24eHhkMlkqFevXnHfhjdilwEREemHUrx1saWlJRo0aKC0ztzcHBUrVhTXDx8+HBMnToSNjQ1kMhnGjh0LT09PtGzZEgDg7e2NevXqYciQIVi0aBESEhIwY8YM+Pv7F1qV0BQTAiIiIh1YtmwZpFIp+vXrh8zMTPj4+GDNmjXidgMDA4SFhWH06NHw9PSEubk5/Pz8MHfu3BKJRyIIglAiRy4FCoUCcrkciU9TlQZ4UPn0RPHmqT1U/mgy+IvKjucKBepVtUNqasn9Hs//roi48hcsLIt/jrTnCrRrWKVEY9UlVgiIiEgvaGuWQXnFQYVERETECgEREemHUhxTWCYxISAiIv3AjEAldhkQERERKwRERKQf/vs8guLuX54xISAiIr3AWQaqMSEgIiK9wCEEqnEMAREREbFCUFZ8v+MEfth5Ag8fJwMA6lZ3wOThXfBB6/o6jozUde7yHXy3/Rhibv2FpKcKrJ4zFB+0aShuT3+ZiSXf/oY/Tl5FiiIdVRwq4pO+bTCoRyuxzccT1+DspTtKxx3Y3RNzJ/Qvteugojl7SfnzXjNX+fOu9f4Xhe43ZVR3jBzYEQDQYdB8PEp8prR90oiu+Gxwp5ILvDxiiUAlJgRlhJOdFWYF9EINZ1sIgoCffjsD30nrcfzHaXCr4ajr8EgNL15moW4NJ/Tr0gIBszYW2B60di9OX7yFJdMHo7KDDf48H4s5K36FXUUZOrX692EpA7q1xLihPuJrMxPj0gif1PQy49Xn3b9LC/gX8nmf2jFL6fXxMzfwf0t+hk+7Rkrrxw3rjI+6eYivzc20/3Cb8o6DClXTaUIQFBSEX3/9FTdu3ICZmRlatWqFhQsXok6dOroM653UpV1DpdeBY3rih51/4vzVOCYEZUx7Dze093B74/aLMffQx7s5PNxrAnj1l//2sNO4fOOhUkJgZmIEW5vydz/18uZtn/frn+HhU1fR0r0GXJwqKq03NzPh500lSqdjCI4fPw5/f3+cPn0a4eHhyM7Ohre3N9LT03UZ1jsvNzcPOw+dx4uXWWjesJquwyEte69+VRyOjEHCk1QIgoDTF2/j3l9P0KZZbaV2ew9fQIs+geg2fDGWfPcbXmZk6Shi0pa/k5/j2Onr6N/Vo8C29T8dQfPegeg56ht8u+0ocnJzdRBh2ZY/y0CTpTzTaYXgwIEDSq83btwIOzs7REVFoV27djqK6t0Vc/sRfD79BhlZOTA3M8GWxSNRtzqrA+XNzIA+mLH0F7QbOBeGBlJIpBLMnzgAzRvVENt0f/89VLa3hl1FOWLvxmPxt78h7uETrJ4zVHeBk8Z+PXQO5hVM4NNWuSL4Sd+2qF+rMuSWFXAh5h6++e53PElW4P/G9NJRpGUThxCo9k6NIUhNTQUA2NjYFLo9MzMTmZn/PgJXoVCUSlzvilqu9ogInQ5F2kvsOXwRY2ZvQdi6cUwKypktu0/g0vX7CJn3KZzsrXHuyl3MDX41hqB101dVgoHdPcX2dao7wraiDH6TQvAg/m+4OFXSVeikoZ37z6JnpyYwMTZSWv/ph+3Ff9et4QQjIwPMXLoDX4zoBhPjd+rXOJVh78y0w7y8PIwfPx6tW7dGgwYNCm0TFBQEuVwuLs7OzqUcpW4ZGxmiurMt3N1cMCugFxrUqoyQbcd0HRZpUUZmNpZ+vx/TRvfE+63qo24NJwzp3QZdOjTGD78ce+N+jeu6AADuP/q7lCIlbTt3+S7uPnyCD7u1fGtb97quyMnNw6OE5FKIrByRaGEpx96ZhMDf3x9Xr17Ftm3b3thm+vTpSE1NFZeHDx+WYoTvnjxBQFZWjq7DIC3KyclFdk4upK91VhpIpcjLE9643/U78QAKDlCjsuOX/WfQoHYVuNVwemvba3ceQSqVoKK1RSlEVn5ItPBfefZO1JoCAgIQFhaGiIgIVKlS5Y3tTExMYGKin1Nt5qzaA69W9eHsYI3nLzKw48B5/Bl1CztXjtF1aKSm9JeZSn/J/5WQjGu3H8HKsgKc7K3RonENLFofBlMTo1ddBpfuYHf4eUwf/aq/+EH839h3+CLae9SFlcwcsXfjsWDNXjRvVB11i/BlQqWrwOf9WPnzBoDn6Rk4cPwypv2vR4H9L8bcQ/T1B2j5Xk2Ym5ng4rV7WLBmL3p5NYXcskKpXQeVfzpNCARBwNixY7Fr1y4cO3YM1apxxPyb/P0sDaNnb0bi3wrILExRv2Zl7Fw5Bh1VTGeid9PV2IcY8sVa8XXQ2r0AgD7ezbBw6iAsm/Exvvnud3yxIBSpz1/Ayd4aEz7tikE9Xo0bMDI0wKkLN7FpZwReZGTB0c4KPm0bYszHH+jkeki1q7EP8fHEfz/vBfmft08zLJo6CADw29GLEAQBPd5/r8D+xkaG+O3oRazcdBBZ2Tmo4lgRw/q3w7D+7Qu0JdX4LAPVJIIgvLkOWcLGjBmDrVu3Ys+ePUr3HpDL5TAzM3vr/gqFAnK5HIlPUyGTsVRa3j1RZL69EZUb5f2XL73yXKFAvap2SE0tud/j+d8VUTcfw8Ky+OdIe65A09qOJRqrLul0DMHatWuRmpqKDh06wNHRUVy2b9+uy7CIiKg84qBClXTeZUBERES6904MKiQiIippfJaBakwIiIhIP2h6++HynQ+8O/chICIiIt1hhYCIiPQCn2WgGhMCIiLSD8wIVGKXAREREbFCQERE+oGzDFRjQkBERHqBty5WjV0GRERExISAiIj0Q2nfuTgoKAjNmzeHpaUl7Ozs0Lt3b8TGxiq1ycjIgL+/PypWrAgLCwv069cPiYmJSm0ePHiAbt26oUKFCrCzs8PkyZORk5OjZjRvx4SAiIj0QylnBMePH4e/vz9Onz6N8PBwZGdnw9vbG+np6WKbCRMmYN++ffjll19w/PhxxMfHo2/fvuL23NxcdOvWDVlZWTh16hQ2bdqEjRs3YubMmcV9F95Ip0871BSfdqhf+LRD/VLe+2vpldJ82uGVuCRYavC0w+fPFWhYrfixPnnyBHZ2djh+/DjatWuH1NRU2NraYuvWrejfvz8A4MaNG3Bzc0NkZCRatmyJ/fv3o3v37oiPj4e9vT0AICQkBFOnTsWTJ09gbGxc7Ot5HSsEREREalAoFEpLZmbR/lhJTU0FANjY2AAAoqKikJ2dDS8vL7FN3bp14eLigsjISABAZGQkGjZsKCYDAODj4wOFQoGYmBhtXRIAJgRERKQnJPh3pkGxln+O4+zsDLlcLi5BQUFvPXdeXh7Gjx+P1q1bo0GDBgCAhIQEGBsbw8rKSqmtvb09EhISxDb/TQbyt+dv0yZOOyQiIr2grRsVPnz4UKnLwMTE5K37+vv74+rVq/jzzz81iKBksUJARESkBplMprS8LSEICAhAWFgYjh49iipVqojrHRwckJWVhZSUFKX2iYmJcHBwENu8Pusg/3V+G21hQkBERHpBo+6CYtzUSBAEBAQEYNeuXThy5AiqVaumtL1p06YwMjLC4cOHxXWxsbF48OABPD09AQCenp64cuUKkpKSxDbh4eGQyWSoV69e8d+MQrDLgIiI9ETpPt3I398fW7duxZ49e2BpaSn2+cvlcpiZmUEul2P48OGYOHEibGxsIJPJMHbsWHh6eqJly5YAAG9vb9SrVw9DhgzBokWLkJCQgBkzZsDf379IXRXqYEJARERUAtauXQsA6NChg9L6DRs2YOjQoQCAZcuWQSqVol+/fsjMzISPjw/WrFkjtjUwMEBYWBhGjx4NT09PmJubw8/PD3PnztV6vLwPAZUZvA+BfuF9CPRDad6H4Pr9J7DU4BzPFQq4udqWaKy6xAoBERHphdLtMCh7OKiQiIiIWCEgIiL9wMcfq8aEgIiI9ILkn/802b88Y0JARET6gYMIVOIYAiIiImKFgIiI9AMLBKoxISAiIr3AQYWqscuAiIiIWCEgIiL9wFkGqjEhICIi/cBBBCqxy4CIiIhYISAiIv3AAoFqTAiIiEgvcJaBauwyICIiIlYIiIhIX2g2y6C8dxowISAiIr3ALgPV2GVARERETAiIiIiIXQZERKQn2GWgGhMCIiLSC7x1sWrsMiAiIiJWCIiISD+wy0A1JgRERKQXeOti1dhlQERERKwQEBGRnmCJQCUmBEREpBc4y0A1dhkQERERKwRERKQfOMtANSYERESkFziEQDUmBEREpB+YEajEMQRERETECgEREekHzjJQjQkBERHpBQ4qVK1MJwSCIAAAnisUOo6ESsNzRaauQ6BSVN5/+dIrac+fA/j393lJUmj4XaHp/u+6Mp0QPP/nB6lmNWcdR0JERJp4/vw55HJ5iRzb2NgYDg4OqKWF7woHBwcYGxtrIap3j0QojbSshOTl5SE+Ph6WlpaQ6NGfEwqFAs7Oznj48CFkMpmuw6ESxM9af+jrZy0IAp4/fw4nJydIpSU3zj0jIwNZWVkaH8fY2BimpqZaiOjdU6YrBFKpFFWqVNF1GDojk8n06heHPuNnrT/08bMuqcrAf5mampbbL3Jt4bRDIiIiYkJARERETAjKJBMTE8yaNQsmJia6DoVKGD9r/cHPmnStTA8qJCIiIu1ghYCIiIiYEBARERETAiIiIgITAiIiIgITgjJn9erVqFq1KkxNTeHh4YGzZ8/qOiQqAREREejRowecnJwgkUiwe/duXYdEJSQoKAjNmzeHpaUl7Ozs0Lt3b8TGxuo6LNJDTAjKkO3bt2PixImYNWsWLly4gMaNG8PHxwdJSUm6Do20LD09HY0bN8bq1at1HQqVsOPHj8Pf3x+nT59GeHg4srOz4e3tjfT0dF2HRnqG0w7LEA8PDzRv3hyrVq0C8OpZDs7Ozhg7diymTZum4+iopEgkEuzatQu9e/fWdShUCp48eQI7OzscP34c7dq103U4pEdYISgjsrKyEBUVBS8vL3GdVCqFl5cXIiMjdRgZEWlTamoqAMDGxkbHkZC+YUJQRvz999/Izc2Fvb290np7e3skJCToKCoi0qa8vDyMHz8erVu3RoMGDXQdDumZMv20QyKi8sTf3x9Xr17Fn3/+qetQSA8xISgjKlWqBAMDAyQmJiqtT0xMhIODg46iIiJtCQgIQFhYGCIiIvT6se6kO+wyKCOMjY3RtGlTHD58WFyXl5eHw4cPw9PTU4eREZEmBEFAQEAAdu3ahSNHjqBatWq6Don0FCsEZcjEiRPh5+eHZs2aoUWLFli+fDnS09MxbNgwXYdGWpaWlobbt2+Lr+Pi4hAdHQ0bGxu4uLjoMDLSNn9/f2zduhV79uyBpaWlOCZILpfDzMxMx9GRPuG0wzJm1apVWLx4MRISEuDu7o7g4GB4eHjoOizSsmPHjqFjx44F1vv5+WHjxo2lHxCVGIlEUuj6DRs2YOjQoaUbDOk1JgRERETEMQRERETEhICIiIjAhICIiIjAhICIiIjAhICIiIjAhICIiIjAhICIiIjAhIBIY0OHDkXv3r3F1x06dMD48eNLPY5jx45BIpEgJSXljW0kEgl2795d5GPOnj0b7u7uGsV17949SCQSREdHa3QcIipZTAioXBo6dCgkEgkkEgmMjY1Rs2ZNzJ07Fzk5OSV+7l9//RXz5s0rUtuifIkTEZUGPsuAyq3OnTtjw4YNyMzMxO+//w5/f38YGRlh+vTpBdpmZWXB2NhYK+e1sbHRynGIiEoTKwRUbpmYmMDBwQGurq4YPXo0vLy8sHfvXgD/lvm/+uorODk5oU6dOgCAhw8fYsCAAbCysoKNjQ169eqFe/fuicfMzc3FxIkTYWVlhYoVK2LKlCl4/e7fr3cZZGZmYurUqXB2doaJiQlq1qyJ77//Hvfu3ROfV2BtbQ2JRCLeuz4vLw9BQUGoVq0azMzM0LhxY+zYsUPpPL///jtq164NMzMzdOzYUSnOopo6dSpq166NChUqoHr16ggMDER2dnaBduvWrYOzszMqVKiAAQMGIDU1VWn7d999Bzc3N5iamqJu3bpYs2aN2rEQkW4xISC9YWZmhqysLPH14cOHERsbi/DwcISFhSE7Oxs+Pj6wtLTEiRMncPLkSVhYWKBz587ift988w02btyIH374AX/++SeSk5Oxa9culef95JNP8NNPPyE4OBjXr1/HunXrYGFhAWdnZ+zcuRMAEBsbi8ePH2PFihUAgKCgIGzevBkhISGIiYnBhAkT8PHHH+P48eMAXiUuffv2RY8ePRAdHY0RI0Zg2rRpar8nlpaW2LhxI65du4YVK1bg22+/xbJly5Ta3L59Gz///DP27duHAwcO4OLFixgzZoy4PTQ0FDNnzsRXX32F69evY8GCBQgMDMSmTZvUjoeIdEggKof8/PyEXr16CYIgCHl5eUJ4eLhgYmIiTJo0Sdxub28vZGZmivts2bJFqFOnjpCXlyeuy8zMFMzMzISDBw8KgiAIjo6OwqJFi8Tt2dnZQpUqVcRzCYIgtG/fXhg3bpwgCIIQGxsrABDCw8MLjfPo0aMCAOHZs2fiuoyMDKFChQrCqVOnlNoOHz5cGDRokCAIgjB9+nShXr16StunTp1a4FivAyDs2rXrjdsXL14sNG3aVHw9a9YswcDAQPjrr7/Edfv37xekUqnw+PFjQRAEoUaNGsLWrVuVjjNv3jzB09NTEARBiIuLEwAIFy9efON5iUj3OIaAyq2wsDBYWFggOzsbeXl5GDx4MGbPni1ub9iwodK4gUuXLuH27duwtLRUOk5GRgbu3LmD1NRUPH78WOlx04aGhmjWrFmBboN80dHRMDAwQPv27Ysc9+3bt/HixQt88MEHSuuzsrLw3nvvAQCuX79e4LHXnp6eRT5Hvu3btyM4OBh37txBWloacnJyIJPJlNq4uLigcuXKSufJy8tDbGwsLC0tcefOHQwfPhwjR44U2+Tk5EAul6sdDxHpDhMCKrc6duyItWvXwtjYGE5OTjA0VP5xNzc3V3qdlpaGpk2bIjQ0tMCxbG1tixWDmZmZ2vukpaUBAH777TelL2Lg1bgIbYmMjISvry/mzJkDHx8fyOVybNu2Dd98843asX777bcFEhQDAwOtxUpEJY8JAZVb5ubmqFmzZpHbN2nSBNu3b4ednV2Bv5LzOTo64syZM2jXrh2AV38JR0VFoUmTJoW2b9iwIfLy8nD8+HF4eXkV2J5focjNzRXX1atXDyYmJnjw4MEbKwtubm7iAMl8p0+ffvtF/sepU6fg6uqKL7/8Ulx3//79Au0ePHiA+Ph4ODk5ieeRSqWoU6cO7O3t4eTkhLt378LX11et8xPRu4WDCon+4evri0qVKqFXr144ceIE4uLicOzYMXz++ef466+/AADjxo3D119/jd27d+PGjRsYM2aMynsIVK1aFX5+fvj000+xe/du8Zg///wzAMDV1RUSiQRhYWF48uQJ0tLSYGlpiUmTJmHChAnYtGkT7ty5gwsXLmDlypXiQL3//e9/uHXrFiZPnozY2Fhs3boVGzduVOt6a9WqhQcPHmDbtm24c+cOgoODCx0gaWpqCj8/P1y6dAknTpzA559/jgEDBsDBwQEAMGfOHAQFBSE4OBg3b97ElStXsGHDBixdulSteIhIt5gQEP2jQoUKiIiIgIuLC/r27Qs3NzcMHz4cGRkZYsXgiy++wJAhQ+Dn5wdPT09YWlqiT58+Ko+7du1a9O/fH2PGjEHdunUxcuRIpKenAwAqV66MOXPmYNq0abC3t0dAQAAAYN68eQgMDERQUBDc3NzQuXNn/Pbbb6hWrRqAV/36O3fuxO7du9G4cWOEhIRgwYIFal1vz549MWHCBAQEBMDd3R2nTp1CYGBggXY1a9ZE37590bVrV3h7e6NRo0ZK0wpHjBiB7777Dhs2bEDDhg3Rvn17bNy4UYyViMoGifCm0VBERESkN1ghICIiIiYERERExISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIwISAiIiIAPw/O29zjNpfUKAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Diagonal entries are the all correct predictions\n",
        "- everything odd the diagonal is wrong\n",
        "- 0 = no lesion, no liver\n",
        "- 1 = lesion visible\n",
        "- 2 = liver visible"
      ],
      "metadata": {
        "id": "-tdSay3GQ4LR"
      },
      "id": "-tdSay3GQ4LR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "num_classes = all_probs.shape[1]\n",
        "\n",
        "# binarize true labels\n",
        "y_true_bin = label_binarize(all_targets, classes=list(range(num_classes)))\n",
        "y_scores = all_probs.numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for i in range(num_classes):\n",
        "    PrecisionRecallDisplay.from_predictions(\n",
        "        y_true_bin[:, i],\n",
        "        y_scores[:, i],\n",
        "        name=f\"Class {i}\"\n",
        "    )\n",
        "\n",
        "plt.title(\"PrecisionRecall Curves  Multi-Class\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C8uBgACtPc87",
        "outputId": "3e507e2d-b7c1-4f55-b544-45b0944b302a"
      },
      "id": "C8uBgACtPc87",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}