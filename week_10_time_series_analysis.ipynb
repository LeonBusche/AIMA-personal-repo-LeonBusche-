{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhgw1j5kXw-"
      },
      "source": [
        "## Chapter 5: AI in Medicine - Time-Series Analysis\n",
        "### Part 1: Time Series Classification using Classical Machine Learning for Atrial Fibrillation Detection\n",
        "\n",
        "Welcome! In this part of the exercises, we'll walk through a full machine learning pipeline for detecting **Atrial Fibrillation (AF)** using ECG data.\n",
        "\n",
        "**This week's exercise has 7 tasks and 1 bonus task (all marked with a üß© symbol) for a total of 10+1 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "---\n",
        "\n",
        "We‚Äôll start with a short introduction to the topic.  \n",
        "After that, there will be time for you to work through the notebook, fill in missing code where needed, and ask questions - we‚Äôll be available to help throughout.\n",
        "\n",
        "---\n",
        "\n",
        "**üéØ Objectives**  \n",
        "- Load and understand ECG data  \n",
        "- Calculate features based on a scientific paper  \n",
        "- Train and evaluate classical ML classifiers for AF detection\n",
        "\n",
        "**ü©∫ Clinical Task**  \n",
        "- Classify short ECG windows as **AF** or **non-AF**\n",
        "\n",
        "**üß©  YOUR Tasks**\n",
        "- Task 1 ‚Äî Implement a Butterworth Bandpass Filter for ECG - **2 points**\n",
        "- Task 2 ‚Äî Detect R-peaks - **1 point**\n",
        "- Task 3 ‚Äî Compute Statistical Features from RR Intervals - **1 point**\n",
        "- Task 4 ‚Äî Leakage free Standarization of Features - **1 point**\n",
        "- Task 5 - Interpretation of error analysis **2 points**\n",
        "- Bonus Task ‚Äî Calculate imbalance ratio for XGBoost - **1 point**\n",
        "\n",
        "<br>\n",
        "\n",
        "### Part 2: Time Series Forecasting of ECG recording\n",
        "\n",
        "In Part 2 of this exercise, we'll reuse the raw ECG data for time series forecasting tasks. We will use a Transformer Model and a Time Series Foundation Model for this.\n",
        "\n",
        "---\n",
        "\n",
        "We‚Äôll start again with a short introduction to the topic.  \n",
        "After that, there will be time for you to work through the notebook, fill in missing code where needed, and ask questions.\n",
        "\n",
        "---\n",
        "\n",
        "**üéØ Objectives**  \n",
        "-  Understand the principles of time-series forecasting in physiological signals (e.g., ECG).\n",
        "- Learn how Transformer-based models can be adapted for medical time series data.\n",
        "-  Compare traditional and foundation model approaches for forecasting physiological signals.\n",
        "\n",
        "**ü©∫ Clinical Task**  \n",
        "- Forecast future ECG signal segments based on previous heart activity to anticipate potential arrhythmias or irregular patterns.\n",
        "\n",
        "**üß©  YOUR Tasks**\n",
        "- Task 6 ‚Äî Baseline & Comparison: Implement baseline forecasters and  compare model performance  - **2 points**\n",
        "- Task 7 ‚Äî Error Analysis: For the PatchTST model, identify and plot the time series windows for which the model produced the worst forecasts. - **1 point**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWdNIqaVmvip"
      },
      "source": [
        "# Part 1: Detecting Atrial Fibrillation in Long-Term ECG Recordings  \n",
        "## ECG fundamentals, what AF is, and the data we‚Äôll use  \n",
        "\n",
        "---\n",
        "\n",
        "**What is an ECG?**  \n",
        "An electrocardiogram is a voltage trace captured by electrodes on the skin. Each heartbeat follows a characteristic sequence:\n",
        "\n",
        "* **P-wave** ‚Äì depolarisation of the atria, leading to atrial contraction.  \n",
        "* **QRS complex** ‚Äì rapid depolarisation and contraction of the ventricles.  \n",
        "* **T-wave** ‚Äì ventricular repolarisation as the heart resets and the ventricles relax for the next beat.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gkts0jToIEZ"
      },
      "outputs": [],
      "source": [
        "# Let's see a graph of ECG where we can identify the P-wave, QRS complex, and T-wave\n",
        "from IPython.display import Image, display, Markdown\n",
        "display(Image(\"https://raw.githubusercontent.com/gernotpuc/ai-in-medicine-ts/main/ecg.png\"))\n",
        "\n",
        "display(Markdown(\n",
        "    \"*A single heartbeat on the ECG trace with the P-wave, QRS complex, and T-wave. Source: Keidar et al. 2021*\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVGChDE9rsBS"
      },
      "source": [
        "---\n",
        "\n",
        "### ü´Ä What is Atrial Fibrillation (AF) ?\n",
        "\n",
        "Atrial fibrillation is a common type of irregular heartbeat caused by disorganized electrical activity in the atria (the upper chambers of the heart). Instead of beating in a steady rhythm, the atria quiver rapidly and unpredictably.\n",
        "\n",
        "This condition is important to detect because it can lead to serious complications, such as stroke, heart failure, or blood clots. Early diagnosis allows timely treatment to reduce these risks.\n",
        "\n",
        "On an ECG, AF is identified by:\n",
        "- **No clear P-waves**, since the atria are not contracting normally  \n",
        "- **Irregular RR intervals**, meaning the time between heartbeats varies  \n",
        "- **Erratic baseline**, where the line between beats is uneven and unstable due to chaotic atrial signals  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBXDrADDw1ky"
      },
      "outputs": [],
      "source": [
        "# Let's compare ECG segments: atrial fibrillation vs. normal sinus rhythm\n",
        "from IPython.display import Image, display\n",
        "display(Image(\"https://raw.githubusercontent.com/gernotpuc/ai-in-medicine-ts/main/ecg_diff.jpeg\"))\n",
        "display(Markdown(\n",
        "    \"**ECG samples showing atrial fibrillation in the upper trace:** absence of P waves (red arrow), a chaotic baseline between QRS complexes, and irregular RR intervals.  \\n\"\n",
        "    \"**Bottom trace shows normal sinus rhythm:** visible P waves (purple arrow) and regular RR intervals.\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McY3M4wBxDsx"
      },
      "source": [
        "---\n",
        "### Dataset: Long-Term Atrial Fibrillation Database (LTAFDB)\n",
        "\n",
        "We‚Äôll use the Long-Term Atrial Fibrillation Database (LTAFDB), which contains 84 full-day ECG recordings (~24 hours each), sampled at 128‚ÄØHz with two leads.\n",
        "Each time point is labeled as AF (1) or non-AF (0).\n",
        "\n",
        "We'll work directly with the raw ECG signals. In order not to compute for too long, we will restrict our extraction to a single patient.\n",
        "\n",
        "Then, we will extract features, computed over fixed-length windows of 150 heartbeats - a setup based on the method from:\n",
        "\n",
        "Keidar et al., Frontiers in Physiology, 2021\n",
        "\n",
        "We will perform the windowing and feature extraction and use them to train a machine learning classifier that predicts whether each window shows AF or not.\n",
        "This kind of feature extraction from raw signals is a common practice in classical machine learning.\n",
        "\n",
        "üëâ In part II of the exercise, we‚Äôll also explore transformer models using raw ECG data for time series forecasting tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHdYEg9LyTJa"
      },
      "source": [
        "---\n",
        "### üìä The Features We‚Äôll Use\n",
        "\n",
        "The features are based on RR intervals - the time between two successive R-peaks in the ECG (the tall spikes in the signal).  \n",
        "R-peaks can be detected using standard algorithms.\n",
        "\n",
        "From the RR intervals, a new time series is created by taking the differences between consecutive RR values.  \n",
        "This captures how much the time between heartbeats changes from one beat to the next.\n",
        "\n",
        "Each 150-beat window is then summarized by three features:\n",
        "\n",
        "- **Mean** - the average change in RR intervals  \n",
        "- **Variability** - how much those changes fluctuate. Higher values mean more variance in timing between beats.  \n",
        "- **Normality** - how well the changes fit a normal (Gaussian) distribution, measured using the p-value from a Kolmogorov-Smirnov test (range: 0 to 1). Higher values mean the pattern looks more random and less structured.\n",
        "\n",
        "These features capture the irregularity of the rhythm and help classify each window as AF or non-AF.\n",
        "\n",
        "---\n",
        "\n",
        "**Useful links**  \n",
        "* Dataset: [PhysioNet - Long-Term AF Database (LTAFDB)](https://physionet.org/content/ltafdb/1.0.0/)  \n",
        "* Paper: [Keidar N. *et al.* 2021, *Frontiers in Physiology*](https://www.frontiersin.org/articles/10.3389/fphys.2021.637680/full)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8YU6zF3QWt"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ü©∫ AI in Medicine ‚Äî ECG Feature Extraction (Student Notebook)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Install dependencies\n",
        "!pip -q install \"wfdb==4.1.2\" \"numpy<2.0\" \"scipy==1.11.4\" \"pandas==2.2.2\" \"scikit-optimize\" tqdm gdown\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal, stats\n",
        "import wfdb\n",
        "from wfdb import processing as wfproc\n",
        "from tqdm import tqdm\n",
        "import os, matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Libraries loaded:\")\n",
        "print(\"wfdb :\", wfdb.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"scipy:\", signal.__name__, stats.__name__)\n",
        "print(\"pandas:\", pd.__version__)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4HU8Jjf37Yz"
      },
      "outputs": [],
      "source": [
        "# --- Download an exemplary short ECG records from PhysioNet ---\n",
        "!mkdir -p data\n",
        "import wfdb\n",
        "\n",
        "SAMPLES = [\"42\"]  # you could expand this list, but we'll stick to one sample only for the sake of performance.\n",
        "DB_NAME = \"ltafdb\"\n",
        "\n",
        "for rec in SAMPLES:\n",
        "    print(f\"Downloading record {rec} ...\")\n",
        "    wfdb.dl_database(DB_NAME, records=[rec], dl_dir=\"data\", keep_subdirs=False)\n",
        "\n",
        "print(\"\\n‚úÖ Download complete. Files in /data:\")\n",
        "!ls -lh data | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9ZNIknS-fSb"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ü©∫ Robust RR Feature Extraction from Raw ECG\n",
        "# In this part of the code, you'll find the following 3 tasks to complete:\n",
        "# üß© Task 1 ‚Äî Implement a Butterworth Bandpass Filter for ECG (2 points)\n",
        "# üß© Task 2 ‚Äî Detect R-peaks (1 point)\n",
        "# üß© Task 3 ‚Äî Compute Statistical Features from RR Intervals (1 point)\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal, stats\n",
        "import wfdb\n",
        "from wfdb import processing as wfproc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config ---\n",
        "WINDOW_BEATS = 150\n",
        "WINDOW_STEP = 1\n",
        "LEAD_INDEX = 0           # choose ECG lead\n",
        "LOWCUT, HIGHCUT = 0.5, 40.0\n",
        "FILTER_ORDER = 3\n",
        "\n",
        "\n",
        "# üß© Task 1 ‚Äî Implement a Butterworth Bandpass Filter for ECG\n",
        "\n",
        "def butter_bandpass(sig, fs, low=LOWCUT, high=HIGHCUT, order=FILTER_ORDER):\n",
        "    \"\"\"\n",
        "    Apply a Butterworth bandpass filter to an ECG signal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sig : ndarray\n",
        "        Raw ECG signal.\n",
        "    fs : float\n",
        "        Sampling frequency (Hz).\n",
        "    low, high : float\n",
        "        Lower and upper cutoff frequencies.\n",
        "    order : int\n",
        "        Filter order (higher = steeper cutoff, but more phase distortion).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sig_f : ndarray\n",
        "        Filtered ECG signal.\n",
        "    \"\"\"\n",
        "    # --- TODO: Compute the Nyquist frequency\n",
        "    nyq = None\n",
        "\n",
        "    # --- TODO: Normalize cutoff frequencies between 0 and 1\n",
        "    low_norm = None\n",
        "    high_norm = None\n",
        "\n",
        "    # --- TODO: Design a Butterworth bandpass filter\n",
        "    # Use signal.butter\n",
        "    b, a = None, None\n",
        "\n",
        "    # --- TODO: Apply zero-phase filtering using signal.filtfilt() ---\n",
        "    sig_f = None\n",
        "\n",
        "    return sig_f\n",
        "\n",
        "# üß© Task 2 ‚Äî Detect R-peaks\n",
        "\n",
        "def detect_clean_rpeaks(sig_f, fs):\n",
        "    \"\"\"\n",
        "    Detect and clean R-peaks from a filtered ECG signal.\n",
        "\n",
        "    Try XQRS detection (fast + adaptive).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # --- TODO: Try detecting R-peaks using the WFDB XQRS detector ---\n",
        "    try:\n",
        "        # Hint: create an XQRS object with wfproc.XQRS(sig=sig_f, fs=fs)\n",
        "        # then call its .detect() method and extract .qrs_inds\n",
        "        xqrs = None\n",
        "        rpeaks = None\n",
        "    except Exception:\n",
        "        # Fallback to GQRS if XQRS fails\n",
        "        rpeaks = np.asarray(wfproc.gqrs_detect(sig=sig_f, fs=fs), dtype=int)\n",
        "\n",
        "\n",
        "    # Remove peaks that are too close (<0.35 s apart)\n",
        "    min_distance = int(0.35 * fs)\n",
        "    if len(rpeaks) > 1:\n",
        "        rpeaks = rpeaks[np.insert(np.diff(rpeaks) > min_distance, 0, True)]\n",
        "\n",
        "    # Normalize ECG polarity\n",
        "    if len(rpeaks) > 0 and np.mean(sig_f[rpeaks]) < 0:\n",
        "        sig_f = -sig_f\n",
        "\n",
        "    # Keep only peaks with the dominant polarity\n",
        "    if len(rpeaks) > 0:\n",
        "        polarities = np.sign(sig_f[rpeaks])\n",
        "        dominant_sign = np.sign(np.median(sig_f[rpeaks]))\n",
        "        rpeaks = rpeaks[polarities == dominant_sign]\n",
        "\n",
        "    print(f\"‚úÖ Detected {len(rpeaks)} R-peaks\")\n",
        "    return rpeaks, sig_f\n",
        "\n",
        "\n",
        "# --- Helper: Clean RR intervals ---\n",
        "def clean_rr(rr):\n",
        "    \"\"\"Keep only physiologically plausible RR intervals (0.4‚Äì2.0 s).\"\"\"\n",
        "    rr = np.asarray(rr)\n",
        "    return rr[(rr > 0.4) & (rr < 2.0)]\n",
        "\n",
        "\n",
        "# üß© Task 3 ‚Äî Compute Statistical Features from RR Intervals\n",
        "\n",
        "def compute_window_features(rr_sec):\n",
        "    \"\"\"\n",
        "    Compute three features from a sequence of RR intervals (seconds):\n",
        "\n",
        "    1. Mean change in RR (ŒîRR)\n",
        "    2. Variability (standard deviation of ŒîRR)\n",
        "    3. Normality (KS-test p-value of standardized ŒîRR)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rr_sec : array-like\n",
        "        Sequence of RR intervals in seconds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mean_drr : float\n",
        "        Mean of ŒîRR values.\n",
        "    std_drr : float\n",
        "        Standard deviation of ŒîRR values.\n",
        "    ks_pval : float\n",
        "        p-value from KS-test (1 ‚Üí normal, 0 ‚Üí non-normal).\n",
        "    \"\"\"\n",
        "    rr_sec = np.asarray(rr_sec)\n",
        "\n",
        "    # --- Handle very short sequences ---\n",
        "    if len(rr_sec) < 2:\n",
        "        return np.nan, np.nan, np.nan\n",
        "\n",
        "    # --- TODO 1: Compute ŒîRR values ---\n",
        "    drr = None   # Hint: difference between consecutive RR intervals\n",
        "\n",
        "    # --- TODO 2: Compute the mean of ŒîRR ---\n",
        "    mean_drr = None\n",
        "\n",
        "    # --- TODO 3: Compute standard deviation (sample std, ddof=1) ---\n",
        "    std_drr = None\n",
        "\n",
        "    # --- TODO 4: Compute KS-test p-value if variance > 0 ---\n",
        "    if std_drr > 1e-9:\n",
        "        # Standardize ŒîRR to z-scores\n",
        "        z = None\n",
        "        _, ks_pval = stats.kstest(z, \"norm\")\n",
        "    else:\n",
        "        ks_pval = np.nan\n",
        "\n",
        "    return mean_drr, std_drr, ks_pval\n",
        "\n",
        "\n",
        "# --- Main processing function ---\n",
        "def process_record(record_path):\n",
        "    rec = wfdb.rdrecord(record_path)\n",
        "    fs = rec.fs\n",
        "    sig = rec.p_signal[:, LEAD_INDEX].astype(float)\n",
        "\n",
        "    # 1Ô∏è‚É£ Filter ECG\n",
        "    sig_f = butter_bandpass(sig, fs, LOWCUT, HIGHCUT, FILTER_ORDER)\n",
        "\n",
        "    # 2Ô∏è‚É£ Detect and clean R-peaks (Task 2)\n",
        "    rpeaks, sig_f = detect_clean_rpeaks(sig_f, fs)\n",
        "\n",
        "    # 3Ô∏è‚É£ Compute RR intervals and clean\n",
        "    rr_intervals = np.diff(rpeaks) / fs\n",
        "    rr_intervals = clean_rr(rr_intervals)\n",
        "\n",
        "    if len(rr_intervals) < WINDOW_BEATS:\n",
        "        raise RuntimeError(f\"Too few clean RR intervals ({len(rr_intervals)}) for a {WINDOW_BEATS}-beat window.\")\n",
        "\n",
        "    # 5Ô∏è‚É£ Build overlapping windows\n",
        "    windows = [rr_intervals[i:i+WINDOW_BEATS]\n",
        "               for i in range(0, len(rr_intervals) - WINDOW_BEATS + 1, WINDOW_STEP)]\n",
        "\n",
        "    # 6Ô∏è‚É£ Compute features\n",
        "    feats = [compute_window_features(w) for w in windows]\n",
        "    df = pd.DataFrame(feats, columns=[\"mean_delta_rr\", \"variability_delta_rr\", \"ks_pvalue_delta_rr\"])\n",
        "\n",
        "    print(f\"‚úÖ Processed {record_path}\")\n",
        "    print(f\"Detected {len(rpeaks)} R-peaks, {len(rr_intervals)} valid RR intervals\")\n",
        "    print(f\"Created {len(windows)} windows of {WINDOW_BEATS} beats each\")\n",
        "\n",
        "    return df, rr_intervals, rpeaks, sig_f, fs\n",
        "\n",
        "# ============================================================\n",
        "# üß™ Run on one example record\n",
        "# ============================================================\n",
        "try:\n",
        "    record_path = \"data/42\"\n",
        "    df, rr_intervals, rpeaks, sig_f, fs = process_record(record_path)\n",
        "\n",
        "    # --- Feature summary ---\n",
        "    print(\"\\nFeature Descriptive Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    # --- Plot ECG segment (150‚Äì200 s) with detected R-peaks ---\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    start_s, end_s = 150, 160\n",
        "    start_idx, end_idx = int(start_s * fs), int(end_s * fs)\n",
        "    time_segment = np.arange(start_idx, end_idx) / fs\n",
        "    sig_segment = sig_f[start_idx:end_idx]\n",
        "\n",
        "    plt.plot(time_segment, sig_segment, label=\"Filtered ECG\")\n",
        "    rpeaks_in_window = rpeaks[(rpeaks >= start_idx) & (rpeaks < end_idx)]\n",
        "    plt.scatter(rpeaks_in_window / fs, sig_f[rpeaks_in_window],\n",
        "                c='r', marker='o', s=40, label=\"Detected R-peaks\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(f\"R-peak detection sanity check ({start_s}‚Äì{end_s} s)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.4)\n",
        "    plt.show()\n",
        "\n",
        "    # --- RR interval distribution ---\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.hist(rr_intervals, bins=50, color='steelblue', edgecolor='k')\n",
        "    plt.xlabel(\"RR interval (s)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Distribution of cleaned RR intervals\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n‚úÖ Clean features ready in df:\", df.shape)\n",
        "\n",
        "except RuntimeError as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "except Exception as e:\n",
        "    if \"No such file or directory\" in str(e) or \"FileNotFoundError\" in str(e):\n",
        "        print(f\"‚ùå Missing data file ('data/00'): {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Unexpected error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEzGk1XnHwZs"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ü©∫ Derive AF labels for each 150-beat window\n",
        "# NO Tasks - just try to understand the code and execute it.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import wfdb\n",
        "\n",
        "# --- 1Ô∏è‚É£ Read AF rhythm intervals from annotations ---\n",
        "def rhythm_af_intervals(record_path):\n",
        "    \"\"\"\n",
        "    Reads WFDB rhythm annotations and extracts all intervals\n",
        "    labeled as (AFIB in the auxiliary notes.\n",
        "    Returns a list of (start_sample, end_sample) tuples.\n",
        "    \"\"\"\n",
        "    ann = wfdb.rdann(record_path, 'atr')\n",
        "    aux = np.array(ann.aux_note, dtype=object)\n",
        "    samp = np.array(ann.sample, dtype=int)\n",
        "\n",
        "    af_intervals = []\n",
        "    current_start = None\n",
        "    for a, s in zip(aux, samp):\n",
        "        label = a.strip().upper()\n",
        "        if label.startswith('(AFIB'):\n",
        "            current_start = s\n",
        "        elif label.startswith('(') and not label.startswith('(AFIB'):\n",
        "            if current_start is not None:\n",
        "                af_intervals.append((current_start, s))\n",
        "                current_start = None\n",
        "    # If the last AFIB segment continues to the end of the record:\n",
        "    if current_start is not None:\n",
        "        af_intervals.append((current_start, None))\n",
        "    return af_intervals\n",
        "\n",
        "\n",
        "# --- 2Ô∏è‚É£ Determine which beats fall inside AF intervals ---\n",
        "def beats_in_af_mask(rpeaks, af_intervals):\n",
        "    \"\"\"\n",
        "    Create a boolean mask for each beat (R-peak):\n",
        "    True if that beat lies inside any AF interval.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(len(rpeaks), dtype=bool)\n",
        "    for start, end in af_intervals:\n",
        "        if end is None:\n",
        "            mask |= (rpeaks >= start)\n",
        "        else:\n",
        "            mask |= ((rpeaks >= start) & (rpeaks < end))\n",
        "    return mask\n",
        "\n",
        "\n",
        "# --- 3Ô∏è‚É£ Window-level labeling rule ---\n",
        "def window_label_majority(beat_is_af, start_idx, window_beats=150):\n",
        "    \"\"\"\n",
        "    Label a window as AF (1) if >=50% of its beats are AF; otherwise 0.\n",
        "    \"\"\"\n",
        "    w = beat_is_af[start_idx:start_idx + window_beats]\n",
        "    return int(np.mean(w) >= 0.5) if w.size else 0\n",
        "\n",
        "\n",
        "# --- 4Ô∏è‚É£ Apply to our record ---\n",
        "af_intervals = rhythm_af_intervals(record_path)\n",
        "beat_is_af = beats_in_af_mask(rpeaks, af_intervals)\n",
        "\n",
        "labels = []\n",
        "for i in range(0, len(rr_intervals) - WINDOW_BEATS + 1, WINDOW_STEP):  # ‚úÖ +1 ensures alignment with df\n",
        "    y = window_label_majority(beat_is_af, i, WINDOW_BEATS)\n",
        "    labels.append(y)\n",
        "\n",
        "labels = np.array(labels)\n",
        "print(f\"‚úÖ Labeled {len(labels)} windows ({np.sum(labels)} AF, {len(labels) - np.sum(labels)} non-AF)\")\n",
        "\n",
        "# --- 5Ô∏è‚É£ Add to the feature DataFrame ---\n",
        "if len(labels) == len(df):\n",
        "    df[\"label_af\"] = labels\n",
        "    print(\"\\n‚úÖ Labels successfully added to feature DataFrame.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Length mismatch: df has {len(df)}, labels have {len(labels)}\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHOOz-C7DdGs"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üìä Visualize feature relationships\n",
        "# NO Tasks - just try to understand the code and execute it.\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make sure your DataFrame looks like this:\n",
        "print(df.columns)\n",
        "# ['mean_delta_rr', 'variability_delta_rr', 'ks_pvalue_delta_rr', 'label_af']\n",
        "\n",
        "def plot_feature_pair(df, feat_x, feat_y):\n",
        "    \"\"\"\n",
        "    Plot two features against each other, color-coded by AF label.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    # Plot AF first (red)\n",
        "    plt.scatter(\n",
        "        df.loc[df[\"label_af\"] == 1, feat_x],\n",
        "        df.loc[df[\"label_af\"] == 1, feat_y],\n",
        "        c=\"red\", s=3, label=\"AF\", alpha=0.1\n",
        "    )\n",
        "    # Plot non-AF second (green) so it lies on top\n",
        "    plt.scatter(\n",
        "        df.loc[df[\"label_af\"] == 0, feat_x],\n",
        "        df.loc[df[\"label_af\"] == 0, feat_y],\n",
        "        c=\"green\", s=3, label=\"non-AF\", alpha=0.1\n",
        "    )\n",
        "    plt.xlabel(feat_x.replace(\"_\", \" \").title())\n",
        "    plt.ylabel(feat_y.replace(\"_\", \" \").title())\n",
        "    plt.title(f\"{feat_x.replace('_',' ').title()} vs {feat_y.replace('_',' ').title()}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ‚úÖ Try all feature pairs\n",
        "plot_feature_pair(df, \"variability_delta_rr\", \"ks_pvalue_delta_rr\")  # Variability vs Normality\n",
        "plot_feature_pair(df, \"variability_delta_rr\", \"mean_delta_rr\")       # Variability vs Mean\n",
        "plot_feature_pair(df, \"ks_pvalue_delta_rr\", \"mean_delta_rr\")         # Normality vs Mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjqA4iCaGvJF"
      },
      "outputs": [],
      "source": [
        "# Using the NumPy labels array you already built\n",
        "labels_flat = labels.ravel()  # same as flatten, but cheaper\n",
        "\n",
        "n_total = labels_flat.size\n",
        "n_af = int(np.sum(labels_flat == 1))\n",
        "n_non = int(np.sum(labels_flat == 0))\n",
        "\n",
        "print(\"AF samples (label 1):   \", n_af,  f\"({n_af / n_total:.1%})\")\n",
        "print(\"Non-AF samples (label 0):\", n_non, f\"({n_non / n_total:.1%})\")\n",
        "print(\"Total windows:           \", n_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDs0yljh-xF"
      },
      "source": [
        "### ‚úÇÔ∏è Splitting the Data ‚Äì Train vs Test\n",
        "\n",
        "To evaluate our model fairly, we split the dataset into a **training set** and a **test set**.  \n",
        "We use 80% for training and 20% for testing, and apply **stratified sampling** to keep the AF/non-AF ratio balanced in both splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hosua1i5G-Z4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# --- Clean feature columns ---\n",
        "df[\"mean_delta_rr\"] = df[\"mean_delta_rr\"].fillna(0)\n",
        "df[\"variability_delta_rr\"] = df[\"variability_delta_rr\"].fillna(0)\n",
        "df[\"ks_pvalue_delta_rr\"] = df[\"ks_pvalue_delta_rr\"].fillna(0)\n",
        "\n",
        "# Optional: if some windows have 0 variability (flat signal),\n",
        "# the KS test is meaningless ‚Äî so cap or replace invalid p-values:\n",
        "df.loc[df[\"variability_delta_rr\"] == 0, \"ks_pvalue_delta_rr\"] = 0.0\n",
        "\n",
        "# --- Extract features and labels from your dataframe ---\n",
        "X = df[[\"mean_delta_rr\", \"variability_delta_rr\", \"ks_pvalue_delta_rr\"]].values\n",
        "y = df[\"label_af\"].values.astype(int)\n",
        "\n",
        "# --- Split into train and test sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,          # preserve AF/non-AF ratio\n",
        "    random_state=23\n",
        ")\n",
        "\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set:\",  X_test.shape,  y_test.shape)\n",
        "print(f\"AF ratio (train): {y_train.mean():.3f}, AF ratio (test): {y_test.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1F4s6cVaC4l"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üß© Task 4 ‚Äî Leakage free Standarization of Features (1 point)\n",
        "# ============================================================\n",
        "# Goal: Scale all numeric features to have mean 0 and standard deviation 1.\n",
        "#\n",
        "# Why?\n",
        "#   - Many ML models work best when input features are on similar scales.\n",
        "#   - Prevents large-valued features (e.g., variability) from dominating others.\n",
        "#\n",
        "# Steps:\n",
        "#   Create and fit a StandardScaler in a way that prevents data leakage)\n",
        "#   Apply the scaling to both training and test sets\n",
        "#   Inspect the resulting mean and standard deviation per feature and per class\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# --- TODO: Fit the scaler (hint: use .fit())\n",
        "\n",
        "\n",
        "# --- TODO: Transform both training and test sets ---\n",
        "X_train_scaled = None\n",
        "X_test_scaled  = None\n",
        "\n",
        "# --- Inspect scaling statistics ---\n",
        "def print_stats(X, y, label):\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    print(\"Overall:\")\n",
        "    print(\"  Mean:\", np.round(np.mean(X, axis=0), 3))\n",
        "    print(\"  Std: \", np.round(np.std(X, axis=0), 3))\n",
        "\n",
        "    for cls in [0, 1]:\n",
        "        cls_name = \"non-AF\" if cls == 0 else \"AF\"\n",
        "        X_cls = X[y == cls]\n",
        "        print(f\"Class {cls} ({cls_name}):\")\n",
        "        print(\"  Mean:\", np.round(np.mean(X_cls, axis=0), 3))\n",
        "        print(\"  Std: \", np.round(np.std(X_cls, axis=0), 3))\n",
        "\n",
        "# --- View statistics after scaling ---\n",
        "print_stats(X_train_scaled, y_train, \"Training Set\")\n",
        "print_stats(X_test_scaled, y_test, \"Test Set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfgFEHnaiKHE"
      },
      "source": [
        "As expected, the overall means and standard deviations of the **training set** after standardization are centered around **0** and **1**.\n",
        "\n",
        "We also see that in the **test set**, the means remain close to 0 and standard deviations close to 1 -  \n",
        "confirming that the scaler generalizes well **without data leakage**.\n",
        "\n",
        "However, when inspecting the statistics **per class (AF vs non-AF)**, we observe that each group maintains **distinct distributions** even after scaling.  \n",
        "This is expected - while the scaler normalizes the overall distribution, it does **not eliminate class-specific differences**, which is important for classification.\n",
        "\n",
        "This confirms that `StandardScaler` worked correctly on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1URmOe7iQvO"
      },
      "source": [
        "## üß† Let's Classify! Starting Simple with Logistic Regression\n",
        "\n",
        "Now that our features are scaled and ready, let's begin with a classic and interpretable baseline: **logistic regression**.  \n",
        "This model will try to separate AF from non-AF windows based on our three engineered features: variability, normality, and mean of RR differences.\n",
        "\n",
        "We‚Äôll use class balancing to handle the AF class being less frequent. Let's see how it performs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-wivYdO0guP"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚ö° Logistic Regression (Linear Model) for AF Detection\n",
        "# NO Task here - just try to understand the code and execute it.\n",
        "# ============================================================\n",
        "import xgboost as xgb # Retaining for comparison if needed\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, precision_recall_curve,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, auc, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# ‚ö†Ô∏è NEW IMPORT: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# --- Placeholder for Data and Imbalance ---\n",
        "# X_train_scaled, y_train, X_test_scaled, y_test are assumed to be defined globally.\n",
        "# For Logistic Regression, class imbalance is handled via class_weight.\n",
        "\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Step 1 ‚Äî Hyperparameter grid (Tuned for Logistic Regression)\n",
        "# ============================================================\n",
        "# The main hyperparameter for Logistic Regression with L2 penalty is 'C'.\n",
        "# C is the inverse of regularization strength (smaller C means stronger regularization).\n",
        "param_dist = {\n",
        "    # C is generally searched on a log scale (e.g., 0.001 to 1000)\n",
        "    'C': Real(1e-3, 1e3, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# ‚ö†Ô∏è Model Setup: Logistic Regression\n",
        "log_reg_base = LogisticRegression(\n",
        "    penalty='l2',                     # Use L2 regularization\n",
        "    solver='liblinear',               # Good solver for small datasets\n",
        "    class_weight='balanced',          # CRITICAL: Handle class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# ‚ö†Ô∏è Using BayesSearchCV\n",
        "grid = BayesSearchCV(\n",
        "    log_reg_base,\n",
        "    search_spaces=param_dist,\n",
        "    n_iter=5,                      # Linear models need fewer iterations than tree models\n",
        "    cv=3,                           # Use 5-fold CV for more stable estimation\n",
        "    scoring='roc_auc',                   # Optimize directly for F1 Score\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True,\n",
        ")\n",
        "\n",
        "print(\"\\n‚è≥ Running Bayesian Search for Logistic Regression...\")\n",
        "# Note: X_train_scaled and y_train are assumed to be defined globally\n",
        "# No need for early stopping setup here as LR is not sequential like XGBoost\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ Bayesian Search complete.\")\n",
        "\n",
        "# ============================================================\n",
        "# üèÜ Step 2 ‚Äî Train with best parameters\n",
        "# ============================================================\n",
        "best_params = grid.best_params_\n",
        "print(f\"\\nüèÜ Best parameters from cross-validation:\")\n",
        "for k, v in best_params.items():\n",
        "    # C parameter often comes back as 'C_inv' or similar from skopt, adjusting key for clarity\n",
        "    print(f\"  C                  : {v}\")\n",
        "\n",
        "# Train the final classifier on the full training set\n",
        "log_reg_clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    solver='liblinear',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    C=best_params['C'] # Pass the best C value\n",
        ")\n",
        "log_reg_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ============================================================\n",
        "# üîç Step 3 ‚Äî Predictions & probabilities\n",
        "# ============================================================\n",
        "y_pred_train  = log_reg_clf.predict(X_train_scaled)\n",
        "# Logistic Regression uses predict_proba to get probabilities\n",
        "y_proba_train = log_reg_clf.predict_proba(X_train_scaled)[:, 1]\n",
        "\n",
        "y_pred_test  = log_reg_clf.predict(X_test_scaled)\n",
        "y_proba_test = log_reg_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# ============================================================\n",
        "# üìä Step 4 ‚Äî Evaluation helper\n",
        "# ============================================================\n",
        "def evaluate_model(y_true, y_pred, y_proba, label=\"Set\"):\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec  = recall_score(y_true, y_pred)\n",
        "    f1   = f1_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_proba)\n",
        "    print(f\"\\nüìä Evaluation on {label}:\")\n",
        "    print(f\"  Accuracy : {acc:.4f}\")\n",
        "    print(f\"  Precision: {prec:.4f}\")\n",
        "    print(f\"  Recall   : {rec:.4f}\")\n",
        "    print(f\"  F1 Score : {f1:.4f}\")\n",
        "    print(f\"  ROC AUC  : {auc_roc:.4f}\")\n",
        "    return acc, prec, rec, f1, auc_roc\n",
        "\n",
        "# --- Evaluate both training & test performance ---\n",
        "evaluate_model(y_train, y_pred_train, y_proba_train, \"Training Set (Threshold=0.5)\")\n",
        "evaluate_model(y_test,  y_pred_test,  y_proba_test,  \"Test Set (Threshold=0.5)\")\n",
        "\n",
        "# ============================================================\n",
        "# ü©∏ Step 5 ‚Äî Precision‚ÄìRecall curve & threshold tuning\n",
        "# ============================================================\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_test)\n",
        "pr_auc = auc(recalls, precisions)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(recalls, precisions, label=f'PR Curve (AUC={pr_auc:.3f})')\n",
        "plt.xlabel(\"Recall (Sensitivity)\")\n",
        "plt.ylabel(\"Precision (PPV)\")\n",
        "plt.title(\"Precision‚ÄìRecall Curve (Test Set)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Find optimal threshold for maximum F1 ---\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n‚öôÔ∏è Optimal probability threshold (max F1): {best_thresh:.3f}\")\n",
        "\n",
        "# --- Apply optimized threshold ---\n",
        "y_pred_opt = (y_proba_test >= best_thresh).astype(int)\n",
        "evaluate_model(y_test, y_pred_opt, y_proba_test, \"Test Set (Optimized Threshold)\")\n",
        "\n",
        "# ============================================================\n",
        "# üìà Step 6 ‚Äî Precision, Recall, and F1 vs Threshold\n",
        "# ============================================================\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(thresholds, precisions[:-1], label=\"Precision\", color=\"green\")\n",
        "plt.plot(thresholds, recalls[:-1], label=\"Recall\", color=\"red\")\n",
        "plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\", color=\"blue\")\n",
        "plt.axvline(best_thresh, color='black', linestyle='--', label=f\"Best F1={best_thresh:.2f}\")\n",
        "plt.xlabel(\"Probability Threshold\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Precision‚ÄìRecall‚ÄìF1 vs Threshold\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Step 7 ‚Äî Confusion Matrix (Optimized Threshold)\n",
        "# ============================================================\n",
        "cm = confusion_matrix(y_test, y_pred_opt)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non-AF', 'AF'])\n",
        "disp.plot(cmap='Blues', values_format='d') # Changed cmap for visual difference\n",
        "plt.title(\"Confusion Matrix (Logistic Regression, Optimized Threshold)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# üß† Step 8 ‚Äî Feature Importance (Logistic Regression specific)\n",
        "# ============================================================\n",
        "# For Logistic Regression, feature importance is represented by the coefficients.\n",
        "feature_names = [\"mean_delta_rr\", \"variability_delta_rr\", \"ks_pvalue_delta_rr\"]\n",
        "coefficients = log_reg_clf.coef_[0]\n",
        "\n",
        "# Prepare data for plotting (using absolute value for comparison magnitude)\n",
        "importance_df = pd.DataFrame(\n",
        "    {'feature': feature_names, 'coefficient': coefficients, 'magnitude': np.abs(coefficients)}\n",
        ").sort_values(by='magnitude', ascending=True)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.barh(importance_df['feature'], importance_df['coefficient'],\n",
        "         color=[('darkred' if c < 0 else 'darkblue') for c in importance_df['coefficient']])\n",
        "plt.xlabel(\"Coefficient Value (Feature Weight)\")\n",
        "plt.title(\"Logistic Regression Feature Weights (Signed)\")\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# ü©∫ Step 9 ‚Äî ROC Curve\n",
        "# ============================================================\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc_score(y_test, y_proba_test):.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve (Test Set)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEGcWxrxjUzC"
      },
      "source": [
        "## üîç Visualizing Errors - 2D Feature Projections\n",
        "\n",
        "To better understand our model's mistakes, we'll perform **error analysis**.  \n",
        "Since our feature space is small (just 3 features), we can **easily visualize it in 2D**.\n",
        "\n",
        "We define a reusable function that plots the **misclassified samples** across all 3 pairs of features.  \n",
        "This helps us identify patterns in errors and see how well-separated the classes are under different projections.\n",
        "\n",
        "We'll use this for different classifiers later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK4HjwDAi0tP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_misclassified_2D(X_test_scaled, y_test, y_pred, title_prefix):\n",
        "    \"\"\"\n",
        "    Plots misclassified points in 2D feature spaces (all feature pairs).\n",
        "\n",
        "    Args:\n",
        "        X_test_scaled: np.ndarray, shape (n_samples, 3)\n",
        "        y_test: true labels (binary)\n",
        "        y_pred: predicted labels (binary)\n",
        "        title_prefix: str to prefix each plot title (e.g., \"LogReg\", \"Tree\")\n",
        "    \"\"\"\n",
        "    feature_names = [\"Variability\", \"Normality\", \"Mean\"]\n",
        "    pairs = [(0, 1), (0, 2), (1, 2)]\n",
        "    misclassified = y_test != y_pred\n",
        "\n",
        "    for i, j in pairs:\n",
        "        plt.figure(figsize=(6, 5))\n",
        "\n",
        "        # Correct predictions\n",
        "        plt.scatter(X_test_scaled[(y_test == 0) & ~misclassified, i],\n",
        "                    X_test_scaled[(y_test == 0) & ~misclassified, j],\n",
        "                    c='green', s=5, label='non-AF (correct)')\n",
        "\n",
        "        plt.scatter(X_test_scaled[(y_test == 1) & ~misclassified, i],\n",
        "                    X_test_scaled[(y_test == 1) & ~misclassified, j],\n",
        "                    c='red', s=5, label='AF (correct)')\n",
        "\n",
        "        # Misclassified points\n",
        "        plt.scatter(X_test_scaled[misclassified, i],\n",
        "                    X_test_scaled[misclassified, j],\n",
        "                    c='black', s=10, label='Misclassified', marker='x')\n",
        "\n",
        "        plt.xlabel(feature_names[i])\n",
        "        plt.ylabel(feature_names[j])\n",
        "        plt.title(f\"{title_prefix}: {feature_names[i]} vs {feature_names[j]}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "plot_misclassified_2D(X_test_scaled, y_test, y_pred_test, \"Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tAbr0Dzab9X"
      },
      "source": [
        "## üß©  TASK 5 - Interpretation of error analysis (0.5 + 0.5 + 1 points)\n",
        "\n",
        "Inspect the three plots showing misclassified ECG windows for the Logistic Regression model:\n",
        "Variability vs Normality\n",
        "Variability vs Mean\n",
        "Normality vs Mean\n",
        "\n",
        "Answer the following:\n",
        "\n",
        "\n",
        "1.   In which regions of the feature space do most misclassifications occur? Describe their location relative to the two classes.\n",
        "2.   Which feature appears to dominate the decision of the logistic regression model?\n",
        "Which feature seems less influential?\n",
        "\n",
        "3.   Why is a linear classifier likely to struggle with these data, based on the plots?\n",
        "Give one plausible signal-processing or physiological explanation.\n",
        "\n",
        "\n",
        "\n",
        "## üß©  TASK 5 - YOUR ANSWERS\n",
        "\n",
        "Short answers (2‚Äì4 sentences per question) are sufficient.\n",
        "\n",
        "1.   Your Answer\n",
        "2.   Your Answer\n",
        "3.   Your Answer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBkNL9StlKgM"
      },
      "source": [
        "## üå≥ Let's move on to a non-linear model\n",
        "\n",
        "Next we'll train and evaluate the gradient boosting model XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MiWxHaLeZ01"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚ö° XGBoost for AF Detection ‚Äî Full Training Evaluation\n",
        "# Here, you will find the BONUS Task:\n",
        "# üß© BONUS Task ‚Äî Calculate imbalance ratio from the training data (1 point)\n",
        "# ============================================================\n",
        "import xgboost as xgb\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, precision_recall_curve,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, auc, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# üß© BONUS Task ‚Äî Calculate imbalance ratio from the training data\n",
        "# Hint: ratio = (# of non-AF samples) / (# of AF samples)\n",
        "\n",
        "SCALE_POS_WEIGHT_RATIO = None\n",
        "print(f\"scale_pos_weight = {SCALE_POS_WEIGHT_RATIO}\")\n",
        "\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Step 1 ‚Äî Hyperparameter grid (Tuned for XGBoost)\n",
        "# ============================================================\n",
        "# Define the search space using skopt space objects for common XGBoost parameters\n",
        "param_dist = {\n",
        "    # Increase the range for boosting rounds (trees) to allow for lower learning rates\n",
        "    'n_estimators': Integer(200, 1000),\n",
        "\n",
        "    # Tighten max_depth: boosting trees are usually shallow for better generalization\n",
        "    'max_depth': Integer(3, 8),\n",
        "\n",
        "    # CRITICAL: Widen learning rate range for better exploration\n",
        "    'learning_rate': Real(0.005, 0.5, prior='log-uniform'),\n",
        "\n",
        "    # regularization terms (kept wide to allow BayesSearchCV to find optimal point)\n",
        "    'reg_alpha': Real(1e-5, 100, prior='log-uniform'),\n",
        "    'reg_lambda': Real(1e-5, 100, prior='log-uniform'),\n",
        "\n",
        "    # Minimum loss reduction required to make a further partition on a leaf node\n",
        "    'gamma': Real(1e-5, 1.0, prior='log-uniform'),\n",
        "\n",
        "    # Minimum sum of instance weight (Hessian) needed in a child\n",
        "    'min_child_weight': Integer(1, 15), # Increased upper bound slightly\n",
        "\n",
        "    # Subsample ratio of columns when constructing each tree\n",
        "    'colsample_bytree': Real(0.5, 1.0, prior='uniform')\n",
        "}\n",
        "\n",
        "# Using XGBClassifier (objective='binary:logistic' for probability)\n",
        "xgb_base = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    scale_pos_weight=SCALE_POS_WEIGHT_RATIO, # Handle class imbalance\n",
        "    eval_metric='logloss',                   # Metric for evaluation\n",
        "    use_label_encoder=False,                 # Required for newer XGBoost versions\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0                              # Suppress internal messages\n",
        ")\n",
        "\n",
        "# Using BayesSearchCV with XGBoost\n",
        "grid = BayesSearchCV(\n",
        "    xgb_base,\n",
        "    search_spaces=param_dist,\n",
        "    n_iter=3,                      # INCREASED ITERATIONS to 50 for more thorough search\n",
        "    cv=3,                           # 3-fold CV\n",
        "    scoring='roc_auc',                   # CRITICAL CHANGE: Optimize directly for F1 Score\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True,\n",
        ")\n",
        "\n",
        "print(\"\\n‚è≥ Running Bayesian Search for XGBoost...\")\n",
        "# Note: X_train_scaled and y_train are assumed to be defined globally\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ Bayesian Search complete.\")\n",
        "\n",
        "# ============================================================\n",
        "# üèÜ Step 2 ‚Äî Train with best parameters\n",
        "# ============================================================\n",
        "best_params = grid.best_params_\n",
        "print(f\"\\nüèÜ Best parameters from cross-validation:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k:<20}: {v}\")\n",
        "\n",
        "# Train the final classifier\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    scale_pos_weight=SCALE_POS_WEIGHT_RATIO,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0,\n",
        "    **best_params\n",
        ")\n",
        "xgb_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ============================================================\n",
        "# üîç Step 3 ‚Äî Predictions & probabilities\n",
        "# ============================================================\n",
        "y_pred_train  = xgb_clf.predict(X_train_scaled)\n",
        "# XGBoost uses predict_proba to get probabilities\n",
        "y_proba_train = xgb_clf.predict_proba(X_train_scaled)[:, 1]\n",
        "\n",
        "y_pred_test  = xgb_clf.predict(X_test_scaled)\n",
        "y_proba_test = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# ============================================================\n",
        "# üìä Step 4 ‚Äî Evaluation helper (no change needed)\n",
        "# ============================================================\n",
        "def evaluate_model(y_true, y_pred, y_proba, label=\"Set\"):\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec  = recall_score(y_true, y_pred)\n",
        "    f1   = f1_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_proba)\n",
        "    print(f\"\\nüìä Evaluation on {label}:\")\n",
        "    print(f\"  Accuracy : {acc:.4f}\")\n",
        "    print(f\"  Precision: {prec:.4f}\")\n",
        "    print(f\"  Recall   : {rec:.4f}\")\n",
        "    print(f\"  F1 Score : {f1:.4f}\")\n",
        "    print(f\"  ROC AUC  : {auc_roc:.4f}\")\n",
        "    return acc, prec, rec, f1, auc_roc\n",
        "\n",
        "# --- Evaluate both training & test performance ---\n",
        "evaluate_model(y_train, y_pred_train, y_proba_train, \"Training Set (Threshold=0.5)\")\n",
        "evaluate_model(y_test,  y_pred_test,  y_proba_test,  \"Test Set (Threshold=0.5)\")\n",
        "\n",
        "# ============================================================\n",
        "# ü©∏ Step 5 ‚Äî Precision‚ÄìRecall curve & threshold tuning (no change needed)\n",
        "# ============================================================\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_test)\n",
        "pr_auc = auc(recalls, precisions)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(recalls, precisions, label=f'PR Curve (AUC={pr_auc:.3f})')\n",
        "plt.xlabel(\"Recall (Sensitivity)\")\n",
        "plt.ylabel(\"Precision (PPV)\")\n",
        "plt.title(\"Precision‚ÄìRecall Curve (Test Set)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Find optimal threshold for maximum F1 ---\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"\\n‚öôÔ∏è Optimal probability threshold (max F1): {best_thresh:.3f}\")\n",
        "\n",
        "# --- Apply optimized threshold ---\n",
        "y_pred_opt = (y_proba_test >= best_thresh).astype(int)\n",
        "evaluate_model(y_test, y_pred_opt, y_proba_test, \"Test Set (Optimized Threshold)\")\n",
        "\n",
        "# ============================================================\n",
        "# üìà Step 6 ‚Äî Precision, Recall, and F1 vs Threshold\n",
        "# ============================================================\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(thresholds, precisions[:-1], label=\"Precision\", color=\"green\")\n",
        "plt.plot(thresholds, recalls[:-1], label=\"Recall\", color=\"red\")\n",
        "plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\", color=\"blue\")\n",
        "plt.axvline(best_thresh, color='black', linestyle='--', label=f\"Best F1={best_thresh:.2f}\")\n",
        "plt.xlabel(\"Probability Threshold\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Precision‚ÄìRecall‚ÄìF1 vs Threshold\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# üß© Step 7 ‚Äî Confusion Matrix (Optimized Threshold)\n",
        "# ============================================================\n",
        "cm = confusion_matrix(y_test, y_pred_opt)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non-AF', 'AF'])\n",
        "disp.plot(cmap='Greens', values_format='d')\n",
        "plt.title(\"Confusion Matrix (XGBoost, Optimized Threshold)\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# üß† Step 8 ‚Äî Feature Importance (XGBoost specific)\n",
        "# ============================================================\n",
        "# ‚ö†Ô∏è CHANGE 4: Adapted to use XGBoost's specific feature importance score\n",
        "importances = xgb_clf.get_booster().get_score(importance_type='weight')\n",
        "feature_names = [\"mean_delta_rr\", \"variability_delta_rr\", \"ks_pvalue_delta_rr\"]\n",
        "\n",
        "# Prepare data for plotting\n",
        "importance_df = pd.DataFrame(\n",
        "    {'feature': feature_names, 'importance': [importances.get(f'f{i}', 0) for i in range(len(feature_names))]}\n",
        ").sort_values(by='importance', ascending=True)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.barh(importance_df['feature'], importance_df['importance'], color='forestgreen')\n",
        "plt.xlabel(\"Feature Importance (Weight)\")\n",
        "plt.title(\"XGBoost Feature Importances\")\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# ü©∫ Step 9 ‚Äî ROC Curve\n",
        "# ============================================================\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc_score(y_test, y_proba_test):.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve (Test Set)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqCHcSuwjuQN"
      },
      "outputs": [],
      "source": [
        "plot_misclassified_2D(X_test_scaled, y_test, y_pred_test, \"XGBoost\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_CwZpykd_e"
      },
      "source": [
        "## üèÅ Conclusion - Part 1\n",
        "\n",
        "In this notebook, we demonstrated a full pipeline for atrial fibrillation (AF) detection from ECG signals using interpretable features. Specifically:\n",
        "\n",
        "- We extracted three features from RR interval differences ‚Äì variability, normality, and mean ‚Äì to capture the irregular heartbeat patterns characteristic of AF.\n",
        "- We trained and evaluated two classifiers:\n",
        "  - Logistic Regression ‚Äì applied with regularization, and tuned using grid search with cross-validation to select the best parameters.\n",
        "  - XGBoost ‚Äì which outperformed logistic regression by capturing non-linear relations in the feature space.\n",
        "- We emphasized the importance of feature scaling and avoiding data leakage by fitting the scaler only on the training set.\n",
        "- We also included error visualization in 2D feature spaces to better understand where and why models make mistakes.\n",
        "\n",
        "This shows that even simple, well-crafted features can be powerful for AF detection from ECG data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9qiDJmTfxNT"
      },
      "source": [
        "# Part 2 - Forecasting ECG Recordings  \n",
        "\n",
        "---\n",
        "\n",
        "**Repetition: What is an ECG?**  \n",
        "An electrocardiogram is a voltage trace captured by electrodes on the skin. Each heartbeat follows a characteristic sequence:\n",
        "\n",
        "* **P-wave** ‚Äì depolarisation of the atria, leading to atrial contraction.  \n",
        "* **QRS complex** ‚Äì rapid depolarisation and contraction of the ventricles.  \n",
        "* **T-wave** ‚Äì ventricular repolarisation as the heart resets and the ventricles relax for the next beat.\n",
        "\n",
        "**Time series forecasting** in the context of ECG involves predicting future segments of the signal based on its recent history. This can help estimate how the heart rhythm will evolve in the next few milliseconds or beats. Such forecasting is valuable for early detection of arrhythmias, anticipating signal anomalies, or supporting real-time monitoring systems that can alert clinicians to irregular patterns before they fully manifest. Because ECGs are highly periodic yet sensitive to small physiological changes, forecasting models must learn both short-term dependencies within individual beats and long-term variations across heart cycles.\n",
        "\n",
        "In our experimental setup, we will **train** our models on ECG recordings from **patient 42**, and **validate** our forecasts using recordings from **patient 11**.\n",
        "\n",
        "This is a very **simplified setup** that will yield somewhat subpar results, acknowledging that we don't want to spend too much time waiting for models to train as well as the limited resources available in our Colab notebooks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FZY2WqYgE38"
      },
      "outputs": [],
      "source": [
        "# --- Download an exemplary short ECG records from PhysioNet ---\n",
        "!mkdir -p data\n",
        "import wfdb\n",
        "\n",
        "DB_NAME  = \"ltafdb\"\n",
        "RECORDS  = [\"42\", \"11\"]   # 42 = train, 11 = test\n",
        "\n",
        "for rec in RECORDS:\n",
        "    rec_path = f\"data/{rec}.hea\"\n",
        "    if os.path.exists(rec_path):\n",
        "        print(f\"‚úÖ Record {rec} already exists in data/, skipping download.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Downloading record {rec} from {DB_NAME} ...\")\n",
        "    wfdb.dl_database(\n",
        "        DB_NAME,\n",
        "        records=[rec],\n",
        "        dl_dir=\"data\",\n",
        "        keep_subdirs=False,   # puts 42.dat / 42.hea directly into data/\n",
        "    )\n",
        "\n",
        "print(\"\\n‚úÖ Download complete. Files in /data:\")\n",
        "!ls -lh data | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feoh7l2ngArF"
      },
      "outputs": [],
      "source": [
        "# This time, we will work with the cleaned ECG data.\n",
        "# Let's start by visualizing an exemplary segment.\n",
        "\n",
        "# ============================================================\n",
        "# üìà Visualize an exemplary RAW ECG segment (16s context + 4s future)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import wfdb\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config for this demo plot ---\n",
        "CONTEXT_SECONDS   = 16.0    # context length\n",
        "FORECAST_SECONDS  = 4.0     # future length\n",
        "LEAD_INDEX        = 0\n",
        "LOWCUT, HIGHCUT   = 0.5, 40.0\n",
        "FILTER_ORDER      = 3\n",
        "\n",
        "def butter_bandpass(sig, fs, low=LOWCUT, high=HIGHCUT, order=FILTER_ORDER):\n",
        "    nyq = 0.5 * fs\n",
        "    low_norm = max(0.001, low / nyq)\n",
        "    high_norm = min(0.999, high / nyq)\n",
        "    b, a = signal.butter(order, [low_norm, high_norm], btype='band')\n",
        "    sig_f = signal.filtfilt(b, a, sig)\n",
        "    return sig_f\n",
        "\n",
        "def load_filtered_ecg(record_path, lead_index=LEAD_INDEX, use_filter=True):\n",
        "    \"\"\"Load a record and return (ecg, fs).\"\"\"\n",
        "    rec = wfdb.rdrecord(record_path)\n",
        "    fs = rec.fs\n",
        "    sig = rec.p_signal[:, lead_index].astype(float)\n",
        "    if use_filter:\n",
        "        sig = butter_bandpass(sig, fs)\n",
        "    return sig, fs\n",
        "\n",
        "# --- Load an exemplary ECG record (e.g., data/42 from ltafdb) ---\n",
        "#ecg, fs = load_filtered_ecg(\"data/42\", use_filter=True)  # set False if you want truly raw\n",
        "ecg, fs = load_filtered_ecg(\"data/11\", use_filter=True)\n",
        "\n",
        "context_samples  = int(CONTEXT_SECONDS * fs)\n",
        "forecast_samples = int(FORECAST_SECONDS * fs)\n",
        "window_samples   = context_samples + forecast_samples\n",
        "\n",
        "if len(ecg) < window_samples:\n",
        "    raise RuntimeError(\n",
        "        f\"ECG too short ({len(ecg)} samples) for a {window_samples}-sample window.\"\n",
        "    )\n",
        "\n",
        "# Take a segment from the middle of the recording\n",
        "start = len(ecg) // 160 - window_samples // 160\n",
        "start = max(start, 0)\n",
        "segment = ecg[start:start + window_samples]\n",
        "\n",
        "context = segment[:context_samples]\n",
        "future  = segment[context_samples:]\n",
        "\n",
        "# --- Plot: 20s context + 4s future (no forecast) ---\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(context_samples), context, color=\"gray\", label=\"Context (first 16 s)\")\n",
        "plt.plot(\n",
        "    range(context_samples, context_samples + forecast_samples),\n",
        "    future,\n",
        "    color=\"green\",\n",
        "    label=\"Future (next 4 s)\",\n",
        ")\n",
        "plt.axvline(context_samples, color=\"black\", linestyle=\"--\")\n",
        "plt.xlabel(\"Sample index (within 20 s window)\")\n",
        "plt.ylabel(\"ECG amplitude (filtered units)\")\n",
        "plt.title(\"Raw ECG Segment ‚Äî 16 s Context + 4 s Future (Record 11)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.5)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA2O9l9SgT1i"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The plot shows a 20-second segment of raw ECG from **record 11** after basic band-pass filtering.  \n",
        "The signal is split into:\n",
        "\n",
        "- **Context (first 16 s, grey)** ‚Äì this is the input that forecasting models will see.  \n",
        "  It contains a sequence of nearly periodic heart beats with fairly regular morphology and\n",
        "  amplitude, typical for long stretches of atrial fibrillation in this record.\n",
        "\n",
        "- **Future (next 4 s, green)** ‚Äì this is the target segment we want the model to predict.\n",
        "  It continues the same rhythm and morphology as the context, but is hidden from the\n",
        "  forecaster during inference.\n",
        "\n",
        "The vertical dashed line marks the **boundary between context and forecast horizon**.  \n",
        "All subsequent models are trained or evaluated\n",
        "on windows with exactly this structure: the first 16 seconds as input and the following\n",
        "4 seconds as the prediction target.```\n",
        "::contentReference[oaicite:0]{index=0}\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53zjMhqVgXKv"
      },
      "source": [
        "## üß∞ Let's start forecasting using PatchTST\n",
        "\n",
        "As explained in the lecture, **PatchTST (Patch Time Series Transformer)** is a Transformer-based architecture specifically designed for long-term forecasting of (multivariate) time series. It has demonstrated state-of-the-art performance by introducing two key innovations to adapt the powerful Transformer model‚Äîoriginally popularized in Natural Language Processing‚Äîfor time series data:\n",
        "\n",
        "**Patching (Segmentation of Time Series)**\n",
        "\n",
        "Instead of treating individual time steps as input tokens (like words in a sentence, which is known as point-wise attention), PatchTST segments each time series into subseries-level \"patches\" (short, fixed-length segments).\n",
        "\n",
        "\n",
        "These patches become the input tokens for the Transformer encoder. This is highly effective because:\n",
        "\n",
        "\n",
        "*   It retains local semantic information (patterns within the short patch).\n",
        "\n",
        "*   It significantly reduces computational complexity and memory usage, allowing the model to process much longer historical sequences.\n",
        "\n",
        "**Channel-Independence**\n",
        "\n",
        "For multivariate time series (data with multiple features or \"channels,\" like temperature, wind speed, and humidity), PatchTST processes each individual time series (channel) independently using the same shared Transformer backbone and weights.\n",
        "\n",
        "This approach simplifies the modeling of complex multi-channel data, improves training efficiency, and enhances the model's ability to generalize across different channels. The final predictions are then generated for each channel and combined.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Useful links\n",
        "\n",
        "\n",
        "\n",
        "*   Github: https://github.com/yuqinie98/PatchTST\n",
        "*   Paper: Nie et al. 2023, arXiv:2211.14730\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84irmF2LhF9j"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚ö° Raw ECG Forecasting with PatchTST\n",
        "# Train on data/42, Test on data/11\n",
        "# NO Task here - try to understand the code and execute it.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import wfdb\n",
        "from scipy import signal\n",
        "from wfdb import processing as wfproc\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# ---------------------- Global Seeds -------------------------\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# --- Configuration for RAW ECG (in samples) ---\n",
        "WINDOW_SECONDS   = 20.0     # total window length (e.g. 20s)\n",
        "CONTEXT_RATIO    = 0.8      # 80% context, 20% forecast\n",
        "WINDOW_STEP_SEC  = WINDOW_SECONDS  # non-overlapping windows\n",
        "\n",
        "LEAD_INDEX = 0\n",
        "LOWCUT, HIGHCUT = 0.5, 40.0\n",
        "FILTER_ORDER = 3\n",
        "\n",
        "MAX_WINDOWS_PER_RECORD = 2000   # safety cap per record\n",
        "MAX_TRAIN_WINDOWS      = 600    # from record 42\n",
        "MAX_TEST_WINDOWS       = 200    # from record 11\n",
        "\n",
        "# PatchTST hyperparams (in *sample* domain now)\n",
        "PATCH_LEN    = 128\n",
        "PATCH_STRIDE = 16\n",
        "D_MODEL      = 128\n",
        "N_HEADS      = 4\n",
        "N_LAYERS     = 4\n",
        "D_FF         = 256\n",
        "DROPOUT      = 0.3\n",
        "\n",
        "# ============================================================\n",
        "# Preprocessing helpers (for raw ECG)\n",
        "# ============================================================\n",
        "def butter_bandpass(sig, fs, low=LOWCUT, high=HIGHCUT, order=FILTER_ORDER):\n",
        "    nyq = 0.5 * fs\n",
        "    low_norm = max(0.001, low / nyq)\n",
        "    high_norm = min(0.999, high / nyq)\n",
        "    b, a = signal.butter(order, [low_norm, high_norm], btype='band')\n",
        "    sig_f = signal.filtfilt(b, a, sig)\n",
        "    return sig_f\n",
        "\n",
        "def process_record_ecg_windows(record_path,\n",
        "                               window_seconds=WINDOW_SECONDS,\n",
        "                               step_seconds=WINDOW_STEP_SEC,\n",
        "                               max_windows=MAX_WINDOWS_PER_RECORD):\n",
        "    \"\"\"Return raw ECG windows (N, window_samples) for a single record.\"\"\"\n",
        "    try:\n",
        "        rec = wfdb.rdrecord(record_path)\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(f\"Cannot find record at {record_path}. Did you download it?\")\n",
        "\n",
        "    fs = rec.fs\n",
        "    sig = rec.p_signal[:, LEAD_INDEX].astype(float)\n",
        "    sig_f = butter_bandpass(sig, fs)\n",
        "\n",
        "    window_samples = int(window_seconds * fs)\n",
        "    step_samples   = int(step_seconds * fs)\n",
        "\n",
        "    if len(sig_f) < window_samples:\n",
        "        raise RuntimeError(\n",
        "            f\"Too few samples ({len(sig_f)}) for {window_samples}-sample windows in {record_path}\"\n",
        "        )\n",
        "\n",
        "    windows = [\n",
        "        sig_f[i:i + window_samples]\n",
        "        for i in range(0, len(sig_f) - window_samples + 1, step_samples)\n",
        "    ]\n",
        "    windows = np.array(windows)\n",
        "\n",
        "    if len(windows) > max_windows:\n",
        "        windows = windows[:max_windows]\n",
        "\n",
        "    print(f\"‚úÖ {record_path}: {len(windows)} windows of {window_samples} samples each (fs={fs} Hz)\")\n",
        "    return windows, fs, window_samples\n",
        "\n",
        "def split_context_forecast_samples(windows, context_ratio=CONTEXT_RATIO):\n",
        "    \"\"\"Split sample windows into context and forecast parts.\"\"\"\n",
        "    window_samples = windows.shape[1]\n",
        "    context_samples = int(window_samples * context_ratio)\n",
        "    forecast_samples = window_samples - context_samples\n",
        "\n",
        "    X = windows[:, :context_samples]\n",
        "    Y = windows[:, context_samples:]\n",
        "    print(f\"Context samples: {context_samples}, Forecast samples: {forecast_samples}\")\n",
        "    print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "    return X, Y, context_samples, forecast_samples\n",
        "\n",
        "# ============================================================\n",
        "# Build TRAIN from data/42, TEST from data/11 (RAW ECG)\n",
        "# ============================================================\n",
        "try:\n",
        "    # --- TRAIN: record 42 ---\n",
        "    win_42, fs_42, window_samples_42 = process_record_ecg_windows(\n",
        "        \"data/42\",\n",
        "        window_seconds=WINDOW_SECONDS,\n",
        "        step_seconds=WINDOW_STEP_SEC,\n",
        "        max_windows=MAX_TRAIN_WINDOWS,\n",
        "    )\n",
        "    X_train_all, Y_train_all, CONTEXT_SAMPLES, FORECAST_SAMPLES = split_context_forecast_samples(\n",
        "        win_42, context_ratio=CONTEXT_RATIO\n",
        "    )\n",
        "\n",
        "    # --- TEST: record 11 ---\n",
        "    win_11, fs_11, window_samples_11 = process_record_ecg_windows(\n",
        "        \"data/11\",\n",
        "        window_seconds=WINDOW_SECONDS,\n",
        "        step_seconds=WINDOW_STEP_SEC,\n",
        "        max_windows=MAX_TEST_WINDOWS,\n",
        "    )\n",
        "    X_test, Y_test, CONTEXT_SAMPLES_test, FORECAST_SAMPLES_test = split_context_forecast_samples(\n",
        "        win_11, context_ratio=CONTEXT_RATIO\n",
        "    )\n",
        "\n",
        "    if fs_42 != fs_11:\n",
        "        print(f\"‚ö†Ô∏è Warning: fs_42={fs_42} Hz, fs_11={fs_11} Hz (not identical).\")\n",
        "    if (CONTEXT_SAMPLES != CONTEXT_SAMPLES_test) or (FORECAST_SAMPLES != FORECAST_SAMPLES_test):\n",
        "        raise RuntimeError(\"Context/forecast sample lengths differ between records 42 and 11.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error while building train/test from records 42 and 11 (raw ECG): {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(f\"\\nFinal RAW ECG shapes:\")\n",
        "print(f\"  TRAIN (42): X={X_train_all.shape}, Y={Y_train_all.shape}\")\n",
        "print(f\"  TEST  (11): X={X_test.shape},     Y={Y_test.shape}\")\n",
        "\n",
        "# ============================================================\n",
        "# PatchTST model + training utilities (for raw ECG)\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "class ECGBaselineDataset(Dataset):\n",
        "    def __init__(self, X_arr, y_arr):\n",
        "        self.X = torch.tensor(X_arr, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_arr, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class PatchTST(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim=1,\n",
        "                 context_len=None,\n",
        "                 forecast_len=None,\n",
        "                 patch_len=PATCH_LEN,\n",
        "                 stride=PATCH_STRIDE,\n",
        "                 d_model=D_MODEL,\n",
        "                 n_heads=N_HEADS,\n",
        "                 num_layers=N_LAYERS,\n",
        "                 d_ff=D_FF,\n",
        "                 dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "\n",
        "        if context_len is None or forecast_len is None:\n",
        "            raise ValueError(\"Please pass context_len and forecast_len when creating PatchTST.\")\n",
        "\n",
        "        self.context_len = context_len\n",
        "        self.forecast_len = forecast_len\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # number of patches along the time dimension\n",
        "        self.n_patches = 1 + (context_len - patch_len) // stride\n",
        "        if self.n_patches <= 0:\n",
        "            raise ValueError(\"context_len must be >= patch_len\")\n",
        "\n",
        "        # Each patch is (patch_len, input_dim) flattened ‚Üí patch_len * input_dim\n",
        "        patch_dim = patch_len * input_dim\n",
        "        self.patch_proj = nn.Linear(patch_dim, d_model)\n",
        "\n",
        "        # Positional embedding over patches\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, self.n_patches, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model * self.n_patches)\n",
        "        self.head = nn.Linear(d_model * self.n_patches, forecast_len)\n",
        "\n",
        "    def _patchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, input_dim)\n",
        "        returns: (batch, n_patches, patch_len * input_dim)\n",
        "        \"\"\"\n",
        "        # Unfold along the time dimension (dim=1) into overlapping patches\n",
        "        # patches: (batch, n_patches, patch_len, input_dim)\n",
        "        patches = x.unfold(dimension=1, size=self.patch_len, step=self.stride)\n",
        "\n",
        "        # In case rounding makes more patches than expected, trim to self.n_patches\n",
        "        patches = patches[:, :self.n_patches, :, :]\n",
        "\n",
        "        b, n_p, p_l, c = patches.shape  # n_p should == self.n_patches, p_l == patch_len, c == input_dim\n",
        "\n",
        "        # Flatten (patch_len, input_dim) into a single feature dimension\n",
        "        patches = patches.contiguous().view(b, n_p, p_l * c)  # (batch, n_patches, patch_len * input_dim)\n",
        "        return patches\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        src: (batch, context_len) for univariate\n",
        "             or (batch, context_len, input_dim) if you later go multivariate.\n",
        "        \"\"\"\n",
        "        if src.dim() == 2:\n",
        "            # univariate: (B, L) ‚Üí (B, L, 1)\n",
        "            x = src.unsqueeze(-1)\n",
        "        else:\n",
        "            # already (B, L, C)\n",
        "            x = src\n",
        "\n",
        "        # (batch, n_patches, patch_len * input_dim)\n",
        "        patches = self._patchify(x)\n",
        "\n",
        "        # Project patches to d_model\n",
        "        # (batch, n_patches, d_model)\n",
        "        tokens = self.patch_proj(patches)\n",
        "\n",
        "        # Add patch-wise positional embedding\n",
        "        tokens = tokens + self.pos_emb\n",
        "\n",
        "        # Transformer encoder over patch tokens\n",
        "        # (batch, n_patches, d_model)\n",
        "        enc_out = self.encoder(tokens)\n",
        "\n",
        "        # Flatten all patch embeddings\n",
        "        flat = enc_out.reshape(enc_out.size(0), -1)   # (batch, n_patches * d_model)\n",
        "        flat = self.norm(flat)\n",
        "\n",
        "        # Final head to forecast the full horizon\n",
        "        y = self.head(flat)                           # (batch, forecast_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "LAMBDA_SMOOTH = 3  # a bit smaller for ECG than RR, tweak as needed\n",
        "\n",
        "def loss_fn(pred, target, lambda_smooth=LAMBDA_SMOOTH):\n",
        "    # base loss\n",
        "    base = mae(pred, target) + 0.5 * mse(pred, target)\n",
        "    # smoothness penalty on forecast differences\n",
        "    diff = pred[:, 1:] - pred[:, :-1]\n",
        "    smooth = torch.mean(diff ** 2)\n",
        "    return base + lambda_smooth * smooth\n",
        "\n",
        "def smape(y_true, y_pred, eps=1e-8):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    denom = np.abs(y_true) + np.abs(y_pred) + eps\n",
        "    return 100.0 * np.mean(2.0 * np.abs(y_true - y_pred) / denom)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Train / Val split on record 42 ONLY (no leakage from 11)\n",
        "# ============================================================\n",
        "n_train_all = len(X_train_all)\n",
        "n_train_core = int(0.9 * n_train_all)   # 90% for train, 10% for val\n",
        "\n",
        "X_train = X_train_all[:n_train_core]\n",
        "Y_train = Y_train_all[:n_train_core]\n",
        "X_val   = X_train_all[n_train_core:]\n",
        "Y_val   = Y_train_all[n_train_core:]\n",
        "\n",
        "print(f\"\\nFrom record 42 (RAW ECG):\")\n",
        "print(f\"  Train core windows: {len(X_train)}\")\n",
        "print(f\"  Val windows       : {len(X_val)}\")\n",
        "\n",
        "print(f\"From record 11 (RAW ECG):\")\n",
        "print(f\"  Test windows      : {len(X_test)}\")\n",
        "\n",
        "# --- Fit scaler ONLY on training context from record 42 ---\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train.reshape(-1, 1))\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
        "X_val_scaled   = scaler.transform(X_val.reshape(-1, 1)).reshape(X_val.shape)\n",
        "X_test_scaled  = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
        "\n",
        "Y_train_scaled = scaler.transform(Y_train.reshape(-1, 1)).reshape(Y_train.shape)\n",
        "Y_val_scaled   = scaler.transform(Y_val.reshape(-1, 1)).reshape(Y_val.shape)\n",
        "Y_test_scaled  = scaler.transform(Y_test.reshape(-1, 1)).reshape(Y_test.shape)\n",
        "\n",
        "train_dataset = ECGBaselineDataset(X_train_scaled, Y_train_scaled)\n",
        "val_dataset   = ECGBaselineDataset(X_val_scaled,   Y_val_scaled)\n",
        "test_dataset  = ECGBaselineDataset(X_test_scaled,  Y_test_scaled)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS     = 10\n",
        "PATIENCE   = 5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ============================================================\n",
        "# Train PatchTST with early stopping on validation (record 42)\n",
        "# ============================================================\n",
        "model = PatchTST(\n",
        "    context_len=CONTEXT_SAMPLES,\n",
        "    forecast_len=FORECAST_SAMPLES,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "best_val = float('inf')\n",
        "best_state = None\n",
        "epochs_no_improve = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses   = []\n",
        "\n",
        "print(\"\\nStarting PatchTST training on RAW ECG from record 42...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "    avg_train = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            total_val_loss += loss_fn(y_pred, y_batch).item()\n",
        "    avg_val = total_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "        f\"Train: {avg_train:.4f} | Val: {avg_val:.4f} | \"\n",
        "        f\"NoImprove: {epochs_no_improve}/{PATIENCE}\"\n",
        "    )\n",
        "\n",
        "    if avg_val < best_val:\n",
        "        best_val = avg_val\n",
        "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(\"‚èπ Early stopping.\")\n",
        "        break\n",
        "\n",
        "# Restore best model\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "    model.to(device)\n",
        "    print(f\"‚úÖ Restored best model (Val Loss={best_val:.4f})\")\n",
        "\n",
        "# ============================================================\n",
        "# Training curves\n",
        "# ============================================================\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"PatchTST Training Progress (RAW ECG, record 42)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluate on TEST set (record 11, RAW ECG)\n",
        "# ============================================================\n",
        "model.eval()\n",
        "y_true_test, y_pred_test = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_pred = model(X_batch).cpu().numpy()\n",
        "        y_pred_test.append(y_pred)\n",
        "        y_true_test.append(y_batch.numpy())\n",
        "\n",
        "y_true_test = np.concatenate(y_true_test, axis=0)\n",
        "y_pred_test = np.concatenate(y_pred_test, axis=0)\n",
        "\n",
        "# Scaled metrics\n",
        "mse_scaled   = mean_squared_error(y_true_test.flatten(), y_pred_test.flatten())\n",
        "mae_scaled   = mean_absolute_error(y_true_test.flatten(), y_pred_test.flatten())\n",
        "smape_scaled = smape(y_true_test.flatten(), y_pred_test.flatten())\n",
        "\n",
        "# Original ECG units (same units as p_signal, usually mV)\n",
        "true_inv = scaler.inverse_transform(\n",
        "    y_true_test.flatten().reshape(-1, 1)\n",
        ").flatten()\n",
        "pred_inv = scaler.inverse_transform(\n",
        "    y_pred_test.flatten().reshape(-1, 1)\n",
        ").flatten()\n",
        "\n",
        "mse_inv   = mean_squared_error(true_inv, pred_inv)\n",
        "mae_inv   = mean_absolute_error(true_inv, pred_inv)\n",
        "smape_inv = smape(true_inv, pred_inv)\n",
        "\n",
        "print(\"\\n===== PatchTST Test Performance (RAW ECG, record 11) =====\")\n",
        "print(f\"Scaled   ‚Üí MSE: {mse_scaled:.6f}, MAE: {mae_scaled:.6f}, sMAPE: {smape_scaled:.2f}%\")\n",
        "print(f\"Original ‚Üí MSE: {mse_inv:.6f}, MAE: {mae_inv:.6f}, sMAPE: {smape_inv:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# Visualize Example Prediction from TEST set (record 11, RAW ECG)\n",
        "# ============================================================\n",
        "plot_idx = 3\n",
        "n_test = len(X_test_scaled)\n",
        "plot_idx = plot_idx % n_test\n",
        "print(f\"\\nPlotting test sample index: {plot_idx} of {n_test} (record 11)\")\n",
        "\n",
        "context_scaled     = X_test_scaled[plot_idx]\n",
        "true_future_scaled = Y_test_scaled[plot_idx]\n",
        "\n",
        "context_tensor = torch.from_numpy(context_scaled).float().unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    pred_future_scaled = model(context_tensor).cpu().numpy().flatten()\n",
        "\n",
        "# Inverse-transform to original ECG units\n",
        "context = scaler.inverse_transform(\n",
        "    context_scaled.reshape(-1, 1)\n",
        ").flatten()\n",
        "true_future = scaler.inverse_transform(\n",
        "    true_future_scaled.reshape(-1, 1)\n",
        ").flatten()\n",
        "pred_future = scaler.inverse_transform(\n",
        "    pred_future_scaled.reshape(-1, 1)\n",
        ").flatten()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(CONTEXT_SAMPLES), context, color='gray', label=\"Context (Input, record 11)\")\n",
        "plt.plot(\n",
        "    range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "    true_future,\n",
        "    color='green',\n",
        "    label=\"True Future (record 11)\",\n",
        ")\n",
        "plt.plot(\n",
        "    range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "    pred_future,\n",
        "    color='red',\n",
        "    linestyle='--',\n",
        "    label=\"Predicted Future (PatchTST, trained on 42)\",\n",
        ")\n",
        "plt.axvline(CONTEXT_SAMPLES, color='black', linestyle='--')\n",
        "plt.xlabel(\"Sample Index (within 20s window)\")\n",
        "plt.ylabel(\"ECG amplitude (filtered units)\")\n",
        "plt.title(\"Raw ECG Forecasting ‚Äî PatchTST\\nTrain: data/42, Test: data/11\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNtQB2VDpLj8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üß© TASK 6 ‚Äî Implement Simple Baseline Forecasters (2 points)\n",
        "# ============================================================\n",
        "# Goal:\n",
        "#   Implement two simple, *non-learning* baseline forecasters for\n",
        "#   raw ECG time-series forecasting.\n",
        "#\n",
        "# Why baselines?\n",
        "#   - They provide a sanity check for deep models like PatchTST\n",
        "#   - If a complex model cannot beat a simple baseline, something is wrong\n",
        "#\n",
        "# You will implement:\n",
        "#   1) A persistence baseline (last value carried forward)\n",
        "#   2) A repeat-last-k baseline (repeat the last k samples)\n",
        "#\n",
        "# Both functions must:\n",
        "#   - Work on SCALED data\n",
        "#   - Return an array of shape (N, FORECAST_SAMPLES)\n",
        "#\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# üß© Task 6.1 ‚Äî Persistence baseline\n",
        "def persistence_forecast(X_context_scaled, forecast_len):\n",
        "    \"\"\"\n",
        "    Persistence baseline forecaster.\n",
        "\n",
        "    Strategy:\n",
        "        Predict the future by repeating the *last observed context value*\n",
        "        for the entire forecast horizon.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_context_scaled : np.ndarray\n",
        "        Shape (N, CONTEXT_SAMPLES), scaled context windows\n",
        "    forecast_len : int\n",
        "        Number of samples to predict\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Y_hat : np.ndarray\n",
        "        Shape (N, forecast_len)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # 1) Extract the last value of each context window\n",
        "    # 2) Repeat it forecast_len times along the time axis\n",
        "    # Hint: np.repeat or np.tile may be useful\n",
        "\n",
        "    Y_hat = None\n",
        "    return Y_hat\n",
        "\n",
        "\n",
        "# üß© Task 6.2 ‚Äî Repeat-last-k baseline\n",
        "def repeat_last_k_forecast(X_context_scaled, forecast_len, k=256):\n",
        "    \"\"\"\n",
        "    Repeat-last-k baseline forecaster.\n",
        "\n",
        "    Strategy:\n",
        "        Use the last k samples of the context window and repeat\n",
        "        them until the forecast horizon is filled.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_context_scaled : np.ndarray\n",
        "        Shape (N, CONTEXT_SAMPLES)\n",
        "    forecast_len : int\n",
        "        Number of samples to predict\n",
        "    k : int\n",
        "        Number of last context samples to repeat\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Y_hat : np.ndarray\n",
        "        Shape (N, forecast_len)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # 1) Ensure k does not exceed CONTEXT_SAMPLES\n",
        "    # 2) Extract the last k samples from each context window\n",
        "    # 3) Repeat / tile them to cover forecast_len samples\n",
        "    # 4) Truncate to exactly forecast_len\n",
        "\n",
        "    Y_hat = None\n",
        "    return Y_hat\n",
        "\n",
        "# ------------------------------\n",
        "# Create baseline predictions (scaled)\n",
        "# ------------------------------\n",
        "Y_base_persist = persistence_forecast(X_test_scaled, FORECAST_SAMPLES)\n",
        "Y_base_lastk   = repeat_last_k_forecast(X_test_scaled, FORECAST_SAMPLES, k=256)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Metrics helper\n",
        "# ------------------------------\n",
        "def compute_metrics(y_true_scaled, y_pred_scaled, scaler):\n",
        "    mse_s = mean_squared_error(y_true_scaled.flatten(), y_pred_scaled.flatten())\n",
        "    mae_s = mean_absolute_error(y_true_scaled.flatten(), y_pred_scaled.flatten())\n",
        "    smp_s = smape(y_true_scaled.flatten(), y_pred_scaled.flatten())\n",
        "\n",
        "    true_inv = scaler.inverse_transform(y_true_scaled.flatten().reshape(-1, 1)).flatten()\n",
        "    pred_inv = scaler.inverse_transform(y_pred_scaled.flatten().reshape(-1, 1)).flatten()\n",
        "\n",
        "    mse_i = mean_squared_error(true_inv, pred_inv)\n",
        "    mae_i = mean_absolute_error(true_inv, pred_inv)\n",
        "    smp_i = smape(true_inv, pred_inv)\n",
        "\n",
        "    return (mse_s, mae_s, smp_s), (mse_i, mae_i, smp_i)\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Print comparison\n",
        "# ------------------------------\n",
        "patch_metrics_scaled, patch_metrics_inv = compute_metrics(y_true_test, y_pred_test, scaler)\n",
        "pers_metrics_scaled,  pers_metrics_inv  = compute_metrics(y_true_test, Y_base_persist, scaler)\n",
        "lastk_metrics_scaled, lastk_metrics_inv = compute_metrics(y_true_test, Y_base_lastk, scaler)\n",
        "\n",
        "print(\"\\n================= TEST SET COMPARISON (record 11) =================\")\n",
        "print(\"Scaled space:\")\n",
        "print(f\"  PatchTST     ‚Üí MSE: {patch_metrics_scaled[0]:.6f}, MAE: {patch_metrics_scaled[1]:.6f}, sMAPE: {patch_metrics_scaled[2]:.2f}%\")\n",
        "print(f\"  Persistence  ‚Üí MSE: {pers_metrics_scaled[0]:.6f}, MAE: {pers_metrics_scaled[1]:.6f}, sMAPE: {pers_metrics_scaled[2]:.2f}%\")\n",
        "print(f\"  Repeat-lastK ‚Üí MSE: {lastk_metrics_scaled[0]:.6f}, MAE: {lastk_metrics_scaled[1]:.6f}, sMAPE: {lastk_metrics_scaled[2]:.2f}%\")\n",
        "\n",
        "print(\"\\nOriginal ECG units:\")\n",
        "print(f\"  PatchTST     ‚Üí MSE: {patch_metrics_inv[0]:.6f}, MAE: {patch_metrics_inv[1]:.6f}, sMAPE: {patch_metrics_inv[2]:.2f}%\")\n",
        "print(f\"  Persistence  ‚Üí MSE: {pers_metrics_inv[0]:.6f}, MAE: {pers_metrics_inv[1]:.6f}, sMAPE: {pers_metrics_inv[2]:.2f}%\")\n",
        "print(f\"  Repeat-lastK ‚Üí MSE: {lastk_metrics_inv[0]:.6f}, MAE: {lastk_metrics_inv[1]:.6f}, sMAPE: {lastk_metrics_inv[2]:.2f}%\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Plot helpers (separate figures)\n",
        "# ------------------------------\n",
        "def _inverse_1d(arr_scaled, scaler):\n",
        "    return scaler.inverse_transform(np.asarray(arr_scaled).reshape(-1, 1)).flatten()\n",
        "\n",
        "def plot_forecast_single(context, true_future, pred_future, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(range(CONTEXT_SAMPLES), context, label=\"Context\", color=\"gray\")\n",
        "\n",
        "    xs = np.arange(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES)\n",
        "    plt.plot(xs, true_future, label=\"True Future\", color=\"green\")\n",
        "    plt.plot(xs, pred_future, label=\"Prediction\", color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    plt.axvline(CONTEXT_SAMPLES, color=\"black\", linestyle=\"--\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Sample index within 20s window\")\n",
        "    plt.ylabel(\"ECG amplitude (filtered units)\")\n",
        "    plt.grid(True, alpha=0.4)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 6) Choose one window and generate 3 separate plots\n",
        "# ------------------------------\n",
        "plot_idx = 3\n",
        "plot_idx = plot_idx % len(X_test_scaled)\n",
        "print(f\"\\nPlotting test sample index: {plot_idx} / {len(X_test_scaled)}\")\n",
        "\n",
        "# Context + True in original units\n",
        "context = _inverse_1d(X_test_scaled[plot_idx], scaler)\n",
        "true_future = _inverse_1d(Y_test_scaled[plot_idx], scaler)\n",
        "\n",
        "# PatchTST prediction in original units\n",
        "patch_pred = _inverse_1d(y_pred_test[plot_idx], scaler)\n",
        "\n",
        "# Baseline predictions in original units\n",
        "persist_pred = _inverse_1d(Y_base_persist[plot_idx], scaler)\n",
        "lastk_pred   = _inverse_1d(Y_base_lastk[plot_idx], scaler)\n",
        "\n",
        "# --- Plot 1: PatchTST ---\n",
        "plot_forecast_single(\n",
        "    context, true_future, patch_pred,\n",
        "    title=f\"PatchTST Forecast (record 11) ‚Äî idx={plot_idx}\"\n",
        ")\n",
        "\n",
        "# --- Plot 2: Persistence baseline ---\n",
        "plot_forecast_single(\n",
        "    context, true_future, persist_pred,\n",
        "    title=f\"Baseline: Persistence Forecast (record 11) ‚Äî idx={plot_idx}\"\n",
        ")\n",
        "\n",
        "# --- Plot 3: Repeat-lastK baseline ---\n",
        "plot_forecast_single(\n",
        "    context, true_future, lastk_pred,\n",
        "    title=f\"Baseline: Repeat-lastK Forecast (record 11) ‚Äî idx={plot_idx}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAO0OfGhllhA"
      },
      "source": [
        "From our performance metrics and the plotted examples, we can tell that our **forecasts are not ideal**.\n",
        "\n",
        "The primary reason for that stems from the conflicting characteristics of the raw Electrocardiogram (ECG) signal: it is both highly periodic and extremely high-frequency.\n",
        "\n",
        "The model quickly identified the overall rhythm (the low-frequency periodicity of the heartbeat) but struggled to capture the detailed morphology and simultaneously fit the inherent noise. This resulted in a forecast (the red dashed line) that correctly followed the general pattern but was unstable, producing a jagged signal that failed to accurately predict the crucial peak and trough amplitudes.\n",
        "\n",
        "##üïµ Error analysis\n",
        "\n",
        "Let's further investigate the cases where our model struggled the most to produce forecasts. This will be your next **üß© Task**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWxvT4Jns8Py"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üß© TASK 7 ‚Äî Error analysis by window (STUDENT VERSION)\n",
        "# ============================================================\n",
        "#\n",
        "# Goal:\n",
        "#   Identify *which forecast windows are hardest* for PatchTST.\n",
        "#\n",
        "# You will:\n",
        "#   1) Compute per-window MAE between Y_test_scaled and y_pred_test\n",
        "#   2) Identify the indices of the top-k WORST windows\n",
        "#   3) Visualize those windows using the plotting helper below\n",
        "#\n",
        "# Why?\n",
        "#   - Aggregate metrics hide failure cases\n",
        "#   - Window-level error analysis reveals *when* and *why* the model fails\n",
        "#\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üß© Task 7 ‚Äî Per-window MAE (1 point)\n",
        "# ------------------------------------------------------------\n",
        "def per_window_mae(y_true_scaled, y_pred_scaled, top_k=3):\n",
        "    \"\"\"\n",
        "    Compute MAE per forecast window (row-wise).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true_scaled : np.ndarray\n",
        "        Shape (n_windows, forecast_len)\n",
        "    y_pred_scaled : np.ndarray\n",
        "        Shape (n_windows, forecast_len)\n",
        "    top_k : int\n",
        "        Number of worst windows to return\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    errors : np.ndarray\n",
        "        Per-window MAE, shape (n_windows,)\n",
        "    worst_indices : np.ndarray\n",
        "        Indices of the top_k worst windows (highest MAE),\n",
        "        sorted from worst to less-worst\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO 1:\n",
        "    # Compute MAE *per window* (mean over forecast dimension)\n",
        "    # Hint: axis=1\n",
        "    errors = None\n",
        "\n",
        "    # TODO 2:\n",
        "    # Find indices of the top_k largest errors\n",
        "    # Hint: np.argsort\n",
        "    worst_indices = None\n",
        "\n",
        "    # TODO 3:\n",
        "    # Print the worst windows and their MAE values\n",
        "    # (format is up to you)\n",
        "\n",
        "    return errors, worst_indices\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helper: plot a single test window\n",
        "# ------------------------------------------------------------\n",
        "def plot_test_window(idx, rank=None):\n",
        "    \"\"\"\n",
        "    Plot context + true future + PatchTST prediction\n",
        "    for a single test window.\n",
        "    \"\"\"\n",
        "    n_test = len(X_test_scaled)\n",
        "    idx = int(idx) % n_test\n",
        "\n",
        "    if rank is not None:\n",
        "        print(f\"\\nPlotting worst window #{rank}: index {idx}\")\n",
        "    else:\n",
        "        print(f\"\\nPlotting test window index: {idx}\")\n",
        "\n",
        "    # --- scaled data ---\n",
        "    context_scaled = X_test_scaled[idx]\n",
        "    true_future_scaled = Y_test_scaled[idx]\n",
        "    pred_future_scaled = y_pred_test[idx]\n",
        "\n",
        "    # --- inverse transform ---\n",
        "    context = scaler.inverse_transform(context_scaled.reshape(-1, 1)).flatten()\n",
        "    true_future = scaler.inverse_transform(true_future_scaled.reshape(-1, 1)).flatten()\n",
        "    pred_future = scaler.inverse_transform(pred_future_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # --- plot ---\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(range(CONTEXT_SAMPLES), context, color=\"gray\", label=\"Context\")\n",
        "    plt.plot(\n",
        "        range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "        true_future,\n",
        "        color=\"green\",\n",
        "        label=\"True Future\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "        pred_future,\n",
        "        color=\"red\",\n",
        "        linestyle=\"--\",\n",
        "        label=\"PatchTST Forecast\",\n",
        "    )\n",
        "    plt.axvline(CONTEXT_SAMPLES, color=\"black\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Sample index within 20s window\")\n",
        "    plt.ylabel(\"ECG amplitude (filtered units)\")\n",
        "    title_suffix = f\"(worst #{rank}, idx={idx})\" if rank is not None else f\"(idx={idx})\"\n",
        "    plt.title(f\"PatchTST Error Analysis ‚Äî Test Window {title_suffix}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7.3 Run analysis + plot worst windows\n",
        "# ------------------------------------------------------------\n",
        "window_errors, worst_indices = per_window_mae(y_true_test, y_pred_test, top_k=3)\n",
        "\n",
        "if worst_indices.size == 0:\n",
        "    print(\"‚ö†Ô∏è No windows available to plot.\")\n",
        "else:\n",
        "    for rank, idx in enumerate(worst_indices, start=1):\n",
        "        plot_test_window(idx, rank=rank)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoUibSxdrkbD"
      },
      "source": [
        "We could better guide the model by using a larger patch size (PATCH_LEN) to give the Transformer a better context of the entire heartbeat and increasing the smoothing penalty (LAMBDA_SMOOTH) to suppress the prediction's high-frequency instability.\n",
        "\n",
        "However, the most important step to correct this would be to heavily **increase the training budget** (epochs and the size of our training set). We were leveriging only a very limited subset of our training data and allowed training for 10 epochs only.\n",
        "\n",
        "We did this becasue we are **constrained time-wise and processing-wise** in our setup. In such scenarios, or when only limited training data is available - which is often the case in medical scenarios - reverting to a more lightweight model could be beneficial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtVkQtwtrmJQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ü§ñ Foundation Models for Time Series Forecasting\n",
        "Foundation Models (FMs) for time series forecasting are a new paradigm inspired by Large Language Models (LLMs), trained on massive, diverse datasets spanning numerous time series from different domains.\n",
        "\n",
        "By learning generalized temporal patterns, trends, and seasonality from vast amounts of data, these models develop a universal understanding of time series dynamics.\n",
        "\n",
        "The key advantage of FMs is their zero-shot or few-shot learning capability, meaning they can accurately forecast new, unseen time series without needing task-specific training or fine-tuning, dramatically reducing the effort required to deploy AI in fields like medicine.\n",
        "\n",
        "These models are typically built upon Transformer architectures, adapting the self-attention mechanism to capture long-range dependencies in sequential data. Prominent examples of time series Foundation Models include TimeGPT-1, Chronos (by Amazon), TimesFM (by Google), and Lag-Llama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu1mEVDKrthH"
      },
      "source": [
        "## ‚è∞ Zero-shot forecasting using Chronos-2\n",
        "\n",
        "The original Chronos family is based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context.\n",
        "\n",
        "The latest iteration offers zero-shot support for univariate, multivariate, and covariate-informed forecasting tasks.\n",
        "\n",
        "Despite being a foundation model, Chronos is relatively light-weight, not requiring heavy resources for inference.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Useful links\n",
        "\n",
        "\n",
        "\n",
        "*   Github: https://github.com/amazon-science/chronos-forecasting\n",
        "*   Paper: Ansari et al. 2024, arXiv:2403.07815\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LqX4t4wkrfZp"
      },
      "outputs": [],
      "source": [
        "!pip install chronos-forecasting  # or: pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
        "from chronos import Chronos2Pipeline\n",
        "pipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okEtQZCNuKoK"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ‚ö° Raw ECG Forecasting with Chronos-2 (Zero/Few-Shot)\n",
        "# Context/Forecast windows from data/11 (+ data/42 preprocessed for consistency)\n",
        "# No Task here - just try to understand the code and execute it.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import wfdb\n",
        "from scipy import signal\n",
        "from wfdb import processing as wfproc\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# ---------------------- Global Seeds -------------------------\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# --- Configuration for RAW ECG (in samples) ---\n",
        "WINDOW_SECONDS   = 20.0     # total window length (e.g. 20 s)\n",
        "CONTEXT_RATIO    = 0.8      # 80% context, 20% forecast\n",
        "WINDOW_STEP_SEC  = WINDOW_SECONDS  # non-overlapping windows\n",
        "\n",
        "LEAD_INDEX = 0\n",
        "LOWCUT, HIGHCUT = 0.5, 40.0\n",
        "FILTER_ORDER = 3\n",
        "\n",
        "MAX_WINDOWS_PER_RECORD = 2000   # safety cap per record\n",
        "MAX_TRAIN_WINDOWS      = 600    # from record 42 (kept for consistency)\n",
        "MAX_TEST_WINDOWS       = 200    # from record 11 (used for evaluation)\n",
        "\n",
        "# ============================================================\n",
        "# Preprocessing helpers (for raw ECG)\n",
        "# ============================================================\n",
        "def butter_bandpass(sig, fs, low=LOWCUT, high=HIGHCUT, order=FILTER_ORDER):\n",
        "    nyq = 0.5 * fs\n",
        "    low_norm = max(0.001, low / nyq)\n",
        "    high_norm = min(0.999, high / nyq)\n",
        "    b, a = signal.butter(order, [low_norm, high_norm], btype='band')\n",
        "    sig_f = signal.filtfilt(b, a, sig)\n",
        "    return sig_f\n",
        "\n",
        "def process_record_ecg_windows(record_path,\n",
        "                               window_seconds=WINDOW_SECONDS,\n",
        "                               step_seconds=WINDOW_STEP_SEC,\n",
        "                               max_windows=MAX_WINDOWS_PER_RECORD):\n",
        "    \"\"\"Return raw ECG windows (N, window_samples) for a single record.\"\"\"\n",
        "    try:\n",
        "        rec = wfdb.rdrecord(record_path)\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(f\"Cannot find record at {record_path}. Did you download it?\")\n",
        "\n",
        "    fs = rec.fs\n",
        "    sig = rec.p_signal[:, LEAD_INDEX].astype(float)\n",
        "    sig_f = butter_bandpass(sig, fs)\n",
        "\n",
        "    window_samples = int(window_seconds * fs)\n",
        "    step_samples   = int(step_seconds * fs)\n",
        "\n",
        "    if len(sig_f) < window_samples:\n",
        "        raise RuntimeError(\n",
        "            f\"Too few samples ({len(sig_f)}) for {window_samples}-sample windows in {record_path}\"\n",
        "        )\n",
        "\n",
        "    windows = [\n",
        "        sig_f[i:i + window_samples]\n",
        "        for i in range(0, len(sig_f) - window_samples + 1, step_samples)\n",
        "    ]\n",
        "    windows = np.array(windows)\n",
        "\n",
        "    if len(windows) > max_windows:\n",
        "        windows = windows[:max_windows]\n",
        "\n",
        "    print(f\"‚úÖ {record_path}: {len(windows)} windows of {window_samples} samples each (fs={fs} Hz)\")\n",
        "    return windows, fs, window_samples\n",
        "\n",
        "def split_context_forecast_samples(windows, context_ratio=CONTEXT_RATIO):\n",
        "    \"\"\"Split sample windows into context and forecast parts.\"\"\"\n",
        "    window_samples = windows.shape[1]\n",
        "    context_samples = int(window_samples * context_ratio)\n",
        "    forecast_samples = window_samples - context_samples\n",
        "\n",
        "    X = windows[:, :context_samples]\n",
        "    Y = windows[:, context_samples:]\n",
        "    print(f\"Context samples: {context_samples}, Forecast samples: {forecast_samples}\")\n",
        "    print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "    return X, Y, context_samples, forecast_samples\n",
        "\n",
        "# ============================================================\n",
        "# Build TRAIN from data/42, TEST from data/11 (RAW ECG)\n",
        "# (Chronos-2 will be evaluated on data/11 windows)\n",
        "# ============================================================\n",
        "try:\n",
        "    # --- TRAIN: record 42 (not used for Chronos training in zero-shot,\n",
        "    #     but kept for symmetry/comparison with PatchTST) ---\n",
        "    win_42, fs_42, window_samples_42 = process_record_ecg_windows(\n",
        "        \"data/42\",\n",
        "        window_seconds=WINDOW_SECONDS,\n",
        "        step_seconds=WINDOW_STEP_SEC,\n",
        "        max_windows=MAX_TRAIN_WINDOWS,\n",
        "    )\n",
        "    X_train_all, Y_train_all, CONTEXT_SAMPLES, FORECAST_SAMPLES = split_context_forecast_samples(\n",
        "        win_42, context_ratio=CONTEXT_RATIO\n",
        "    )\n",
        "\n",
        "    # --- TEST: record 11 ---\n",
        "    win_11, fs_11, window_samples_11 = process_record_ecg_windows(\n",
        "        \"data/11\",\n",
        "        window_seconds=WINDOW_SECONDS,\n",
        "        step_seconds=WINDOW_STEP_SEC,\n",
        "        max_windows=MAX_TEST_WINDOWS,\n",
        "    )\n",
        "    X_test, Y_test, CONTEXT_SAMPLES_test, FORECAST_SAMPLES_test = split_context_forecast_samples(\n",
        "        win_11, context_ratio=CONTEXT_RATIO\n",
        "    )\n",
        "\n",
        "    if fs_42 != fs_11:\n",
        "        print(f\"‚ö†Ô∏è Warning: fs_42={fs_42} Hz, fs_11={fs_11} Hz (not identical).\")\n",
        "    if (CONTEXT_SAMPLES != CONTEXT_SAMPLES_test) or (FORECAST_SAMPLES != FORECAST_SAMPLES_test):\n",
        "        raise RuntimeError(\"Context/forecast sample lengths differ between records 42 and 11.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error while building train/test from records 42 and 11 (raw ECG): {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(f\"\\nFinal RAW ECG shapes:\")\n",
        "print(f\"  TRAIN (42): X={X_train_all.shape}, Y={Y_train_all.shape}\")\n",
        "print(f\"  TEST  (11): X={X_test.shape},     Y={Y_test.shape}\")\n",
        "\n",
        "N_TEST = len(X_test)\n",
        "\n",
        "# ============================================================\n",
        "# Chronos-2 Zero-Shot Forecasting on RAW ECG windows (record 11)\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import torch\n",
        "from chronos import Chronos2Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def smape(y_true, y_pred, eps=1e-8):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    denom = np.abs(y_true) + np.abs(y_pred) + eps\n",
        "    return 100.0 * np.mean(2.0 * np.abs(y_true - y_pred) / denom)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device for Chronos-2: {device}\")\n",
        "\n",
        "# Load Chronos-2 zero-shot forecaster\n",
        "pipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=device)\n",
        "\n",
        "# Build long-format context DataFrame from TEST windows (record 11)\n",
        "# Each window is a separate short time series with its own id\n",
        "# id: window index, timestamp: synthetic index, target: ECG amplitude\n",
        "timestamps = pd.date_range(\"2000-01-01\", periods=CONTEXT_SAMPLES, freq=\"ms\")  # 1 ms spacing (arbitrary)\n",
        "\n",
        "records = []\n",
        "for i in range(N_TEST):\n",
        "    for t_idx, value in enumerate(X_test[i]):\n",
        "        records.append({\"id\": i, \"timestamp\": timestamps[t_idx], \"target\": float(value)})\n",
        "\n",
        "context_df = pd.DataFrame(records)\n",
        "print(\"\\nSample of Chronos context_df (record 11 windows):\")\n",
        "print(context_df.head())\n",
        "\n",
        "# Run zero-shot forecasting for all test windows\n",
        "pred_df = pipeline.predict_df(\n",
        "    context_df,\n",
        "    prediction_length=FORECAST_SAMPLES,\n",
        "    quantile_levels=[0.5],      # median as point forecast\n",
        "    id_column=\"id\",\n",
        "    timestamp_column=\"timestamp\",\n",
        "    target=\"target\",\n",
        ")\n",
        "\n",
        "print(\"\\nSample of Chronos prediction DataFrame:\")\n",
        "print(pred_df.head())\n",
        "\n",
        "# ============================================================\n",
        "# Collect predictions per window and compute metrics\n",
        "# ============================================================\n",
        "# Chronos output: one row per (id, future timestamp)\n",
        "# We expect a column \"0.5\" for the median forecast; if not, fall back to \"predictions\"\n",
        "if \"0.5\" in pred_df.columns:\n",
        "    pred_col = \"0.5\"\n",
        "elif \"predictions\" in pred_df.columns:\n",
        "    pred_col = \"predictions\"\n",
        "else:\n",
        "    raise RuntimeError(f\"Unexpected Chronos-2 output columns: {pred_df.columns}\")\n",
        "\n",
        "y_pred = np.zeros_like(Y_test, dtype=float)\n",
        "\n",
        "for i in range(N_TEST):\n",
        "    series_pred = (\n",
        "        pred_df[pred_df[\"id\"] == i]\n",
        "        .sort_values(\"timestamp\")[pred_col]\n",
        "        .to_numpy()\n",
        "    )\n",
        "    if len(series_pred) < FORECAST_SAMPLES:\n",
        "        raise RuntimeError(\n",
        "            f\"Chronos-2 returned only {len(series_pred)} steps for id={i}, \"\n",
        "            f\"expected {FORECAST_SAMPLES}\"\n",
        "        )\n",
        "    y_pred[i, :] = series_pred[:FORECAST_SAMPLES]\n",
        "\n",
        "y_true = Y_test.copy()\n",
        "\n",
        "# Metrics in original ECG units (filtered amplitude)\n",
        "mse_ecg   = mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
        "mae_ecg   = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
        "smape_ecg = smape(y_true.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"\\n===== Chronos-2 Zero-Shot Performance (RAW ECG, record 11 windows) =====\")\n",
        "print(f\"MSE   : {mse_ecg:.6f}\")\n",
        "print(f\"MAE   : {mae_ecg:.6f}\")\n",
        "print(f\"sMAPE: {smape_ecg:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# Visualize one example forecast (record 11, RAW ECG)\n",
        "# ============================================================\n",
        "example_idx = 3\n",
        "if example_idx >= N_TEST:\n",
        "    example_idx = 0\n",
        "\n",
        "context = X_test[example_idx]\n",
        "true_future = Y_test[example_idx]\n",
        "pred_future = y_pred[example_idx]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(CONTEXT_SAMPLES), context, color='gray', label=\"Context (Input, record 11)\")\n",
        "plt.plot(\n",
        "    range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "    true_future,\n",
        "    color='green',\n",
        "    label=\"True Future (record 11)\",\n",
        ")\n",
        "plt.plot(\n",
        "    range(CONTEXT_SAMPLES, CONTEXT_SAMPLES + FORECAST_SAMPLES),\n",
        "    pred_future,\n",
        "    color='red',\n",
        "    linestyle='--',\n",
        "    label=\"Chronos-2 (zero-shot)\",\n",
        ")\n",
        "plt.axvline(CONTEXT_SAMPLES, color='black', linestyle='--')\n",
        "plt.xlabel(\"Sample Index (within 20s window)\")\n",
        "plt.ylabel(\"ECG amplitude (filtered units)\")\n",
        "plt.title(\"Raw ECG Forecasting ‚Äî Chronos-2 Zero-Shot\\nTest: data/11 (windowed)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nllAzUVJ2hqw"
      },
      "source": [
        "## üèÅ Conclusion - Part 2\n",
        "\n",
        "In this second part of the exercise, we moved beyond classification and explored time series forecasting on raw ECG signals.\n",
        "\n",
        "We implemented and trained a Transformer-based forecasting model (PatchTST), training on one patient record and testing on a different one. This simplified setup mirrors real clinical deployment scenarios, where models must generalize across patients rather than memorizing individual signal characteristics.\n",
        "\n",
        "We identified shortcomings of our simplief modelling approach through trageted error analysis and comparison with baseline forecasters.\n",
        "\n",
        "By implementing a time series foundation model (Chronos-2), we saw how such models can deliver superior zero-shot performance in resource-constrained environments, where training complex models on large datasets is not feasible.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
